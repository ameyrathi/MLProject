{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README: This is the final notebook. Use this notebook to paste the final code.\n",
    "Do not test code here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('raw_trump_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >= 20th jan 2017, <= 31st Dec 2018\n",
    "tweets['date']= pd.to_datetime(tweets['date'])\n",
    "tweets = tweets[tweets['date'] >= np.datetime64('2017-01-20')]\n",
    "tweets = tweets[tweets['date'] <= np.datetime64('2018-12-31')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_remove_duplicates(tweets, date):\n",
    "    duplicated_rows = tweets[tweets['date'] == date]\n",
    "    id = max(duplicated_rows['id'])\n",
    "    max_favorite = max(duplicated_rows['favorites'])\n",
    "    max_retweet = max(duplicated_rows['retweets'])\n",
    "    combined_tweet = \"\"\n",
    "    device = duplicated_rows['device'].iloc[0]\n",
    "    for index, row in duplicated_rows.iterrows():\n",
    "        combined_tweet = row['text'] + ' ' + combined_tweet.strip('.')\n",
    "    tweets = tweets[tweets.date != date]\n",
    "    row = {'id': id, 'text': combined_tweet, 'isRetweet': 'f', 'isDeleted': 'f', 'device': device, 'favorites': max_favorite, 'retweets': max_retweet, 'date': date}\n",
    "    tweets = tweets.append(row, ignore_index = True)   \n",
    "    print(f\"Combined {len(duplicated_rows)} tweets on {date}\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-11-21T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T21:59:00.000000000\n",
      "Combined 2 tweets on 2018-08-14T01:37:00.000000000\n",
      "Combined 2 tweets on 2018-05-31T10:56:00.000000000\n",
      "Combined 2 tweets on 2018-02-01T22:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T17:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:48:00.000000000\n",
      "Combined 3 tweets on 2017-12-28T22:17:00.000000000\n",
      "Combined 2 tweets on 2017-12-22T20:47:00.000000000\n",
      "Combined 2 tweets on 2017-12-16T03:09:00.000000000\n",
      "Combined 2 tweets on 2017-11-18T13:49:00.000000000\n",
      "Combined 2 tweets on 2017-11-15T10:22:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:53:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-28T21:09:00.000000000\n",
      "Combined 2 tweets on 2017-10-26T01:47:00.000000000\n",
      "Combined 2 tweets on 2017-10-14T11:08:00.000000000\n",
      "Combined 2 tweets on 2017-05-10T10:31:00.000000000\n",
      "Combined 2 tweets on 2017-09-30T01:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-22T10:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-20T10:00:00.000000000\n",
      "Combined 2 tweets on 2017-09-17T12:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:27:00.000000000\n",
      "Combined 2 tweets on 2017-07-09T12:51:00.000000000\n",
      "Combined 2 tweets on 2017-08-29T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-28T13:08:00.000000000\n",
      "Combined 2 tweets on 2017-08-24T13:15:00.000000000\n",
      "Combined 2 tweets on 2017-08-19T20:41:00.000000000\n",
      "Combined 2 tweets on 2017-08-18T15:27:00.000000000\n",
      "Combined 2 tweets on 2017-08-16T11:32:00.000000000\n",
      "Combined 2 tweets on 2017-08-15T11:03:00.000000000\n",
      "Combined 2 tweets on 2017-11-08T11:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-08T01:18:00.000000000\n",
      "Combined 2 tweets on 2017-09-08T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-08T10:59:00.000000000\n",
      "Combined 3 tweets on 2017-04-08T13:18:00.000000000\n",
      "Combined 2 tweets on 2017-10-07T11:54:00.000000000\n",
      "Combined 2 tweets on 2017-06-27T11:00:00.000000000\n",
      "Combined 2 tweets on 2017-06-26T18:16:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T08:29:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T20:27:00.000000000\n",
      "Combined 2 tweets on 2017-02-06T10:32:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T12:45:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T11:57:00.000000000\n",
      "Combined 2 tweets on 2017-04-27T14:39:00.000000000\n",
      "Combined 3 tweets on 2017-04-27T14:37:00.000000000\n",
      "Combined 2 tweets on 2017-04-22T15:18:00.000000000\n",
      "Combined 2 tweets on 2017-03-30T22:16:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:54:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:51:00.000000000\n",
      "Combined 2 tweets on 2018-12-27T21:04:00.000000000\n",
      "Combined 2 tweets on 2018-12-20T22:21:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T02:07:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-12-16T20:29:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T18:17:00.000000000\n",
      "Combined 2 tweets on 2018-12-13T17:34:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T18:58:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T14:19:00.000000000\n",
      "Combined 2 tweets on 2018-07-12T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-04-12T22:56:00.000000000\n",
      "Combined 2 tweets on 2018-01-12T15:21:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T22:14:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T16:34:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:41:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T19:05:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T04:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T20:20:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T19:47:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T16:42:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T00:43:00.000000000\n",
      "Combined 2 tweets on 2018-07-11T19:44:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:26:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:25:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:23:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:20:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:19:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:41:00.000000000\n",
      "Combined 3 tweets on 2018-04-11T15:40:00.000000000\n",
      "Combined 4 tweets on 2018-04-11T15:39:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:32:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:28:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:27:00.000000000\n",
      "Combined 2 tweets on 2018-03-11T21:12:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T18:37:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T03:34:00.000000000\n",
      "Combined 2 tweets on 2018-10-30T17:37:00.000000000\n",
      "Combined 2 tweets on 2018-10-27T21:41:00.000000000\n",
      "Combined 2 tweets on 2018-10-25T18:47:00.000000000\n",
      "Combined 2 tweets on 2018-10-22T19:18:00.000000000\n",
      "Combined 2 tweets on 2018-10-18T15:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-17T17:11:00.000000000\n",
      "Combined 2 tweets on 2018-10-16T18:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:50:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T22:33:00.000000000\n",
      "Combined 4 tweets on 2018-10-10T12:48:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:47:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T16:00:00.000000000\n",
      "Combined 2 tweets on 2018-05-10T21:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-26T19:23:00.000000000\n",
      "Combined 2 tweets on 2018-09-22T03:09:00.000000000\n",
      "Combined 2 tweets on 2018-09-21T20:24:00.000000000\n",
      "Combined 2 tweets on 2018-09-20T18:10:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:33:00.000000000\n",
      "Combined 2 tweets on 2018-09-18T15:50:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:47:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:46:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:45:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:44:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:43:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T13:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:35:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:32:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:31:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:28:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T00:11:00.000000000\n",
      "Combined 2 tweets on 2018-09-13T12:16:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:08:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-08-09T01:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-09T20:41:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T11:05:00.000000000\n",
      "Combined 2 tweets on 2018-02-09T01:23:00.000000000\n",
      "Combined 4 tweets on 2018-08-29T21:23:00.000000000\n",
      "Combined 2 tweets on 2018-08-28T15:02:00.000000000\n",
      "Combined 2 tweets on 2018-08-27T22:07:00.000000000\n",
      "Combined 2 tweets on 2018-08-26T13:21:00.000000000\n",
      "Combined 3 tweets on 2018-08-24T17:36:00.000000000\n",
      "Combined 2 tweets on 2018-08-23T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-08-17T19:25:00.000000000\n",
      "Combined 2 tweets on 2018-08-13T17:14:00.000000000\n",
      "Combined 2 tweets on 2018-05-08T01:43:00.000000000\n",
      "Combined 4 tweets on 2018-04-08T03:05:00.000000000\n",
      "Combined 2 tweets on 2018-07-31T17:33:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:29:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:28:00.000000000\n",
      "Combined 4 tweets on 2018-07-29T19:09:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-29T11:59:00.000000000\n",
      "Combined 2 tweets on 2018-07-27T15:53:00.000000000\n",
      "Combined 2 tweets on 2018-07-26T22:48:00.000000000\n",
      "Combined 2 tweets on 2018-07-25T23:42:00.000000000\n",
      "Combined 3 tweets on 2018-07-15T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-07-24T11:01:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:39:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:35:00.000000000\n",
      "Combined 2 tweets on 2018-07-15T13:40:00.000000000\n",
      "Combined 2 tweets on 2018-07-16T04:28:00.000000000\n",
      "Combined 6 tweets on 2018-07-15T13:33:00.000000000\n",
      "Combined 2 tweets on 2018-12-07T15:00:00.000000000\n",
      "Combined 3 tweets on 2018-11-07T21:11:00.000000000\n",
      "Combined 4 tweets on 2018-11-07T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-11-07T12:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-07T09:00:00.000000000\n",
      "Combined 4 tweets on 2018-07-07T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-05-07T19:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:52:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:13:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:59:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:49:00.000000000\n",
      "Combined 2 tweets on 2018-06-28T03:24:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-06-25T16:58:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T14:00:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:21:00.000000000\n",
      "Combined 3 tweets on 2018-06-23T11:19:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:16:00.000000000\n",
      "Combined 2 tweets on 2018-06-21T20:46:00.000000000\n",
      "Combined 4 tweets on 2018-06-19T13:52:00.000000000\n",
      "Combined 2 tweets on 2018-06-18T13:50:00.000000000\n",
      "Combined 3 tweets on 2018-06-17T14:36:00.000000000\n",
      "Combined 2 tweets on 2018-06-15T11:56:00.000000000\n",
      "Combined 4 tweets on 2018-06-15T11:55:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T08:53:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:09:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:08:00.000000000\n",
      "Combined 2 tweets on 2018-06-13T09:40:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T20:40:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:58:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:56:00.000000000\n",
      "Combined 2 tweets on 2018-06-06T00:40:00.000000000\n",
      "Combined 2 tweets on 2018-05-30T12:47:00.000000000\n",
      "Combined 4 tweets on 2018-05-25T12:04:00.000000000\n",
      "Combined 2 tweets on 2018-05-23T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-05-16T18:40:00.000000000\n",
      "Combined 3 tweets on 2018-05-16T13:09:00.000000000\n",
      "Combined 2 tweets on 2018-08-05T20:33:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T13:10:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T12:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-20T10:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-19T22:30:00.000000000\n",
      "Combined 2 tweets on 2018-04-18T12:05:00.000000000\n",
      "Combined 3 tweets on 2018-04-18T09:42:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:55:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:26:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:25:00.000000000\n",
      "Combined 4 tweets on 2018-04-17T12:24:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:57:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:56:00.000000000\n",
      "Combined 2 tweets on 2018-03-28T21:31:00.000000000\n",
      "Combined 2 tweets on 2018-03-15T17:47:00.000000000\n",
      "Combined 2 tweets on 2018-03-02T14:40:00.000000000\n",
      "Combined 2 tweets on 2018-01-28T13:18:00.000000000\n",
      "Combined 2 tweets on 2018-01-21T02:31:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:53:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:52:00.000000000\n",
      "Combined 2 tweets on 2018-01-13T13:14:00.000000000\n"
     ]
    }
   ],
   "source": [
    "dates = tweets[\"date\"]\n",
    "duplicated = tweets[dates.isin(dates[dates.duplicated()])]\n",
    "duplicated_dates = duplicated['date'].unique()\n",
    "\n",
    "for date in duplicated_dates:\n",
    "    tweets = process_and_remove_duplicates(tweets, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets that are Trump's \n",
    "# remove retweets\n",
    "tweets1 = tweets[tweets['isRetweet'] == 'f'].reset_index(drop=True)\n",
    "tweets1 = tweets1[tweets1['text'].str.contains('RT @') == False]\n",
    "tweets1['cleaned_text'] = tweets1['text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S*', '', x, flags=re.MULTILINE))\n",
    "tweets1 = tweets1[tweets1['cleaned_text'].str.strip() != '']\n",
    "\n",
    "# Feauture extraction\n",
    "tweets2 = tweets1.loc[:, ['id', 'cleaned_text', 'favorites', 'retweets', 'date']]\n",
    "tweets2['date_new'] = [i + datetime.timedelta(hours = 8) for i in tweets2['date']]\n",
    "tweets2['date_part'] = [i.date() for i in tweets2['date_new']]\n",
    "tweets2['time_part'] = [i.time() for i in tweets2['date_new']]\n",
    "tweets2['hour'] = [int(str(i).split(\":\")[0]) for i in tweets2['time_part']]\n",
    "tweets2['year'] = [int(str(i).split(\"-\")[0]) for i in tweets2['date_part']]\n",
    "tweets2['month'] = [int(str(i).split(\"-\")[1]) for i in tweets2['date_part']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet cleaning functions\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_mentions(string):\n",
    "    return re.sub(\"@[A-Za-z0-9]+\",\"\",string)\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_tweet(x):\n",
    "    \n",
    "    x = remove_emoji(x)\n",
    "    x = remove_mentions(x)\n",
    "    x = url_free_text(x)\n",
    "    # remove punctuations \n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), '', x)\n",
    "    # remove numbers\n",
    "    x = re.sub('\\w*\\d\\w*', '', x)\n",
    "    # remove single quotes\n",
    "    x = x.strip(\"'\")\n",
    "    # remove hashtags\n",
    "    x = x.replace(\"#\", \"\")\n",
    "    # remove double quotes\n",
    "    x = x.strip('\"') \n",
    "    x = x.lower()\n",
    "    return x\n",
    "    \n",
    "tweets2['cleaned_text_2'] = tweets2.cleaned_text.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets outside trading hours \n",
    "trump_tweets = tweets2.copy()\n",
    "trump_tweets['date_new'] = pd.to_datetime(trump_tweets['date_new'])\n",
    "trump_tweets.rename(columns={\"date_new\": \"tweet_datetime\"}, inplace=True)\n",
    "trump_tweets.set_index('tweet_datetime', inplace=True, drop=False)\n",
    "trump_tweets.index.name = None\n",
    "trump_tweets = trump_tweets.between_time(datetime.time(4), datetime.time(20), include_start=True, include_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets.to_csv(\"trump_tweets_cleaned_2.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are combined after pre processing! (else kernel crashes)\n",
    "stock_price_2017 = pd.read_csv('yr2017_cleaned.csv')\n",
    "stock_price_2018 = pd.read_csv('yr2018_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME_M</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170120</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>226.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170120</td>\n",
       "      <td>04:04:00</td>\n",
       "      <td>226.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170120</td>\n",
       "      <td>04:05:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170120</td>\n",
       "      <td>04:08:00</td>\n",
       "      <td>226.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170120</td>\n",
       "      <td>04:10:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185597</th>\n",
       "      <td>20180119</td>\n",
       "      <td>19:56:00</td>\n",
       "      <td>280.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185598</th>\n",
       "      <td>20180119</td>\n",
       "      <td>19:57:00</td>\n",
       "      <td>280.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185599</th>\n",
       "      <td>20180119</td>\n",
       "      <td>19:58:00</td>\n",
       "      <td>280.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185600</th>\n",
       "      <td>20180119</td>\n",
       "      <td>19:59:00</td>\n",
       "      <td>280.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185601</th>\n",
       "      <td>20180119</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>280.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185602 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DATE    TIME_M   PRICE\n",
       "0       20170120  04:00:00  226.50\n",
       "1       20170120  04:04:00  226.49\n",
       "2       20170120  04:05:00  226.52\n",
       "3       20170120  04:08:00  226.54\n",
       "4       20170120  04:10:00  226.52\n",
       "...          ...       ...     ...\n",
       "185597  20180119  19:56:00  280.50\n",
       "185598  20180119  19:57:00  280.55\n",
       "185599  20180119  19:58:00  280.55\n",
       "185600  20180119  19:59:00  280.55\n",
       "185601  20180119  20:00:00  280.41\n",
       "\n",
       "[185602 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "stock_price_2017.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)\n",
    "stock_price_2018.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datetime objects\n",
    "stock_price_2017['TIME_M'] = pd.to_datetime(stock_price_2017['TIME_M'], format='%H:%M:%S.%f')\n",
    "stock_price_2018['TIME_M'] = pd.to_datetime(stock_price_2018['TIME_M'], format='%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, milliseconds from data\n",
    "stock_price_2017['TIME_M'] = stock_price_2017['TIME_M'].dt.floor('Min').dt.time\n",
    "stock_price_2018['TIME_M'] = stock_price_2018['TIME_M'].dt.floor('Min').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minute to minute data (one entry for each minute)\n",
    "stock_price_2017.drop_duplicates(subset=['TIME_M','DATE'],inplace=True)\n",
    "stock_price_2018.drop_duplicates(subset=['TIME_M', 'DATE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv, will be used for preprocessing later!\n",
    "stock_price = pd.concat([stock_price_2017,stock_price_2018], ignore_index=True, sort=False)\n",
    "stock_price.to_csv(\"stock_price.csv\", index=False, header=True)\n",
    "stock_price = pd.read_csv(\"stock_price.csv\", parse_dates=[['DATE', 'TIME_M']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME_M</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-20 04:00:00</td>\n",
       "      <td>226.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-20 04:04:00</td>\n",
       "      <td>226.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-20 04:05:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-20 04:08:00</td>\n",
       "      <td>226.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-20 04:10:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381308</th>\n",
       "      <td>2018-12-31 19:56:00</td>\n",
       "      <td>250.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381309</th>\n",
       "      <td>2018-12-31 19:57:00</td>\n",
       "      <td>250.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381310</th>\n",
       "      <td>2018-12-31 19:58:00</td>\n",
       "      <td>250.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381311</th>\n",
       "      <td>2018-12-31 19:59:00</td>\n",
       "      <td>250.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381312</th>\n",
       "      <td>2018-12-31 20:00:00</td>\n",
       "      <td>249.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381313 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_TIME_M   PRICE\n",
       "0      2017-01-20 04:00:00  226.50\n",
       "1      2017-01-20 04:04:00  226.49\n",
       "2      2017-01-20 04:05:00  226.52\n",
       "3      2017-01-20 04:08:00  226.54\n",
       "4      2017-01-20 04:10:00  226.52\n",
       "...                    ...     ...\n",
       "381308 2018-12-31 19:56:00  250.06\n",
       "381309 2018-12-31 19:57:00  250.07\n",
       "381310 2018-12-31 19:58:00  250.10\n",
       "381311 2018-12-31 19:59:00  250.07\n",
       "381312 2018-12-31 20:00:00  249.92\n",
       "\n",
       "[381313 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolation of missing values\n",
    "idx = pd.date_range(stock_price.iloc[0, 0], stock_price.iloc[-1, 0], freq='1min')\n",
    "stock_price.index = pd.DatetimeIndex(stock_price.loc[:, 'DATE_TIME_M'])\n",
    "stock_price = stock_price.reindex(idx, fill_value='NaN')\n",
    "stock_price['DATE_TIME_M'] = stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_price.astype({'PRICE': 'float'})\n",
    "stock_price['PRICE'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_csv(\"stock_price_cleaned.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Stock Data & Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = pd.read_csv(\"trump_tweets_cleaned_2.csv\")\n",
    "stock_price = pd.read_csv(\"stock_price_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cleaned_text', 'favorites', 'retweets', 'date', 'tweet_datetime',\n",
       "       'date_part', 'time_part', 'hour', 'year', 'month', 'cleaned_text_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.rename(columns={\"DATE_TIME_M\": \"datetime\", \"PRICE\": \"price\"}, inplace=True)\n",
    "stock_price['datetime']= pd.to_datetime(stock_price['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Stock Price Difference X mins after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand!</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>thank you rand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>join me live from fort myer in arlington virgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole!</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>thank you nicole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.778460e+17</td>\n",
       "      <td>Great night in Iowa - special people. Thank you!</td>\n",
       "      <td>56446</td>\n",
       "      <td>8039</td>\n",
       "      <td>2017-06-22 11:11:00</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>19:11:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>great night in iowa  special people thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>If the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>So General Michael Flynn’s life can be totally...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>My thoughts, prayers and condolences are with ...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>Today’s Court decision means that Congress mus...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>I am pleased to announce that I intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2726 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.353400e+17                                   Thank you Rand!    \n",
       "1     8.997980e+17  Join me live from Fort Myer in Arlington, Virg...   \n",
       "2     8.939700e+17                                 Thank you Nicole!    \n",
       "3     8.819770e+17  Thank you to Shawn Steel for the nice words on...   \n",
       "4     8.778460e+17   Great night in Iowa - special people. Thank you!   \n",
       "...            ...                                                ...   \n",
       "2721  9.990960e+17  If the person placed very early into my campai...   \n",
       "2722  9.874600e+17  So General Michael Flynn’s life can be totally...   \n",
       "2723  9.870960e+17  My thoughts, prayers and condolences are with ...   \n",
       "2724  9.863570e+17  Today’s Court decision means that Congress mus...   \n",
       "2725  9.791090e+17  I am pleased to announce that I intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4         56446      8039  2017-06-22 11:11:00  2017-06-22 19:11:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2721      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2722      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2723      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2724      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2725      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   \n",
       "1     2017-08-22  09:00:00     9  2017      8   \n",
       "2     2017-05-09  07:01:00     7  2017      5   \n",
       "3     2017-03-08  04:44:00     4  2017      3   \n",
       "4     2017-06-22  19:11:00    19  2017      6   \n",
       "...          ...       ...   ...   ...    ...   \n",
       "2721  2018-05-23  09:13:00     9  2018      5   \n",
       "2722  2018-04-20  18:34:00    18  2018      4   \n",
       "2723  2018-04-20  06:30:00     6  2018      4   \n",
       "2724  2018-04-18  05:34:00     5  2018      4   \n",
       "2725  2018-03-29  05:31:00     5  2018      3   \n",
       "\n",
       "                                         cleaned_text_2  \n",
       "0                                       thank you rand   \n",
       "1     join me live from fort myer in arlington virgi...  \n",
       "2                                     thank you nicole   \n",
       "3       thank you to shawn steel for the nice words on   \n",
       "4         great night in iowa  special people thank you  \n",
       "...                                                 ...  \n",
       "2721  if the person placed very early into my campai...  \n",
       "2722  so general michael flynn’s life can be totally...  \n",
       "2723  my thoughts prayers and condolences are with t...  \n",
       "2724  today’s court decision means that congress mus...  \n",
       "2725  i am pleased to announce that i intend to nomi...  \n",
       "\n",
       "[2726 rows x 12 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-20 04:00:00</td>\n",
       "      <td>226.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-20 04:04:00</td>\n",
       "      <td>226.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-20 04:05:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-20 04:08:00</td>\n",
       "      <td>226.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-20 04:10:00</td>\n",
       "      <td>226.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381308</th>\n",
       "      <td>2018-12-31 19:56:00</td>\n",
       "      <td>250.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381309</th>\n",
       "      <td>2018-12-31 19:57:00</td>\n",
       "      <td>250.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381310</th>\n",
       "      <td>2018-12-31 19:58:00</td>\n",
       "      <td>250.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381311</th>\n",
       "      <td>2018-12-31 19:59:00</td>\n",
       "      <td>250.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381312</th>\n",
       "      <td>2018-12-31 20:00:00</td>\n",
       "      <td>249.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381313 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime   price\n",
       "0      2017-01-20 04:00:00  226.50\n",
       "1      2017-01-20 04:04:00  226.49\n",
       "2      2017-01-20 04:05:00  226.52\n",
       "3      2017-01-20 04:08:00  226.54\n",
       "4      2017-01-20 04:10:00  226.52\n",
       "...                    ...     ...\n",
       "381308 2018-12-31 19:56:00  250.06\n",
       "381309 2018-12-31 19:57:00  250.07\n",
       "381310 2018-12-31 19:58:00  250.10\n",
       "381311 2018-12-31 19:59:00  250.07\n",
       "381312 2018-12-31 20:00:00  249.92\n",
       "\n",
       "[381313 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price_x_mins_after(trump_tweets, stock_price, x_mins):\n",
    "    stock_price[f\"datetime_{x_mins}mins_before\"] = stock_price[\"datetime\"] - pd.Timedelta(f\"{x_mins} min\")\n",
    "    stock_price_merged = stock_price.merge(stock_price, how='inner', left_on=[f\"datetime_{x_mins}mins_before\"], right_on=[\"datetime\"])\n",
    "    stock_price_merged.drop(columns=[\"datetime_y\", f\"datetime_{x_mins}mins_before_y\"], inplace=True)\n",
    "    stock_price_merged.rename(columns={\"datetime_x\": f\"datetime_{x_mins}mins_after\", \"price_x\": f\"price_{x_mins}mins_after\", f\"datetime_{x_mins}mins_before_x\": \"datetime_now\", \"price_y\": \"price_now\"}, inplace=True)\n",
    "\n",
    "    stock_price_merged[f'{x_mins}mins_price_diff_abs'] = stock_price_merged[f'price_{x_mins}mins_after'] - stock_price_merged['price_now']\n",
    "    stock_price_merged[f'{x_mins}mins_price_diff_perc'] = (stock_price_merged[f'price_{x_mins}mins_after'] - stock_price_merged['price_now']) / stock_price_merged['price_now']\n",
    "\n",
    "    trump_tweets.tweet_datetime = pd.to_datetime(trump_tweets.tweet_datetime)\n",
    "    stock_price_merged.datetime_now = pd.to_datetime(stock_price_merged.datetime_now)\n",
    "\n",
    "    merged = trump_tweets.merge(stock_price_merged, how='inner', left_on=['tweet_datetime'], right_on=[\"datetime_now\"])\n",
    "    # merged.drop(columns=[\"datetime_now\", \"price_now\", \"cleaned_text\"], inplace=True)\n",
    "    # merged.insert(1, 'cleaned_text', merged['cleaned_text2'])\n",
    "    # merged.drop(columns=[\"cleaned_text2\"], inplace=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "merged = get_stock_price_x_mins_after(trump_tweets, stock_price, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Array of Stock Prices 30 minutes before, ie. [-5, -4, -3, -2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_of_prices_xmin_before(datetime_now, x_min, stock_price_merged):\n",
    "    earliest_time = (datetime_now - timedelta(minutes = x_min)).time()\n",
    "    if earliest_time >= datetime.time(4, 0):\n",
    "        result = []\n",
    "        for min_interval in range(x_min, 0, -1):\n",
    "            datetime_query = datetime_now - timedelta(minutes = min_interval)\n",
    "            price = stock_price_merged[stock_price_merged['datetime_now'] == datetime_query].iloc[0, stock_price_merged.columns.get_loc('price_now')]\n",
    "            result.append(price)\n",
    "        print(f\"{datetime_now}: {result}\")\n",
    "        return result\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-28 10:50:00: [260.96, 261.03, 261.01, 261.015, 261.04, 261.01, 261.02, 261.01, 260.955, 260.94, 260.94, 260.97, 260.97900000000004, 261.01, 261.015, 261.03, 261.08, 261.08, 261.075, 261.125, 261.13, 261.17, 261.105, 261.0861, 261.09, 261.15, 261.11, 261.1, 261.09, 261.0825]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-9a3751b0727f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'prev_30mins_prices'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_array_of_prices_xmin_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_price_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tweets_stocks_combined_final.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4211\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4212\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   4195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4196\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4197\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-0f21d5853744>\u001b[0m in \u001b[0;36mget_array_of_prices_xmin_before\u001b[0;34m(datetime_now, x_min, stock_price_merged)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmin_interval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mdatetime_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime_now\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_price_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock_price_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime_now'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdatetime_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_price_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'price_now'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime_now}: {result}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "merged[f'prev_30mins_prices'] = merged['tweet_datetime'].apply(get_array_of_prices_xmin_before, args=(30, stock_price_merged))\n",
    "merged.to_csv(f\"tweets_stocks_combined_final.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
