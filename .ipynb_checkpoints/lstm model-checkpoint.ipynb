{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2612 entries, 0 to 2611\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      2612 non-null   float64\n",
      " 1   text                    2562 non-null   object \n",
      " 2   favorites               2612 non-null   int64  \n",
      " 3   retweets                2612 non-null   int64  \n",
      " 4   date                    2612 non-null   object \n",
      " 5   tweet_datetime          2612 non-null   object \n",
      " 6   date_part               2612 non-null   object \n",
      " 7   time_part               2612 non-null   object \n",
      " 8   hour                    2612 non-null   int64  \n",
      " 9   year                    2612 non-null   int64  \n",
      " 10  month                   2612 non-null   int64  \n",
      " 11  datetime_15mins_after   2612 non-null   object \n",
      " 12  price_15mins_after      2612 non-null   float64\n",
      " 13  datetime_now            2612 non-null   object \n",
      " 14  price_now               2612 non-null   float64\n",
      " 15  15mins_price_diff_abs   2612 non-null   float64\n",
      " 16  15mins_price_diff_perc  2612 non-null   float64\n",
      " 17  prev_30mins_prices      2474 non-null   object \n",
      "dtypes: float64(5), int64(5), object(8)\n",
      "memory usage: 367.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_for_lstm.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>datetime_15mins_after</th>\n",
       "      <th>price_15mins_after</th>\n",
       "      <th>datetime_now</th>\n",
       "      <th>price_now</th>\n",
       "      <th>15mins_price_diff_abs</th>\n",
       "      <th>15mins_price_diff_perc</th>\n",
       "      <th>prev_30mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-11-28 11:05:00</td>\n",
       "      <td>261.150000</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>261.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>[260.96, 261.03, 261.01, 261.015, 261.04, 261....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>join me live from fort myer in arlington virginia</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>2017-08-22 09:15:00</td>\n",
       "      <td>243.630000</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>[243.76, 243.79, 243.85, 243.86, 243.81, 243.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-05-09 07:16:00</td>\n",
       "      <td>239.940000</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>239.875000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>[239.73, 239.73, 239.73, 239.73, 239.73, 239.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-08 04:59:00</td>\n",
       "      <td>236.915000</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>236.880000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>[236.84, 236.84, 236.84, 236.85333333333332, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.778460e+17</td>\n",
       "      <td>great night in iowa special people thank you</td>\n",
       "      <td>56446</td>\n",
       "      <td>8039</td>\n",
       "      <td>2017-06-22 11:11:00</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>19:11:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-22 19:26:00</td>\n",
       "      <td>242.893333</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>242.880000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>[242.85, 242.85, 242.85, 242.85, 242.85, 242.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-05-23 09:28:00</td>\n",
       "      <td>271.110000</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>271.040000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>[271.18, 271.16, 271.18, 271.15, 271.08, 271.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>so general michael flynns life can be totally ...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-20 18:49:00</td>\n",
       "      <td>266.890000</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>266.820000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>[266.74625000000003, 266.7475, 266.74875, 266....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-20 06:45:00</td>\n",
       "      <td>268.910000</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>268.620000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>[268.77, 268.74333333333334, 268.7166666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>todays court decision means that congress must...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-18 05:49:00</td>\n",
       "      <td>270.710000</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>[270.73, 270.7, 270.72749999999996, 270.755, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-29 05:46:00</td>\n",
       "      <td>261.065000</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>260.982857</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>[260.82, 260.89, 260.9, 260.87666666666667, 26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0     9.353400e+17                                     thank you rand   \n",
       "1     8.997980e+17  join me live from fort myer in arlington virginia   \n",
       "2     8.939700e+17                                   thank you nicole   \n",
       "3     8.819770e+17     thank you to shawn steel for the nice words on   \n",
       "4     8.778460e+17       great night in iowa special people thank you   \n",
       "...            ...                                                ...   \n",
       "2607  9.990960e+17  if the person placed very early into my campai...   \n",
       "2608  9.874600e+17  so general michael flynns life can be totally ...   \n",
       "2609  9.870960e+17  my thoughts prayers and condolences are with t...   \n",
       "2610  9.863570e+17  todays court decision means that congress must...   \n",
       "2611  9.791090e+17  i am pleased to announce that i intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4         56446      8039  2017-06-22 11:11:00  2017-06-22 19:11:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2607      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2608      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2609      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2610      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2611      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month datetime_15mins_after  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   2017-11-28 11:05:00   \n",
       "1     2017-08-22  09:00:00     9  2017      8   2017-08-22 09:15:00   \n",
       "2     2017-05-09  07:01:00     7  2017      5   2017-05-09 07:16:00   \n",
       "3     2017-03-08  04:44:00     4  2017      3   2017-03-08 04:59:00   \n",
       "4     2017-06-22  19:11:00    19  2017      6   2017-06-22 19:26:00   \n",
       "...          ...       ...   ...   ...    ...                   ...   \n",
       "2607  2018-05-23  09:13:00     9  2018      5   2018-05-23 09:28:00   \n",
       "2608  2018-04-20  18:34:00    18  2018      4   2018-04-20 18:49:00   \n",
       "2609  2018-04-20  06:30:00     6  2018      4   2018-04-20 06:45:00   \n",
       "2610  2018-04-18  05:34:00     5  2018      4   2018-04-18 05:49:00   \n",
       "2611  2018-03-29  05:31:00     5  2018      3   2018-03-29 05:46:00   \n",
       "\n",
       "      price_15mins_after         datetime_now   price_now  \\\n",
       "0             261.150000  2017-11-28 10:50:00  261.100000   \n",
       "1             243.630000  2017-08-22 09:00:00  243.670000   \n",
       "2             239.940000  2017-05-09 07:01:00  239.875000   \n",
       "3             236.915000  2017-03-08 04:44:00  236.880000   \n",
       "4             242.893333  2017-06-22 19:11:00  242.880000   \n",
       "...                  ...                  ...         ...   \n",
       "2607          271.110000  2018-05-23 09:13:00  271.040000   \n",
       "2608          266.890000  2018-04-20 18:34:00  266.820000   \n",
       "2609          268.910000  2018-04-20 06:30:00  268.620000   \n",
       "2610          270.710000  2018-04-18 05:34:00  270.600000   \n",
       "2611          261.065000  2018-03-29 05:31:00  260.982857   \n",
       "\n",
       "      15mins_price_diff_abs  15mins_price_diff_perc  \\\n",
       "0                  0.050000                0.000191   \n",
       "1                 -0.040000               -0.000164   \n",
       "2                  0.065000                0.000271   \n",
       "3                  0.035000                0.000148   \n",
       "4                  0.013333                0.000055   \n",
       "...                     ...                     ...   \n",
       "2607               0.070000                0.000258   \n",
       "2608               0.070000                0.000262   \n",
       "2609               0.290000                0.001080   \n",
       "2610               0.110000                0.000407   \n",
       "2611               0.082143                0.000315   \n",
       "\n",
       "                                     prev_30mins_prices  \n",
       "0     [260.96, 261.03, 261.01, 261.015, 261.04, 261....  \n",
       "1     [243.76, 243.79, 243.85, 243.86, 243.81, 243.7...  \n",
       "2     [239.73, 239.73, 239.73, 239.73, 239.73, 239.7...  \n",
       "3     [236.84, 236.84, 236.84, 236.85333333333332, 2...  \n",
       "4     [242.85, 242.85, 242.85, 242.85, 242.85, 242.8...  \n",
       "...                                                 ...  \n",
       "2607  [271.18, 271.16, 271.18, 271.15, 271.08, 271.0...  \n",
       "2608  [266.74625000000003, 266.7475, 266.74875, 266....  \n",
       "2609  [268.77, 268.74333333333334, 268.7166666666666...  \n",
       "2610  [270.73, 270.7, 270.72749999999996, 270.755, 2...  \n",
       "2611  [260.82, 260.89, 260.9, 260.87666666666667, 26...  \n",
       "\n",
       "[2562 rows x 18 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X = model_a_df.loc[:, 'text']\n",
    "model_a_y = model_a_df.loc[:, '15mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train, model_a_X_test, model_a_y_train, model_a_y_test = train_test_split(model_a_X, model_a_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_corpus_list = []\n",
    "\n",
    "for i in model_a_X_train:\n",
    "    model_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(model_a_corpus_list, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_num_words = [len(i) for i in model_a_corpus_list]\n",
    "model_a_longest_sentence_len = max(model_a_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sentence_to_indices_padded(sentences, longest_sentence_len, word2vec_model):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train_padded = word2vec_sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len, model_a_word2vec_model)\n",
    "model_a_X_test_padded = word2vec_sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len, model_a_word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len, glove_model):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.100d.txt', encoding='utf8')\n",
    "glove_vocab = []\n",
    "glove_vocab_index = {}\n",
    "count = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    glove_vocab.append(word)\n",
    "    glove_vocab_index[word] = count\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    count += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1359, 62, 21, 1121, 62, 13, 1, 2550, 25, 1, 1360, 4, 28, 375, 120, 7, 75, 75, 75, 37, 19]\n",
      "Found 5169 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(model_a_X_train)\n",
    "sequences = tokenizer.texts_to_sequences(model_a_X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(sequences[1])\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_count = dict(tokenizer.word_counts)\n",
    "glove_sorted=dict(sorted(glove_word_count.items(), key=lambda x: x[1],reverse=True))\n",
    "print(glove_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in model_a_word2vec_model.wv.vocab:\n",
    "    w2c[item]=model_a_word2vec_model.wv.vocab[item].count\n",
    "w2c_sorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "print(w2c_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    global glove_vocab\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in glove_vocab:\n",
    "                indices.append(glove_vocab_index[word])\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_glove = glove_sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "x_test_glove = glove_sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5170\n",
      "100\n",
      "652\n",
      "['don’t', 'didn’t', '“the', 'it’s', 'doesn’t', 'can’t', 'trump”', 'america’s', 'he’s', 'strzok', 'kavanaugh', 'wasn’t', '“trump', 'that’s', 'couldn’t', '“president', 'americafirst', 'states”', 'aren’t', 'today’s', 'jongun', 'i’m', '“it', 'isn’t', 'shouldn’t', 'i’ve', '“this', 'trump’s', 'rosendale', 'balderson', 'magarally', 'denuclearization', 'we’re', 'jobsnotmobs', 'prstrong', 'wouldn’t', '“you', 'lawabiding', 'they’re', 'we’ve', 'maralago', '“a', 'taxreform', '“collusion”', 'you’re', 'cryin’', 'weren’t', 'councel', 'they’ve', '“i', 'uswomensopen', 'times”', 'happyindependenceday', 'workers”', '“sources”', '“fbi', 'stop”', 'there’s', 'flynn’s', 'he’ll', '“it’s', '“when', 'country’s', 'haven’t', 'hurricanemichael', 'potusabroad', '“these', 'clinton’s', '“they', '“consumer', 'singaporesummit', 'here’s', '“presidential', 'hillary’s', 'sampp', '“o”', 'won’t', 'r’s', 'sheriff’s', '“thank', 'blakeman', 'america”', 'votekarenhandel', '“there', 'obama’s', 'party’s', 'covfefe', 'usmca', '“justice”', 'gillum', 'brennan’s', 'people”', '“special”', 'something’s', 'antitrump', 'world’s', 'indopacific', 'scalise', '“bruce', 'ohr’s', 'i’ll', '“what', 'correspondents’', 'expropriations', '“south', 'farmers”', 'nation’s', 'daines', 'counterintelligence', '“anonymous', 'sources”', 'news”', 'cajunnavy', 'wilsond', 'coleading', 'secretarygeneral', '“make', 'again”', 'militaryveterans', 'upfake', 'antius', 'magajobsnotmobs', 'buildthewall', 'worldautismawarenessday', 'liub', 'woolsey', '“pledge', '“during', 'foundation”', 'priding', 'raffensperger', '“press”', 'puertoricowith', 'badmouths', 'harm’s', 'east”', 'securitya', 'democrats’', 'authorizescont', 'unverifiable”', 'strassel', 'there”', 'carafano', 'blagojevich', 'comeys', 'worker”', 'coauthor', '“trumponomics”', '“best', 'gains”', 'nonwalled', 'greenvillespartanburg', 'campaign”', '“please', 'war”', 'meet”', 'maria”', 'killed”', 'onceinageneration', 'grandstander', 'storyopinion', 'hurricaneflorence', 'books”', '“judge', 'court”', '“private', 'adp”', 'extinguishers”', 'wstate', 'agentlover', 'scjustice', 'reassigned”', 'ocarenightmare', 'youlesm', '“greenbrier', 'dinner”', 'yesterday’s', '“big', 'one”', 'hardearned', 'career”', 'shamrockbowl', 'varadkar', 'house”', 'roomtoday', 'changedcomplete', 'kate’s', 'againpotusabroad', 'repealandreplace', 'jobcreating', 'storm’s', 'tillerson', 'exposeda', 'days”', 'economy”', '“bet', 'force”', 'gentiloni', '”this', 'criminals”', 'stopthebias', 'progrowth', 'riyadhsummit', 'atampt', 'vamissionact', 'government’s', 'loveragent', '“overseas”', 'shuette', '“his', 'cubanamerican', 'flagstand', '“senator”', 'sobbingly', 'warfighters', 'tradecraft', '“mentally', 'retarded”', 'southerner”', 'opportunity”', 'rothfus', 'republicansgot', 'teamusaon', 'exfbi', '“purge”', 'firstrespondersday', 'suv’s', 'presidency”', 'citiesboth', 'recordhigh', 'billionyear', 'happenalso', 'houstonstrong', 'poroshenko', 'before”', 'bordersthey', 'makeamericagreatagain', 'liyuan', '“mr', 'candidates”', 'fandos', 'highand', 'amvets', 'prohibitions', 'lawenforcementappreciationday', 'senate’s', 'nationsusaatungaunga', '“resist”', '“obstruct”', '“forever”', 'voteralphnorman', 'vfwconvention', 'thingsbut', 'cubavideo', 'tarkanian', 'mexico’s', 'issawith', 'vetshas', 'northamwho', 'virginiais', 'wiedefeld', 'todayhappythanksgiving', '“had', 'securitycharles', 'mcculloughfmr', 'livescharlottesville', 'outand', 'cubamemorandum', 'opioidepidemicfull', 'violationall', 'crimesbecause', 'sessions”', 'parentteacher', 'else”', 'country”', '“trump’s', 'speech”', 'trumptime', '“no', 'steele”', 'bordersthe', 'ourselvesnot', 'jurists', 'bastilleday', 'hunt”', 'august”', '“paper', 'trail”', '“private”', 'winour', '“media”', 'commanderinchiefs', 'writein', 'kustoff', 'lowerpriced', 'hispanicamericans', 'hispanicamerican', '“we’re', 'great”', 'you’ve', 'abcwashington', '“ship', 'fools”', 'dodgersred', 'shellacked', 'congressionalbaseballgame', 'cutreform', 'otherwiseand', 'ussfitzgerald', 'mindtruly', 'ndsen', 'unredacted', 'yearmost', 'we’d', 'rescissions', '“infestation”', 'elelments', 'leakin’', 'wife’s', 'georgiabad', 'jobkilling', 'partnersduring', 'prince’s', 'jobsborder', 'securityisis', '“manufacturing', 'survey”', 'profitsthere', 'watch”', 'morici', 'happyhanukkah', 'specificbut', 'ukraineanother', '“unprecedented', 'rise”', '“more', 'gop”', 'distroys', 'crimes”', '“hillary', 'digenova', 'kateslaw', 'me”', 'bookbring', 'justin’s', 'countrystandforouranthem', '“acid', 'washed”', '“pastor', 'problack', 'event”', 'state”', 'lavar’s', 'man’s', 'trishregan', '“did', 'weaponizing', 'gain”', '“china', 'server”', '“director', 'oppositedisgraceful', 'trumpstyle', 'ussjohnsmccain', 'aideit', 'campaignpeople', 'spyingthis', 'scandalous”', 'peaceofficersmemorialday', 'andpoliceweek', 'is”', '“ohr', '“media', '“fake', 'commanderinchief', '“obama', 'judges”', '“independent', 'judiciary”', 'magarallyin', 'countries’', 'racefake', 'heavytruck', 'nationfull', 'usss', 'ossoff', 'jeanclaude', 'trumppence', '“boom', 'high”', 'trouncingthe', 'magatickets', '“john', 'this”', '“spygate', 'china’s', 'tarrifs', 'barriersalso', '“liars', 'conspiracy”', 'russiatrump', 'pelosi’s', 'hearby', 'rv’s', 'president”', 'shepherding', 'generalsecretary', 'wins”', 'code”', 'credit”', 'staminai', '“department', 'considerations”', '“other', 'side”', 'vasecretary', 'shulkin', 'europe’s', 'unverifiable', 'trumprussia', 'convictions”', 'tigersfull', 'hurricaneirma', 'hillarythey', 'cutsmakeamericagreatagain', 'ingloriously', 'relationshipbut', 'importantbut', '“out', 'mind”', 'antisecond', 'yearfour', 'thatstory', '“martin', 'act”', 'redesignates', '“why', 'offensesconstitution', 'thisstormtrooper', 'almost”', '“heart”', 'goodlatte', 'infrastructureweek', 'wdiverse', 'playbookcall', 'excoriated', 'lawthe', 'investigators”', 'astonishingthat', '“how', 'place”', '“richard', 'hero”', 'americaexecutive', 'hillaryclinton', 'justicedepartment', 'terroristalbaghdaditheir', 'jobsand', 'commanderinchief’s', '“if', 'scam”', 'whope', 'exonerating', '“there’s', 'weissman', 'terrorismrelated', 'foreignborn', 'taxcutsandjobsact', 'placesspent', 'differentthat’s', 'elected”', 'colmery', 'greatgrandchildren', 'satisfiedtruly', 'lifecont', 'texasstrong', '‘caravan’', 'placethe', 'lifemerry', 'extortionist', 'hacking”', 'centuryofservice', 'ncaachampionsphotos', 'jfkfiles', 'shulkin’s', 'happened”', 'hydesmith', 'wowthe', 'possiblefar', '“a”and', 'disgracedraintheswamp', 'destinybut', 'armynavygame', '“unsolved', 'mystery”', 'autismawarenessday', 'lightitupblue', 'wyou', 'year’s', 'presidentelect', 'comey’s', 'collusion”', 'trumponomics', 'longheld', 'wellwishers', 'anthemrespect', 'murl', 'workforceweek', '“tariffed”', 'yougod', 'year”', 'showdavid', 'congratspeggy', 'alabamathat', 'countrybipartisan', 'godblesstheusa', 'american’s', 'gratefullawenforcementappreciationdaypresident', 'happened”“how', 'russia”', 'annie’s', 'congratulationshave', 'officer”', 'fbi’s', 'strzokpage', '“nuclear', 'ohio’s', 'opioidepidemic', 'againamericafirst', 'poll”', 'giaritelli', 'ussarizona', 'honorthemremarks', 'goal”rob', 'goldmanvice', 'ripevelyn', 'incrediblethanks', 'frankenstien', 'presidential”', 'autoworkers', 'admonished', '“peanuts”', 'ribbonno', 'thisobviously', 'rexnord', 'boomingbut', 'mulvaney', 'couched', 'journalism”', 'she’s', '“barrack', 'what’s', 'dream”', 'memembers', 'governorsi', 'gps”', '“fakers”', 'ssteel', '“leadership”', 'u”', 'melania’s', 'zinke', 'alike“remember', 'harbor”', 'circuit”', 'haspel', '“papers”', 'unelectable', '“mike”', '“toast”', 'teachers”', 'opioidcrisis', '“national', 'soon”', 'lineitem', 'staes', 'republicansremember', 'membersnot', 'magarallytonight', 'gobbler’s', 'hcare', 'marineone', 'shinzō', '“us', 'insulting”', '“will', 'around”', 'koreanationalassembly', 'potusinasia', 'cantbut', 'canamp', 'teamscalise', 'aftermaththe', 'dossierthe', 'breach”', 'medalofhonor', 'slabinski', 'overcomeeven', '“secretary', 'allegations”', 'viewingreporting', 'republicanconservative', 'canley', '“we’ll', 'who’s', '“i’d', 'gaetz', 'uskorea', 'interdictact', '“flood', 'gates”', 'let’s', 'democratled', '“comedian”', 'linesmuch', 'builthas', 'mansioncompound', 'rightthe', 'mistake”', 'countrytell', '“border', 'wall”', 'democrat’s', 'stateno', 'weeklyaddress', 'commmitment', 'nationsmet', 'discgrace', '“an', 'outrageous”', 'draintheswamp', '“former', 'dossier”', 'wilkesbarre', '“releasing”', 'situationwill', 'xinping', 'lesm', '“russians', 'disgraceand', 'magarallyreplay', '“capitalist', 'comeback”', 'nonmonetary', '“some', 'known”', 'andeavor', 'mattarella', 'russiarussia', '“mainstream', 'depression”', 'cutsreform']\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "        \n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "\n",
    "# print(vocab_size_glove)\n",
    "# print(embedding_size_glove)\n",
    "# print(count)\n",
    "# print(skipped_words)\n",
    "\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=model_a_longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_2seq_tanh(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_1seq_tanh(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_1seq_sigmoid(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_2seq_sigmoid(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTMs, tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 159, 100)          522200    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 159, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 159, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 524,049\n",
      "Trainable params: 1,849\n",
      "Non-trainable params: 522,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a_2seq_tanh = create_model_a_2seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_2seq_tanh.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1372 samples, validate on 344 samples\n",
      "Epoch 1/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0482\n",
      "Epoch 00001: val_loss improved from inf to 0.00469, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 8s 6ms/sample - loss: 0.0071 - mae: 0.0477 - val_loss: 0.0047 - val_mae: 0.0378\n",
      "Epoch 2/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0391\n",
      "Epoch 00002: val_loss improved from 0.00469 to 0.00457, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0389 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 3/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0382\n",
      "Epoch 00003: val_loss did not improve from 0.00457\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0386 - val_loss: 0.0046 - val_mae: 0.0357\n",
      "Epoch 4/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0380\n",
      "Epoch 00004: val_loss improved from 0.00457 to 0.00456, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0384 - val_loss: 0.0046 - val_mae: 0.0344\n",
      "Epoch 5/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0378- ETA: 0s - loss: 0.0067 - mae: \n",
      "Epoch 00005: val_loss improved from 0.00456 to 0.00456, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0343\n",
      "Epoch 6/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0381\n",
      "Epoch 00006: val_loss did not improve from 0.00456\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0397\n",
      "Epoch 7/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0388\n",
      "Epoch 00007: val_loss did not improve from 0.00456\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0065 - mae: 0.0388 - val_loss: 0.0046 - val_mae: 0.0353\n",
      "Epoch 8/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0391\n",
      "Epoch 00008: val_loss did not improve from 0.00456\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0390 - val_loss: 0.0047 - val_mae: 0.0387\n",
      "Epoch 9/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0408\n",
      "Epoch 00009: val_loss improved from 0.00456 to 0.00455, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0409 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 10/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0393\n",
      "Epoch 00010: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0396 - val_loss: 0.0046 - val_mae: 0.0371\n",
      "Epoch 11/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0381\n",
      "Epoch 00011: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0345\n",
      "Epoch 12/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0381\n",
      "Epoch 00012: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0063 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0355\n",
      "Epoch 13/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0392\n",
      "Epoch 00013: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0390 - val_loss: 0.0047 - val_mae: 0.0384\n",
      "Epoch 14/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0381\n",
      "Epoch 00014: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 15/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0374\n",
      "Epoch 00015: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0375 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 16/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0374\n",
      "Epoch 00016: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 17/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0381\n",
      "Epoch 00017: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0384 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 18/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0372\n",
      "Epoch 00018: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0372 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 19/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0393\n",
      "Epoch 00019: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0389 - val_loss: 0.0046 - val_mae: 0.0344\n",
      "Epoch 20/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0373\n",
      "Epoch 00020: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0374 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 21/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0379\n",
      "Epoch 00021: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0363\n",
      "Epoch 22/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0376\n",
      "Epoch 00022: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0376 - val_loss: 0.0046 - val_mae: 0.0359\n",
      "Epoch 23/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0376\n",
      "Epoch 00023: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 24/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0378\n",
      "Epoch 00024: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 25/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0369\n",
      "Epoch 00025: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0063 - mae: 0.0371 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 26/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0374\n",
      "Epoch 00026: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0380 - val_loss: 0.0046 - val_mae: 0.0366\n",
      "Epoch 27/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0375\n",
      "Epoch 00027: val_loss improved from 0.00455 to 0.00455, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1852h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0374 - val_loss: 0.0045 - val_mae: 0.0343\n",
      "Epoch 28/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0375\n",
      "Epoch 00028: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0376 - val_loss: 0.0045 - val_mae: 0.0344\n",
      "Epoch 29/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0379\n",
      "Epoch 00029: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0063 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0379\n",
      "Epoch 00030: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 31/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0376\n",
      "Epoch 00031: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0371 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 32/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0375\n",
      "Epoch 00032: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0373 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 33/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0360\n",
      "Epoch 00033: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0368 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 34/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0373\n",
      "Epoch 00034: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0371 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 35/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0376\n",
      "Epoch 00035: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0375 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 36/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0382\n",
      "Epoch 00036: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0366\n",
      "Epoch 37/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0376\n",
      "Epoch 00037: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0375 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 38/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0373\n",
      "Epoch 00038: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0374 - val_loss: 0.0046 - val_mae: 0.0353\n",
      "Epoch 39/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0372\n",
      "Epoch 00039: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0373 - val_loss: 0.0046 - val_mae: 0.0342\n",
      "Epoch 40/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0374\n",
      "Epoch 00040: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0373 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 41/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0372- ETA: 1s - loss:\n",
      "Epoch 00041: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0371 - val_loss: 0.0046 - val_mae: 0.0343\n",
      "Epoch 42/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0383\n",
      "Epoch 00042: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0380 - val_loss: 0.0046 - val_mae: 0.0354\n",
      "Epoch 43/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0379- ETA: 2\n",
      "Epoch 00043: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 44/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0372\n",
      "Epoch 00044: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0373 - val_loss: 0.0046 - val_mae: 0.0351\n",
      "Epoch 45/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0379\n",
      "Epoch 00045: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0376 - val_loss: 0.0046 - val_mae: 0.0343\n",
      "Epoch 46/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0372\n",
      "Epoch 00046: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0369 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 47/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0378\n",
      "Epoch 00047: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 48/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0372- ETA: 2s - l\n",
      "Epoch 00048: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0373 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 49/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0374\n",
      "Epoch 00049: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0376 - val_loss: 0.0046 - val_mae: 0.0345\n",
      "Epoch 50/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0369\n",
      "Epoch 00050: val_loss did not improve from 0.00455\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0064 - mae: 0.0376 - val_loss: 0.0046 - val_mae: 0.0364\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_2seq_tanh {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_2seq_tanh_history = model_a_2seq_tanh.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00152868],\n",
       "       [0.00139915],\n",
       "       [0.00167112],\n",
       "       [0.00160962],\n",
       "       [0.00176441],\n",
       "       [0.00155087],\n",
       "       [0.00176477],\n",
       "       [0.00158254],\n",
       "       [0.00166291],\n",
       "       [0.00117626],\n",
       "       [0.0019331 ],\n",
       "       [0.00129923],\n",
       "       [0.00152098],\n",
       "       [0.0023543 ],\n",
       "       [0.00176979],\n",
       "       [0.00166193],\n",
       "       [0.00170861],\n",
       "       [0.00178246],\n",
       "       [0.00175226],\n",
       "       [0.0018637 ],\n",
       "       [0.00168709],\n",
       "       [0.00159256],\n",
       "       [0.00176223],\n",
       "       [0.00174286],\n",
       "       [0.00179702],\n",
       "       [0.0016817 ],\n",
       "       [0.00163848],\n",
       "       [0.0013097 ],\n",
       "       [0.00196105],\n",
       "       [0.00175423],\n",
       "       [0.00161763],\n",
       "       [0.00173057],\n",
       "       [0.00166576],\n",
       "       [0.00188478],\n",
       "       [0.00144834],\n",
       "       [0.001554  ],\n",
       "       [0.00167813],\n",
       "       [0.00149864],\n",
       "       [0.00187939],\n",
       "       [0.00094656],\n",
       "       [0.00123483],\n",
       "       [0.00162814],\n",
       "       [0.00164099],\n",
       "       [0.00197306],\n",
       "       [0.00042266],\n",
       "       [0.00154643],\n",
       "       [0.00118938],\n",
       "       [0.00144887],\n",
       "       [0.00152732],\n",
       "       [0.00174805],\n",
       "       [0.0017484 ],\n",
       "       [0.00180172],\n",
       "       [0.0016766 ],\n",
       "       [0.00196893],\n",
       "       [0.00079587],\n",
       "       [0.00150959],\n",
       "       [0.00180878],\n",
       "       [0.0019298 ],\n",
       "       [0.0019268 ],\n",
       "       [0.00141755],\n",
       "       [0.001797  ],\n",
       "       [0.00163388],\n",
       "       [0.00124711],\n",
       "       [0.00170918],\n",
       "       [0.00175626],\n",
       "       [0.00154678],\n",
       "       [0.00143066],\n",
       "       [0.00194055],\n",
       "       [0.00112736],\n",
       "       [0.00117017],\n",
       "       [0.00156523],\n",
       "       [0.00185251],\n",
       "       [0.00161622],\n",
       "       [0.00137524],\n",
       "       [0.00167985],\n",
       "       [0.00174568],\n",
       "       [0.00152011],\n",
       "       [0.00140887],\n",
       "       [0.00181018],\n",
       "       [0.00076144],\n",
       "       [0.00138183],\n",
       "       [0.00181429],\n",
       "       [0.00195561],\n",
       "       [0.00184368],\n",
       "       [0.00200174],\n",
       "       [0.00167453],\n",
       "       [0.00133733],\n",
       "       [0.00112124],\n",
       "       [0.00171045],\n",
       "       [0.00142439],\n",
       "       [0.00181949],\n",
       "       [0.00134496],\n",
       "       [0.00167911],\n",
       "       [0.00146776],\n",
       "       [0.00174154],\n",
       "       [0.00162226],\n",
       "       [0.00150352],\n",
       "       [0.00141812],\n",
       "       [0.00188839],\n",
       "       [0.00160467],\n",
       "       [0.00141357],\n",
       "       [0.0019635 ],\n",
       "       [0.00141096],\n",
       "       [0.00123542],\n",
       "       [0.00208691],\n",
       "       [0.0015632 ],\n",
       "       [0.00192883],\n",
       "       [0.00196668],\n",
       "       [0.0013774 ],\n",
       "       [0.00176112],\n",
       "       [0.00217119],\n",
       "       [0.00146724],\n",
       "       [0.00125224],\n",
       "       [0.00139378],\n",
       "       [0.00191019],\n",
       "       [0.0018088 ],\n",
       "       [0.0019221 ],\n",
       "       [0.00088708],\n",
       "       [0.00153878],\n",
       "       [0.00151677],\n",
       "       [0.00175879],\n",
       "       [0.00170481],\n",
       "       [0.00078787],\n",
       "       [0.00142883],\n",
       "       [0.00167153],\n",
       "       [0.00208953],\n",
       "       [0.0012852 ],\n",
       "       [0.00163639],\n",
       "       [0.00132807],\n",
       "       [0.00137876],\n",
       "       [0.00167882],\n",
       "       [0.00206154],\n",
       "       [0.00148878],\n",
       "       [0.00162168],\n",
       "       [0.00143901],\n",
       "       [0.00162634],\n",
       "       [0.00140152],\n",
       "       [0.00154711],\n",
       "       [0.00154631],\n",
       "       [0.00162796],\n",
       "       [0.0015114 ],\n",
       "       [0.00123373],\n",
       "       [0.00171382],\n",
       "       [0.00143608],\n",
       "       [0.00189384],\n",
       "       [0.00135229],\n",
       "       [0.00105673],\n",
       "       [0.00172487],\n",
       "       [0.00179861],\n",
       "       [0.00162412],\n",
       "       [0.00164551],\n",
       "       [0.00141554],\n",
       "       [0.0020147 ],\n",
       "       [0.001709  ],\n",
       "       [0.00159532],\n",
       "       [0.00194298],\n",
       "       [0.00198636],\n",
       "       [0.00202832],\n",
       "       [0.00170265],\n",
       "       [0.00136431],\n",
       "       [0.00173491],\n",
       "       [0.00138996],\n",
       "       [0.00167822],\n",
       "       [0.00174896],\n",
       "       [0.00189384],\n",
       "       [0.00170713],\n",
       "       [0.00080915],\n",
       "       [0.00150967],\n",
       "       [0.00190067],\n",
       "       [0.00172469],\n",
       "       [0.00167676],\n",
       "       [0.00169156],\n",
       "       [0.00175166],\n",
       "       [0.00143312],\n",
       "       [0.00105668],\n",
       "       [0.00178453],\n",
       "       [0.00184381],\n",
       "       [0.00179203],\n",
       "       [0.00167744],\n",
       "       [0.00178574],\n",
       "       [0.00160477],\n",
       "       [0.00146843],\n",
       "       [0.00164998],\n",
       "       [0.00170162],\n",
       "       [0.00200746],\n",
       "       [0.00172429],\n",
       "       [0.00185127],\n",
       "       [0.00184323],\n",
       "       [0.00168756],\n",
       "       [0.00191362],\n",
       "       [0.00160465],\n",
       "       [0.00106163],\n",
       "       [0.00143008],\n",
       "       [0.00212453],\n",
       "       [0.00189088],\n",
       "       [0.00178428],\n",
       "       [0.00067581],\n",
       "       [0.00119497],\n",
       "       [0.00170081],\n",
       "       [0.00151844],\n",
       "       [0.00173482],\n",
       "       [0.00170659],\n",
       "       [0.00153138],\n",
       "       [0.00121559],\n",
       "       [0.00141735],\n",
       "       [0.00174985],\n",
       "       [0.00171107],\n",
       "       [0.00151387],\n",
       "       [0.00039119],\n",
       "       [0.0017429 ],\n",
       "       [0.00168965],\n",
       "       [0.00173522],\n",
       "       [0.00160098],\n",
       "       [0.00105052],\n",
       "       [0.00168197],\n",
       "       [0.00150863],\n",
       "       [0.00174877],\n",
       "       [0.0020344 ],\n",
       "       [0.00189177],\n",
       "       [0.00129287],\n",
       "       [0.00175029],\n",
       "       [0.0016227 ],\n",
       "       [0.00155697],\n",
       "       [0.00210151],\n",
       "       [0.0015386 ],\n",
       "       [0.00164896],\n",
       "       [0.00143971],\n",
       "       [0.0017396 ],\n",
       "       [0.00111742],\n",
       "       [0.00156074],\n",
       "       [0.00190783],\n",
       "       [0.00218134],\n",
       "       [0.00163194],\n",
       "       [0.00132818],\n",
       "       [0.00168175],\n",
       "       [0.00156941],\n",
       "       [0.00129946],\n",
       "       [0.00124162],\n",
       "       [0.00171745],\n",
       "       [0.00111164],\n",
       "       [0.00142092],\n",
       "       [0.00143195],\n",
       "       [0.00203375],\n",
       "       [0.0013488 ],\n",
       "       [0.00142404],\n",
       "       [0.00179375],\n",
       "       [0.00145051],\n",
       "       [0.00186294],\n",
       "       [0.00147177],\n",
       "       [0.00173072],\n",
       "       [0.0019272 ],\n",
       "       [0.00170174],\n",
       "       [0.00174569],\n",
       "       [0.00158963],\n",
       "       [0.00134362],\n",
       "       [0.00115014],\n",
       "       [0.00125255],\n",
       "       [0.00151795],\n",
       "       [0.0017032 ],\n",
       "       [0.00177659],\n",
       "       [0.00171038],\n",
       "       [0.0018027 ],\n",
       "       [0.00177417],\n",
       "       [0.00168569],\n",
       "       [0.00185811],\n",
       "       [0.00111005],\n",
       "       [0.00173446],\n",
       "       [0.00187428],\n",
       "       [0.00193287],\n",
       "       [0.00164714],\n",
       "       [0.00125327],\n",
       "       [0.00197525],\n",
       "       [0.00152061],\n",
       "       [0.00212091],\n",
       "       [0.00187018],\n",
       "       [0.00166479],\n",
       "       [0.0019984 ],\n",
       "       [0.00119481],\n",
       "       [0.00159521],\n",
       "       [0.00144998],\n",
       "       [0.00185603],\n",
       "       [0.00184177],\n",
       "       [0.00160467],\n",
       "       [0.00175241],\n",
       "       [0.00134978],\n",
       "       [0.00178946],\n",
       "       [0.00179557],\n",
       "       [0.00175362],\n",
       "       [0.00106199],\n",
       "       [0.00063068],\n",
       "       [0.00139154],\n",
       "       [0.00143776],\n",
       "       [0.00130198],\n",
       "       [0.0017276 ],\n",
       "       [0.00168381],\n",
       "       [0.00129646],\n",
       "       [0.00100611],\n",
       "       [0.00146712],\n",
       "       [0.00123653],\n",
       "       [0.00158482],\n",
       "       [0.0013619 ],\n",
       "       [0.00159252],\n",
       "       [0.00125708],\n",
       "       [0.0019002 ],\n",
       "       [0.00171667],\n",
       "       [0.0015889 ],\n",
       "       [0.00110124],\n",
       "       [0.00180338],\n",
       "       [0.00124646],\n",
       "       [0.0020358 ],\n",
       "       [0.00152037],\n",
       "       [0.00162299],\n",
       "       [0.00175408],\n",
       "       [0.00192704],\n",
       "       [0.00102133],\n",
       "       [0.00162505],\n",
       "       [0.00173378],\n",
       "       [0.00170404],\n",
       "       [0.00179589],\n",
       "       [0.00163808],\n",
       "       [0.00121869],\n",
       "       [0.00141694],\n",
       "       [0.00134309],\n",
       "       [0.00197495],\n",
       "       [0.00168496],\n",
       "       [0.00189272],\n",
       "       [0.0014677 ],\n",
       "       [0.00168716],\n",
       "       [0.00176674],\n",
       "       [0.00212632],\n",
       "       [0.00149762],\n",
       "       [0.00109639],\n",
       "       [0.00145629],\n",
       "       [0.00122715],\n",
       "       [0.00101722],\n",
       "       [0.00207683],\n",
       "       [0.00149997],\n",
       "       [0.00170593],\n",
       "       [0.00166362],\n",
       "       [0.00179432],\n",
       "       [0.00175908],\n",
       "       [0.00150094],\n",
       "       [0.00164084],\n",
       "       [0.00195292],\n",
       "       [0.00192406],\n",
       "       [0.00159621],\n",
       "       [0.00202248],\n",
       "       [0.00205406],\n",
       "       [0.0015681 ],\n",
       "       [0.00172044],\n",
       "       [0.0016727 ],\n",
       "       [0.00142404],\n",
       "       [0.0020583 ],\n",
       "       [0.00171625],\n",
       "       [0.00159201],\n",
       "       [0.00152119],\n",
       "       [0.00151741],\n",
       "       [0.00140801],\n",
       "       [0.0016276 ],\n",
       "       [0.00148781],\n",
       "       [0.00189941],\n",
       "       [0.00158139],\n",
       "       [0.00129148],\n",
       "       [0.0023421 ],\n",
       "       [0.00138862],\n",
       "       [0.00168124],\n",
       "       [0.00205193],\n",
       "       [0.00219012],\n",
       "       [0.00181024],\n",
       "       [0.00201052],\n",
       "       [0.00115687],\n",
       "       [0.00129118],\n",
       "       [0.00160229],\n",
       "       [0.00162505],\n",
       "       [0.00178193],\n",
       "       [0.00165227],\n",
       "       [0.00079448],\n",
       "       [0.00139781],\n",
       "       [0.00155718],\n",
       "       [0.00170864],\n",
       "       [0.00144815],\n",
       "       [0.00155631],\n",
       "       [0.00143145],\n",
       "       [0.00144698],\n",
       "       [0.0015433 ],\n",
       "       [0.00162214],\n",
       "       [0.00146094],\n",
       "       [0.0015153 ],\n",
       "       [0.00187607],\n",
       "       [0.00176824],\n",
       "       [0.0020821 ],\n",
       "       [0.00177035],\n",
       "       [0.00158586],\n",
       "       [0.00136214],\n",
       "       [0.00114428],\n",
       "       [0.00198184],\n",
       "       [0.00150283],\n",
       "       [0.00151181],\n",
       "       [0.00162265],\n",
       "       [0.00155983],\n",
       "       [0.0015945 ],\n",
       "       [0.00162789],\n",
       "       [0.00127181],\n",
       "       [0.00184482],\n",
       "       [0.00122181],\n",
       "       [0.00111912],\n",
       "       [0.00176534],\n",
       "       [0.00121881],\n",
       "       [0.00169265],\n",
       "       [0.00144923],\n",
       "       [0.00177946],\n",
       "       [0.00149507],\n",
       "       [0.00155257],\n",
       "       [0.00193917],\n",
       "       [0.00154874],\n",
       "       [0.00151749],\n",
       "       [0.0017994 ],\n",
       "       [0.00191839],\n",
       "       [0.0019208 ],\n",
       "       [0.00120256],\n",
       "       [0.00144326],\n",
       "       [0.00178162],\n",
       "       [0.0014601 ],\n",
       "       [0.0015386 ],\n",
       "       [0.00178067],\n",
       "       [0.00129611],\n",
       "       [0.00200051],\n",
       "       [0.00136596],\n",
       "       [0.00214918],\n",
       "       [0.00164858],\n",
       "       [0.00160507],\n",
       "       [0.0019543 ],\n",
       "       [0.0013203 ],\n",
       "       [0.00155423],\n",
       "       [0.00143545],\n",
       "       [0.00142301],\n",
       "       [0.00196299],\n",
       "       [0.00202388],\n",
       "       [0.00192912],\n",
       "       [0.00164073],\n",
       "       [0.00156898],\n",
       "       [0.00170944],\n",
       "       [0.00133965],\n",
       "       [0.00187654],\n",
       "       [0.00210387],\n",
       "       [0.00172789],\n",
       "       [0.00154341],\n",
       "       [0.00152084],\n",
       "       [0.00151107],\n",
       "       [0.00162506],\n",
       "       [0.00183052],\n",
       "       [0.00164409],\n",
       "       [0.00133449],\n",
       "       [0.00201805],\n",
       "       [0.00081662],\n",
       "       [0.00181304],\n",
       "       [0.00177955],\n",
       "       [0.00143691],\n",
       "       [0.00116834],\n",
       "       [0.00135356],\n",
       "       [0.00153841],\n",
       "       [0.00179277],\n",
       "       [0.00172623],\n",
       "       [0.00167851],\n",
       "       [0.00068081],\n",
       "       [0.00144216],\n",
       "       [0.00203255],\n",
       "       [0.00162455],\n",
       "       [0.00182394],\n",
       "       [0.00157354],\n",
       "       [0.00153026],\n",
       "       [0.00195633],\n",
       "       [0.00165503],\n",
       "       [0.00181006],\n",
       "       [0.00156012],\n",
       "       [0.00135338],\n",
       "       [0.00194277],\n",
       "       [0.00162462],\n",
       "       [0.00079397],\n",
       "       [0.0011894 ],\n",
       "       [0.00141241],\n",
       "       [0.0016538 ],\n",
       "       [0.00157576],\n",
       "       [0.00154643],\n",
       "       [0.00183672],\n",
       "       [0.00148434],\n",
       "       [0.00136082],\n",
       "       [0.00139633],\n",
       "       [0.00167315],\n",
       "       [0.00162067],\n",
       "       [0.00169484],\n",
       "       [0.00141862],\n",
       "       [0.00123995],\n",
       "       [0.00123943],\n",
       "       [0.00174051],\n",
       "       [0.00161208],\n",
       "       [0.00157384],\n",
       "       [0.00164011],\n",
       "       [0.001187  ],\n",
       "       [0.00197603],\n",
       "       [0.00142588],\n",
       "       [0.0017765 ],\n",
       "       [0.00093898],\n",
       "       [0.00153263],\n",
       "       [0.00160342],\n",
       "       [0.00150017],\n",
       "       [0.00130222],\n",
       "       [0.00142676],\n",
       "       [0.00175871],\n",
       "       [0.00124495],\n",
       "       [0.00184271],\n",
       "       [0.00172833],\n",
       "       [0.00162015],\n",
       "       [0.00188843],\n",
       "       [0.00131725],\n",
       "       [0.00161769],\n",
       "       [0.00163759],\n",
       "       [0.00188423],\n",
       "       [0.00161177],\n",
       "       [0.00126356],\n",
       "       [0.00159732],\n",
       "       [0.00188118],\n",
       "       [0.00096037],\n",
       "       [0.00165057],\n",
       "       [0.00138506],\n",
       "       [0.00176607],\n",
       "       [0.00160768],\n",
       "       [0.00147071],\n",
       "       [0.00162369],\n",
       "       [0.00145922],\n",
       "       [0.00190343],\n",
       "       [0.00194071],\n",
       "       [0.00167111],\n",
       "       [0.00139319],\n",
       "       [0.00191685],\n",
       "       [0.00209209],\n",
       "       [0.00114141],\n",
       "       [0.00152699],\n",
       "       [0.00130161],\n",
       "       [0.00167886],\n",
       "       [0.00185127],\n",
       "       [0.00204747],\n",
       "       [0.00170119],\n",
       "       [0.00140913],\n",
       "       [0.00181529],\n",
       "       [0.0019489 ],\n",
       "       [0.00118505],\n",
       "       [0.00169944],\n",
       "       [0.00155211],\n",
       "       [0.00168923],\n",
       "       [0.00158868],\n",
       "       [0.00151715],\n",
       "       [0.00169098],\n",
       "       [0.00132336],\n",
       "       [0.0017606 ],\n",
       "       [0.00174971],\n",
       "       [0.00160467],\n",
       "       [0.00143357],\n",
       "       [0.00212373],\n",
       "       [0.00141392],\n",
       "       [0.00117356],\n",
       "       [0.00121873],\n",
       "       [0.00130477],\n",
       "       [0.00170935],\n",
       "       [0.00113053],\n",
       "       [0.00124365],\n",
       "       [0.00150796],\n",
       "       [0.00160679],\n",
       "       [0.00144875],\n",
       "       [0.00213415],\n",
       "       [0.00126357],\n",
       "       [0.00197004],\n",
       "       [0.00127377],\n",
       "       [0.00152652],\n",
       "       [0.00174322],\n",
       "       [0.00166952],\n",
       "       [0.00151931],\n",
       "       [0.00133573],\n",
       "       [0.00145741],\n",
       "       [0.00123089],\n",
       "       [0.00178046],\n",
       "       [0.00129646],\n",
       "       [0.00118895],\n",
       "       [0.00162877],\n",
       "       [0.00127988],\n",
       "       [0.00161108],\n",
       "       [0.00152009],\n",
       "       [0.00188549],\n",
       "       [0.0014568 ],\n",
       "       [0.00207617],\n",
       "       [0.0017751 ],\n",
       "       [0.00154569],\n",
       "       [0.00201008],\n",
       "       [0.00178876],\n",
       "       [0.00138161],\n",
       "       [0.00141067],\n",
       "       [0.00146259],\n",
       "       [0.00176359],\n",
       "       [0.00143733],\n",
       "       [0.00126367],\n",
       "       [0.00184433],\n",
       "       [0.00095447],\n",
       "       [0.00188874],\n",
       "       [0.00201486],\n",
       "       [0.00155294],\n",
       "       [0.00152115],\n",
       "       [0.00218524],\n",
       "       [0.00193963],\n",
       "       [0.00200268],\n",
       "       [0.00140266],\n",
       "       [0.00196614],\n",
       "       [0.00152359],\n",
       "       [0.00184227],\n",
       "       [0.00148874],\n",
       "       [0.00162763],\n",
       "       [0.00175475],\n",
       "       [0.00151371],\n",
       "       [0.00180165],\n",
       "       [0.001072  ],\n",
       "       [0.00155982],\n",
       "       [0.00195335],\n",
       "       [0.00189879],\n",
       "       [0.00208211],\n",
       "       [0.0015661 ],\n",
       "       [0.00168029],\n",
       "       [0.00177184],\n",
       "       [0.00205675],\n",
       "       [0.00168004],\n",
       "       [0.00142158],\n",
       "       [0.00157596],\n",
       "       [0.00179824],\n",
       "       [0.00139177],\n",
       "       [0.00126067],\n",
       "       [0.00189263],\n",
       "       [0.00169241],\n",
       "       [0.00154406],\n",
       "       [0.00144443],\n",
       "       [0.00167582],\n",
       "       [0.00204147],\n",
       "       [0.00128348],\n",
       "       [0.00116866],\n",
       "       [0.00211155],\n",
       "       [0.00151478],\n",
       "       [0.00165178],\n",
       "       [0.00176216],\n",
       "       [0.00173685],\n",
       "       [0.00205091],\n",
       "       [0.0010426 ],\n",
       "       [0.00152608],\n",
       "       [0.00184499],\n",
       "       [0.00168985],\n",
       "       [0.00186311],\n",
       "       [0.00145559],\n",
       "       [0.00149848],\n",
       "       [0.00180786],\n",
       "       [0.00172738],\n",
       "       [0.00136395],\n",
       "       [0.00186432],\n",
       "       [0.00187796],\n",
       "       [0.00188548],\n",
       "       [0.00092402],\n",
       "       [0.00193174],\n",
       "       [0.00204122],\n",
       "       [0.00157549],\n",
       "       [0.00105491],\n",
       "       [0.00162505],\n",
       "       [0.0015781 ],\n",
       "       [0.00158763],\n",
       "       [0.0013079 ],\n",
       "       [0.00184808],\n",
       "       [0.00143847],\n",
       "       [0.00175626],\n",
       "       [0.00178473],\n",
       "       [0.00186118],\n",
       "       [0.00184406],\n",
       "       [0.00160468],\n",
       "       [0.00184225],\n",
       "       [0.00147214],\n",
       "       [0.00060003],\n",
       "       [0.00128672],\n",
       "       [0.00155442],\n",
       "       [0.0014361 ],\n",
       "       [0.00136502],\n",
       "       [0.00171439],\n",
       "       [0.00095068],\n",
       "       [0.00152569],\n",
       "       [0.00194579],\n",
       "       [0.00149387],\n",
       "       [0.00089914],\n",
       "       [0.00198073],\n",
       "       [0.0012607 ],\n",
       "       [0.00092249],\n",
       "       [0.0018111 ],\n",
       "       [0.00212147],\n",
       "       [0.00161086],\n",
       "       [0.00136385],\n",
       "       [0.00204092],\n",
       "       [0.0017046 ],\n",
       "       [0.00171615],\n",
       "       [0.0012546 ],\n",
       "       [0.00165497],\n",
       "       [0.00175881],\n",
       "       [0.00184138],\n",
       "       [0.00089313],\n",
       "       [0.00095435],\n",
       "       [0.00189942],\n",
       "       [0.00114758],\n",
       "       [0.00160283],\n",
       "       [0.00139343],\n",
       "       [0.00148628],\n",
       "       [0.00179151],\n",
       "       [0.00105547],\n",
       "       [0.00185832],\n",
       "       [0.00136681],\n",
       "       [0.00134027],\n",
       "       [0.00163951],\n",
       "       [0.00205672],\n",
       "       [0.00170484],\n",
       "       [0.00200133],\n",
       "       [0.00166982],\n",
       "       [0.00103991],\n",
       "       [0.00121633],\n",
       "       [0.00113854],\n",
       "       [0.00121437],\n",
       "       [0.00190656],\n",
       "       [0.0012542 ],\n",
       "       [0.00149278],\n",
       "       [0.00161316],\n",
       "       [0.00121017],\n",
       "       [0.00141525],\n",
       "       [0.00179015],\n",
       "       [0.00171244],\n",
       "       [0.00142227],\n",
       "       [0.00149541],\n",
       "       [0.00165951],\n",
       "       [0.00124725],\n",
       "       [0.00175027],\n",
       "       [0.00157337],\n",
       "       [0.00151988],\n",
       "       [0.00131136],\n",
       "       [0.00141018],\n",
       "       [0.00187008],\n",
       "       [0.00172052],\n",
       "       [0.00138055],\n",
       "       [0.00155068],\n",
       "       [0.00193509],\n",
       "       [0.00147931],\n",
       "       [0.00133971],\n",
       "       [0.00150088],\n",
       "       [0.00131372],\n",
       "       [0.00132769],\n",
       "       [0.0016742 ],\n",
       "       [0.00172576],\n",
       "       [0.00155957],\n",
       "       [0.00152603],\n",
       "       [0.00116409],\n",
       "       [0.00130537],\n",
       "       [0.0015467 ],\n",
       "       [0.00179163],\n",
       "       [0.00155014],\n",
       "       [0.00155631],\n",
       "       [0.00118399],\n",
       "       [0.00145289],\n",
       "       [0.00156656],\n",
       "       [0.00110735],\n",
       "       [0.00149909],\n",
       "       [0.00178613],\n",
       "       [0.00161169],\n",
       "       [0.00181307],\n",
       "       [0.00195003],\n",
       "       [0.00148506],\n",
       "       [0.00150103],\n",
       "       [0.00135045],\n",
       "       [0.0017139 ],\n",
       "       [0.00150352],\n",
       "       [0.00168391],\n",
       "       [0.00149299],\n",
       "       [0.00179486],\n",
       "       [0.00118831],\n",
       "       [0.00146561],\n",
       "       [0.00167991],\n",
       "       [0.00144085],\n",
       "       [0.0013154 ],\n",
       "       [0.00139037],\n",
       "       [0.00140815],\n",
       "       [0.00171906],\n",
       "       [0.00155453],\n",
       "       [0.00179198],\n",
       "       [0.00176885],\n",
       "       [0.00126891],\n",
       "       [0.00168611],\n",
       "       [0.00153287],\n",
       "       [0.00108089],\n",
       "       [0.00171938],\n",
       "       [0.00178151],\n",
       "       [0.00169177],\n",
       "       [0.00189562],\n",
       "       [0.00111186],\n",
       "       [0.00198586],\n",
       "       [0.00150268],\n",
       "       [0.00160536],\n",
       "       [0.00193647],\n",
       "       [0.00188609],\n",
       "       [0.00183464],\n",
       "       [0.00135433],\n",
       "       [0.00167484],\n",
       "       [0.00102062],\n",
       "       [0.00174374],\n",
       "       [0.00129065],\n",
       "       [0.00151209],\n",
       "       [0.00130949],\n",
       "       [0.00155841],\n",
       "       [0.00176582],\n",
       "       [0.00159714],\n",
       "       [0.00175782],\n",
       "       [0.0011862 ],\n",
       "       [0.00094833],\n",
       "       [0.00185018],\n",
       "       [0.00167751],\n",
       "       [0.00144111],\n",
       "       [0.00191605],\n",
       "       [0.00160283],\n",
       "       [0.00198732],\n",
       "       [0.00193939],\n",
       "       [0.00125222],\n",
       "       [0.00199016],\n",
       "       [0.00139292],\n",
       "       [0.00123604],\n",
       "       [0.00105808],\n",
       "       [0.00155739],\n",
       "       [0.0017358 ],\n",
       "       [0.00178398],\n",
       "       [0.00196111],\n",
       "       [0.00116651],\n",
       "       [0.00177733],\n",
       "       [0.00090021],\n",
       "       [0.00198381],\n",
       "       [0.00174738],\n",
       "       [0.00163204],\n",
       "       [0.00081243],\n",
       "       [0.00155165],\n",
       "       [0.00133356],\n",
       "       [0.00144032],\n",
       "       [0.00155194],\n",
       "       [0.00104926],\n",
       "       [0.00193479]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_2seq_tanh_loaded = create_model_a_2seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "model_a_2seq_tanh_loaded.load_weights(checkpoint_filepath)\n",
    "model_a_2seq_tanh_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_tanh_pred = model_a_2seq_tanh_loaded.predict(model_a_X_test_padded)\n",
    "model_a_2seq_tanh_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 1s 2ms/sample - loss: 0.0071 - mae: 0.0353\n",
      "Training MSE: 0.07733019570591906\n",
      "Test MSE: 0.08407118843118702\n",
      "Baseline MSE: 0.08418924631944838\n"
     ]
    }
   ],
   "source": [
    "dev_loss, dev_acc = model_a_2seq_tanh_loaded.evaluate(model_a_X_test_padded,  model_a_y_test, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_train, model_a_2seq_tanh_loaded.predict(model_a_X_train_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, model_a_2seq_tanh_loaded.predict(model_a_X_test_padded)) ) }\")\n",
    "print(f\"Baseline MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, 0*model_a_y_test ) ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_a_2seq_tanh_history.history['loss'])\n",
    "plt.plot(model_a_2seq_tanh_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LSTM, tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 159, 100)          522200    \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 1680      \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 523,905\n",
      "Trainable params: 1,705\n",
      "Non-trainable params: 522,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a_1seq_tanh = create_model_a_1seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_1seq_tanh.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1372 samples, validate on 344 samples\n",
      "Epoch 1/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.1046\n",
      "Epoch 00001: val_loss improved from inf to 0.00483, saving model to ./model_a_checkpoint/model_a_1seq_tanh 11112020 1856h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0266 - mae: 0.1033 - val_loss: 0.0048 - val_mae: 0.0418\n",
      "Epoch 2/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0069 - mae: 0.0438\n",
      "Epoch 00002: val_loss improved from 0.00483 to 0.00471, saving model to ./model_a_checkpoint/model_a_1seq_tanh 11112020 1856h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0067 - mae: 0.0431 - val_loss: 0.0047 - val_mae: 0.0387\n",
      "Epoch 3/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0392\n",
      "Epoch 00003: val_loss improved from 0.00471 to 0.00454, saving model to ./model_a_checkpoint/model_a_1seq_tanh 11112020 1856h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0393 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 4/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0388\n",
      "Epoch 00004: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0388 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 5/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0394\n",
      "Epoch 00005: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0392 - val_loss: 0.0046 - val_mae: 0.0362\n",
      "Epoch 6/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0416\n",
      "Epoch 00006: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0412 - val_loss: 0.0049 - val_mae: 0.0437\n",
      "Epoch 7/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0414\n",
      "Epoch 00007: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0066 - mae: 0.0414 - val_loss: 0.0046 - val_mae: 0.0369\n",
      "Epoch 8/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0402\n",
      "Epoch 00008: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0399 - val_loss: 0.0046 - val_mae: 0.0382\n",
      "Epoch 9/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0404\n",
      "Epoch 00009: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0405 - val_loss: 0.0049 - val_mae: 0.0430\n",
      "Epoch 10/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0400\n",
      "Epoch 00010: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0406 - val_loss: 0.0047 - val_mae: 0.0391\n",
      "Epoch 11/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0067 - mae: 0.0403\n",
      "Epoch 00011: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0396 - val_loss: 0.0045 - val_mae: 0.0351\n",
      "Epoch 12/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0387\n",
      "Epoch 00012: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0388 - val_loss: 0.0045 - val_mae: 0.0346\n",
      "Epoch 13/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0380\n",
      "Epoch 00013: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0383 - val_loss: 0.0046 - val_mae: 0.0345\n",
      "Epoch 14/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0402- ETA: 0s - loss: 0.0062 - mae: 0\n",
      "Epoch 00014: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0398 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 15/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0396\n",
      "Epoch 00015: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0394 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 16/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0055 - mae: 0.0386\n",
      "Epoch 00016: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0397 - val_loss: 0.0046 - val_mae: 0.0373\n",
      "Epoch 17/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0400\n",
      "Epoch 00017: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0399 - val_loss: 0.0046 - val_mae: 0.0353\n",
      "Epoch 18/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0410\n",
      "Epoch 00018: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0410 - val_loss: 0.0050 - val_mae: 0.0429\n",
      "Epoch 19/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0399\n",
      "Epoch 00019: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0066 - mae: 0.0405 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 20/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0398\n",
      "Epoch 00020: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0398 - val_loss: 0.0046 - val_mae: 0.0360\n",
      "Epoch 21/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0065 - mae: 0.0385\n",
      "Epoch 00021: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0384\n",
      "Epoch 22/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0380\n",
      "Epoch 00022: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0392 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 23/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0056 - mae: 0.0386\n",
      "Epoch 00023: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0065 - mae: 0.0392 - val_loss: 0.0046 - val_mae: 0.0365\n",
      "Epoch 24/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0390\n",
      "Epoch 00024: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0396 - val_loss: 0.0046 - val_mae: 0.0359\n",
      "Epoch 25/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0382\n",
      "Epoch 00025: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0362\n",
      "Epoch 26/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0387\n",
      "Epoch 00026: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0383 - val_loss: 0.0046 - val_mae: 0.0351\n",
      "Epoch 27/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0388\n",
      "Epoch 00027: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0387 - val_loss: 0.0046 - val_mae: 0.0354\n",
      "Epoch 28/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0065 - mae: 0.0386\n",
      "Epoch 00028: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0385 - val_loss: 0.0046 - val_mae: 0.0366\n",
      "Epoch 29/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0064 - mae: 0.0393\n",
      "Epoch 00029: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0393 - val_loss: 0.0046 - val_mae: 0.0356\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0383\n",
      "Epoch 00030: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0386 - val_loss: 0.0046 - val_mae: 0.0348\n",
      "Epoch 31/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0383\n",
      "Epoch 00031: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0380 - val_loss: 0.0046 - val_mae: 0.0361\n",
      "Epoch 32/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0063 - mae: 0.0388\n",
      "Epoch 00032: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0388 - val_loss: 0.0046 - val_mae: 0.0356\n",
      "Epoch 33/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0385\n",
      "Epoch 00033: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0382 - val_loss: 0.0046 - val_mae: 0.0365\n",
      "Epoch 34/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0390\n",
      "Epoch 00034: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0387 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 35/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0389\n",
      "Epoch 00035: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0386 - val_loss: 0.0047 - val_mae: 0.0378\n",
      "Epoch 36/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0380\n",
      "Epoch 00036: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0370\n",
      "Epoch 37/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0386\n",
      "Epoch 00037: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0386 - val_loss: 0.0047 - val_mae: 0.0387\n",
      "Epoch 38/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0384\n",
      "Epoch 00038: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0360\n",
      "Epoch 39/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0383\n",
      "Epoch 00039: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0382 - val_loss: 0.0046 - val_mae: 0.0345\n",
      "Epoch 40/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0387\n",
      "Epoch 00040: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0385 - val_loss: 0.0046 - val_mae: 0.0345\n",
      "Epoch 41/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0372\n",
      "Epoch 00041: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0377 - val_loss: 0.0046 - val_mae: 0.0359\n",
      "Epoch 42/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0386\n",
      "Epoch 00042: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0355\n",
      "Epoch 43/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0065 - mae: 0.0382\n",
      "Epoch 00043: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0344\n",
      "Epoch 44/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0064 - mae: 0.0374\n",
      "Epoch 00044: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0374 - val_loss: 0.0046 - val_mae: 0.0363\n",
      "Epoch 45/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0066 - mae: 0.0384\n",
      "Epoch 00045: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0379 - val_loss: 0.0046 - val_mae: 0.0344\n",
      "Epoch 46/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0064 - mae: 0.0382\n",
      "Epoch 00046: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0383\n",
      "Epoch 47/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0387\n",
      "Epoch 00047: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0388 - val_loss: 0.0046 - val_mae: 0.0350\n",
      "Epoch 48/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0397\n",
      "Epoch 00048: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0398 - val_loss: 0.0046 - val_mae: 0.0367\n",
      "Epoch 49/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0065 - mae: 0.0392\n",
      "Epoch 00049: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0389 - val_loss: 0.0046 - val_mae: 0.0362\n",
      "Epoch 50/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0064 - mae: 0.0377\n",
      "Epoch 00050: val_loss did not improve from 0.00454\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0064 - mae: 0.0378 - val_loss: 0.0046 - val_mae: 0.0347\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_1seq_tanh {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_1seq_tanh_history = model_a_1seq_tanh.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.83341862e-03],\n",
       "       [ 1.43237016e-03],\n",
       "       [ 7.73080625e-04],\n",
       "       [ 4.02416708e-03],\n",
       "       [ 7.06388941e-03],\n",
       "       [ 3.32023180e-03],\n",
       "       [ 4.78567975e-03],\n",
       "       [ 1.65059092e-03],\n",
       "       [ 2.63031223e-03],\n",
       "       [ 1.76591892e-03],\n",
       "       [ 3.33504146e-03],\n",
       "       [ 8.55704304e-04],\n",
       "       [ 2.90811295e-03],\n",
       "       [ 6.03189506e-03],\n",
       "       [ 2.30995892e-03],\n",
       "       [ 4.64002322e-03],\n",
       "       [ 1.29768113e-03],\n",
       "       [ 1.14364596e-03],\n",
       "       [ 5.14863385e-03],\n",
       "       [ 2.58132280e-03],\n",
       "       [-1.62007893e-03],\n",
       "       [ 9.73038375e-04],\n",
       "       [ 2.35096272e-03],\n",
       "       [ 4.52719210e-03],\n",
       "       [ 2.01788032e-03],\n",
       "       [ 5.28745120e-03],\n",
       "       [ 1.45520642e-03],\n",
       "       [-2.44638155e-04],\n",
       "       [ 3.13458173e-03],\n",
       "       [ 3.35191749e-03],\n",
       "       [ 3.95391631e-04],\n",
       "       [ 2.22323975e-03],\n",
       "       [ 3.06605408e-03],\n",
       "       [ 3.58972349e-03],\n",
       "       [ 5.67039615e-03],\n",
       "       [ 2.82368599e-03],\n",
       "       [ 2.84025911e-03],\n",
       "       [ 1.89252663e-03],\n",
       "       [ 1.83544680e-03],\n",
       "       [ 1.13464380e-03],\n",
       "       [-8.87843489e-05],\n",
       "       [ 5.16902329e-03],\n",
       "       [ 4.07842780e-03],\n",
       "       [ 2.80688377e-03],\n",
       "       [ 1.50562963e-03],\n",
       "       [-2.35758699e-03],\n",
       "       [-1.72202801e-03],\n",
       "       [ 6.59626769e-03],\n",
       "       [ 1.79398013e-03],\n",
       "       [ 9.61859245e-04],\n",
       "       [ 1.77998818e-03],\n",
       "       [ 5.91053441e-03],\n",
       "       [ 1.78282056e-03],\n",
       "       [ 1.63142011e-02],\n",
       "       [-4.94299689e-04],\n",
       "       [-1.15251378e-03],\n",
       "       [ 1.18556013e-03],\n",
       "       [ 1.14682689e-03],\n",
       "       [ 1.89625239e-03],\n",
       "       [ 5.71741839e-06],\n",
       "       [ 3.83420149e-03],\n",
       "       [ 1.53481890e-03],\n",
       "       [ 6.76081982e-04],\n",
       "       [ 1.02486956e-04],\n",
       "       [ 3.81316524e-03],\n",
       "       [ 4.65875817e-03],\n",
       "       [-5.54499915e-04],\n",
       "       [ 4.60420363e-03],\n",
       "       [ 1.57120405e-03],\n",
       "       [ 2.03007110e-03],\n",
       "       [ 1.55032775e-03],\n",
       "       [ 1.27665838e-03],\n",
       "       [ 2.88188178e-03],\n",
       "       [ 2.62813456e-03],\n",
       "       [ 2.97222054e-03],\n",
       "       [ 2.21898383e-03],\n",
       "       [ 2.17654486e-03],\n",
       "       [ 1.78697193e-03],\n",
       "       [-9.71040572e-06],\n",
       "       [ 8.41792440e-04],\n",
       "       [-2.35004886e-03],\n",
       "       [ 2.38850852e-03],\n",
       "       [ 6.58522965e-03],\n",
       "       [ 5.64867444e-03],\n",
       "       [ 4.76018945e-03],\n",
       "       [ 4.42173239e-03],\n",
       "       [ 1.47925736e-03],\n",
       "       [ 1.42725266e-03],\n",
       "       [ 3.72027000e-03],\n",
       "       [ 4.50691069e-03],\n",
       "       [ 1.14905797e-02],\n",
       "       [ 1.19026075e-03],\n",
       "       [ 1.49153033e-03],\n",
       "       [ 5.96658187e-03],\n",
       "       [ 6.16534846e-03],\n",
       "       [ 3.60390334e-03],\n",
       "       [ 2.87596165e-04],\n",
       "       [-5.62194036e-04],\n",
       "       [ 9.42940544e-03],\n",
       "       [ 3.72671383e-03],\n",
       "       [ 1.92754436e-03],\n",
       "       [ 5.33887185e-04],\n",
       "       [ 2.24855216e-03],\n",
       "       [ 2.00499617e-03],\n",
       "       [ 5.66798868e-03],\n",
       "       [-3.39928176e-03],\n",
       "       [ 1.41140260e-03],\n",
       "       [ 2.76107574e-03],\n",
       "       [ 1.59666920e-03],\n",
       "       [ 5.83742512e-03],\n",
       "       [ 4.89825197e-03],\n",
       "       [ 7.03650946e-03],\n",
       "       [ 3.06042749e-03],\n",
       "       [ 3.70193878e-03],\n",
       "       [ 1.14746103e-02],\n",
       "       [ 2.44053034e-03],\n",
       "       [ 2.22634478e-03],\n",
       "       [ 8.91020638e-04],\n",
       "       [ 3.35284509e-03],\n",
       "       [ 1.91602996e-03],\n",
       "       [ 4.07976937e-03],\n",
       "       [ 1.45165832e-03],\n",
       "       [ 2.73157423e-03],\n",
       "       [ 4.89365263e-03],\n",
       "       [ 4.85050911e-03],\n",
       "       [ 1.72739383e-03],\n",
       "       [ 3.22152628e-03],\n",
       "       [ 1.74744194e-03],\n",
       "       [ 1.91790517e-03],\n",
       "       [ 8.01715301e-04],\n",
       "       [ 3.67115252e-03],\n",
       "       [ 1.07019162e-02],\n",
       "       [ 4.00146964e-04],\n",
       "       [ 1.30945584e-03],\n",
       "       [ 3.65043245e-03],\n",
       "       [ 2.49000965e-03],\n",
       "       [ 8.01041164e-03],\n",
       "       [ 1.42167695e-03],\n",
       "       [ 3.82256974e-03],\n",
       "       [ 2.72958679e-03],\n",
       "       [ 2.49303528e-03],\n",
       "       [ 1.15571823e-03],\n",
       "       [ 5.41715790e-03],\n",
       "       [ 4.43144841e-03],\n",
       "       [ 2.42331484e-03],\n",
       "       [-2.18868698e-03],\n",
       "       [ 4.24419792e-04],\n",
       "       [ 5.52387442e-04],\n",
       "       [ 4.41110088e-03],\n",
       "       [ 6.42757537e-03],\n",
       "       [ 6.75137853e-03],\n",
       "       [ 4.12987638e-03],\n",
       "       [ 4.69713501e-04],\n",
       "       [ 1.11735659e-03],\n",
       "       [ 1.28963822e-03],\n",
       "       [ 5.29325427e-03],\n",
       "       [ 1.40079064e-03],\n",
       "       [ 4.64777183e-03],\n",
       "       [ 3.47045064e-03],\n",
       "       [ 4.62100143e-03],\n",
       "       [ 1.90340425e-03],\n",
       "       [ 1.32135348e-03],\n",
       "       [ 6.11706171e-04],\n",
       "       [ 4.57691588e-03],\n",
       "       [ 2.42331484e-03],\n",
       "       [ 2.53526820e-03],\n",
       "       [ 2.94033648e-03],\n",
       "       [-1.15675479e-03],\n",
       "       [ 4.46890434e-03],\n",
       "       [ 7.59149902e-04],\n",
       "       [ 6.72779512e-03],\n",
       "       [-1.02700636e-04],\n",
       "       [ 4.10792913e-04],\n",
       "       [ 2.52175983e-03],\n",
       "       [ 4.24502679e-04],\n",
       "       [ 1.75505551e-03],\n",
       "       [ 5.75636048e-04],\n",
       "       [ 3.44879879e-03],\n",
       "       [ 1.19154138e-04],\n",
       "       [ 2.24473164e-03],\n",
       "       [ 6.67813700e-04],\n",
       "       [ 4.64237714e-03],\n",
       "       [ 3.07094026e-03],\n",
       "       [ 3.10285483e-03],\n",
       "       [ 5.00610704e-03],\n",
       "       [ 2.45202892e-03],\n",
       "       [ 5.05462475e-03],\n",
       "       [ 2.92876596e-03],\n",
       "       [-6.75230287e-04],\n",
       "       [ 9.75269359e-04],\n",
       "       [ 9.55804251e-04],\n",
       "       [ 2.01544911e-03],\n",
       "       [-3.01148015e-04],\n",
       "       [ 5.14354743e-03],\n",
       "       [ 4.09293408e-03],\n",
       "       [ 4.07873886e-03],\n",
       "       [ 7.83564989e-04],\n",
       "       [ 1.39946118e-03],\n",
       "       [ 2.47484585e-03],\n",
       "       [ 4.47864784e-03],\n",
       "       [-1.19264144e-03],\n",
       "       [ 3.53702763e-03],\n",
       "       [-2.96405487e-04],\n",
       "       [ 1.37764309e-03],\n",
       "       [ 6.28286600e-03],\n",
       "       [ 1.16589665e-03],\n",
       "       [ 1.11702969e-02],\n",
       "       [-1.11045595e-03],\n",
       "       [ 1.76680740e-03],\n",
       "       [-4.00782534e-04],\n",
       "       [ 3.35063506e-03],\n",
       "       [ 4.64579416e-03],\n",
       "       [ 2.80714082e-03],\n",
       "       [ 5.25305280e-03],\n",
       "       [ 2.84387218e-03],\n",
       "       [ 1.56017253e-03],\n",
       "       [ 6.30206987e-03],\n",
       "       [ 3.07947304e-03],\n",
       "       [ 1.85179088e-04],\n",
       "       [ 4.68205690e-04],\n",
       "       [ 8.93161166e-04],\n",
       "       [ 2.00207275e-03],\n",
       "       [ 2.99220975e-03],\n",
       "       [ 4.77826828e-03],\n",
       "       [ 1.96092809e-03],\n",
       "       [ 2.68867984e-03],\n",
       "       [-3.22613842e-03],\n",
       "       [ 4.92502889e-03],\n",
       "       [-7.35533889e-04],\n",
       "       [-7.45609170e-04],\n",
       "       [ 4.79809567e-03],\n",
       "       [ 3.04131163e-03],\n",
       "       [ 5.83528075e-03],\n",
       "       [ 4.15019458e-03],\n",
       "       [ 4.50289715e-03],\n",
       "       [ 4.58242698e-03],\n",
       "       [ 3.15328734e-03],\n",
       "       [ 1.46596297e-03],\n",
       "       [ 2.39579938e-03],\n",
       "       [ 1.62214460e-03],\n",
       "       [ 6.23001158e-03],\n",
       "       [ 3.17838485e-03],\n",
       "       [ 2.95494264e-03],\n",
       "       [-3.96954565e-04],\n",
       "       [ 2.45122379e-03],\n",
       "       [ 1.85538374e-05],\n",
       "       [ 2.58300151e-03],\n",
       "       [ 3.37889651e-03],\n",
       "       [ 2.63995095e-03],\n",
       "       [ 3.65789467e-03],\n",
       "       [ 4.39814432e-03],\n",
       "       [ 5.32167684e-03],\n",
       "       [ 8.68326519e-04],\n",
       "       [ 3.42178531e-03],\n",
       "       [ 4.56579990e-04],\n",
       "       [ 2.59204046e-03],\n",
       "       [ 2.79303035e-03],\n",
       "       [ 2.43368931e-03],\n",
       "       [ 2.07492290e-03],\n",
       "       [ 2.60579237e-03],\n",
       "       [ 1.34917721e-03],\n",
       "       [ 4.50347224e-03],\n",
       "       [ 3.53537221e-03],\n",
       "       [ 2.43163668e-03],\n",
       "       [ 1.28548173e-03],\n",
       "       [ 2.50342209e-03],\n",
       "       [ 2.29952019e-03],\n",
       "       [ 3.39359068e-03],\n",
       "       [ 1.54920341e-03],\n",
       "       [ 1.39434775e-03],\n",
       "       [ 5.34216291e-04],\n",
       "       [ 3.78196687e-03],\n",
       "       [-9.61552607e-04],\n",
       "       [ 4.49504331e-03],\n",
       "       [ 1.96141051e-03],\n",
       "       [ 4.54321085e-03],\n",
       "       [ 1.38452975e-03],\n",
       "       [ 8.00618553e-04],\n",
       "       [ 7.05925049e-04],\n",
       "       [ 1.64967217e-03],\n",
       "       [ 3.67089221e-03],\n",
       "       [ 3.83114722e-03],\n",
       "       [ 3.72671383e-03],\n",
       "       [ 3.08453664e-03],\n",
       "       [ 2.07203953e-03],\n",
       "       [ 3.80688533e-03],\n",
       "       [ 2.47621513e-03],\n",
       "       [ 1.02290139e-03],\n",
       "       [ 8.67678085e-04],\n",
       "       [ 6.86932239e-04],\n",
       "       [ 3.02715809e-03],\n",
       "       [ 2.64966069e-03],\n",
       "       [-6.19848492e-04],\n",
       "       [ 1.92323839e-03],\n",
       "       [ 3.23044742e-03],\n",
       "       [ 5.36230858e-04],\n",
       "       [ 5.48773678e-04],\n",
       "       [ 6.23753993e-03],\n",
       "       [ 2.81403121e-03],\n",
       "       [ 3.35395755e-03],\n",
       "       [ 1.71286752e-03],\n",
       "       [ 4.49705694e-05],\n",
       "       [ 1.13944267e-03],\n",
       "       [ 2.02206662e-03],\n",
       "       [ 1.17485411e-03],\n",
       "       [ 1.88778481e-03],\n",
       "       [ 2.92128912e-04],\n",
       "       [ 6.31791260e-03],\n",
       "       [ 5.06325113e-03],\n",
       "       [ 3.00191529e-03],\n",
       "       [ 1.17893750e-03],\n",
       "       [ 1.37445144e-03],\n",
       "       [ 1.01340190e-03],\n",
       "       [ 5.39784832e-03],\n",
       "       [ 1.18691503e-04],\n",
       "       [ 3.14386562e-03],\n",
       "       [ 3.79989622e-03],\n",
       "       [ 3.21334368e-03],\n",
       "       [ 3.88136180e-03],\n",
       "       [ 2.78407778e-03],\n",
       "       [-3.49205890e-04],\n",
       "       [-3.19767714e-04],\n",
       "       [-7.86922406e-04],\n",
       "       [ 3.82000767e-03],\n",
       "       [ 1.22986297e-04],\n",
       "       [ 4.73744422e-03],\n",
       "       [ 7.68287620e-03],\n",
       "       [ 2.95577059e-03],\n",
       "       [ 2.78821262e-03],\n",
       "       [ 6.27031084e-04],\n",
       "       [ 7.13314512e-06],\n",
       "       [ 2.00704345e-03],\n",
       "       [ 8.99937702e-04],\n",
       "       [ 2.58629187e-03],\n",
       "       [ 1.93901360e-03],\n",
       "       [ 6.36035809e-03],\n",
       "       [ 7.51272775e-04],\n",
       "       [-7.29800668e-04],\n",
       "       [ 3.65317566e-03],\n",
       "       [ 4.27069794e-03],\n",
       "       [ 1.85460877e-03],\n",
       "       [ 4.24196804e-03],\n",
       "       [ 1.53892301e-03],\n",
       "       [ 3.95900663e-03],\n",
       "       [ 1.61777884e-02],\n",
       "       [ 5.62320696e-04],\n",
       "       [ 7.92815816e-04],\n",
       "       [ 4.92996722e-03],\n",
       "       [ 3.13883089e-03],\n",
       "       [ 9.87715553e-04],\n",
       "       [ 5.21199545e-03],\n",
       "       [ 7.95676000e-03],\n",
       "       [ 5.41720632e-03],\n",
       "       [ 2.42980337e-03],\n",
       "       [ 9.21852421e-04],\n",
       "       [ 4.90034837e-03],\n",
       "       [ 5.05791418e-03],\n",
       "       [ 1.62546709e-03],\n",
       "       [-1.89626636e-03],\n",
       "       [ 2.80531589e-03],\n",
       "       [ 3.19970027e-03],\n",
       "       [ 2.37758690e-03],\n",
       "       [ 2.93931412e-03],\n",
       "       [ 7.82143790e-03],\n",
       "       [ 1.55459694e-03],\n",
       "       [ 3.91083770e-03],\n",
       "       [ 3.32288444e-03],\n",
       "       [ 6.14221161e-03],\n",
       "       [ 2.57681077e-03],\n",
       "       [ 5.39420685e-03],\n",
       "       [-1.19643216e-03],\n",
       "       [ 2.25330843e-03],\n",
       "       [ 5.79234678e-04],\n",
       "       [ 3.14386562e-03],\n",
       "       [ 7.47297425e-04],\n",
       "       [ 5.32382447e-03],\n",
       "       [ 1.60719082e-03],\n",
       "       [ 1.64437643e-03],\n",
       "       [ 3.81107442e-03],\n",
       "       [-3.41427483e-04],\n",
       "       [ 7.09510641e-04],\n",
       "       [ 1.24473963e-03],\n",
       "       [ 3.17854574e-03],\n",
       "       [ 1.39882602e-03],\n",
       "       [ 1.70797273e-03],\n",
       "       [ 4.02798178e-03],\n",
       "       [ 1.73706631e-03],\n",
       "       [ 2.21378542e-03],\n",
       "       [ 4.44609532e-03],\n",
       "       [ 3.69621022e-03],\n",
       "       [ 4.45962185e-03],\n",
       "       [ 2.07816809e-03],\n",
       "       [-1.30791415e-03],\n",
       "       [ 2.18582083e-03],\n",
       "       [ 1.42781326e-04],\n",
       "       [ 5.28380368e-03],\n",
       "       [-5.31690661e-04],\n",
       "       [ 1.60488370e-03],\n",
       "       [ 2.22479488e-04],\n",
       "       [ 1.75906811e-03],\n",
       "       [ 5.70288161e-04],\n",
       "       [ 2.90283281e-03],\n",
       "       [-1.90088572e-03],\n",
       "       [ 3.70528363e-03],\n",
       "       [-3.87635053e-04],\n",
       "       [ 1.66948396e-03],\n",
       "       [ 5.37082972e-03],\n",
       "       [ 7.62539683e-04],\n",
       "       [ 2.35182815e-03],\n",
       "       [ 6.32047141e-03],\n",
       "       [ 3.31722759e-03],\n",
       "       [ 3.03275837e-03],\n",
       "       [ 1.25111931e-03],\n",
       "       [ 2.42874702e-03],\n",
       "       [ 3.56881786e-03],\n",
       "       [ 1.99240400e-03],\n",
       "       [ 2.08089501e-03],\n",
       "       [ 2.95883557e-03],\n",
       "       [ 2.17546243e-03],\n",
       "       [ 7.35608535e-03],\n",
       "       [ 2.44805153e-04],\n",
       "       [ 2.26912601e-03],\n",
       "       [ 1.43291429e-03],\n",
       "       [ 3.86594841e-03],\n",
       "       [ 4.82528610e-03],\n",
       "       [ 3.48654343e-03],\n",
       "       [ 5.20282285e-03],\n",
       "       [ 9.42834653e-04],\n",
       "       [ 5.16378740e-03],\n",
       "       [ 4.73315874e-03],\n",
       "       [ 4.23616497e-03],\n",
       "       [ 4.66400478e-03],\n",
       "       [-4.72908810e-04],\n",
       "       [ 2.77523184e-03],\n",
       "       [ 1.28272176e-03],\n",
       "       [-9.24866181e-04],\n",
       "       [ 3.72504257e-03],\n",
       "       [ 3.06989253e-03],\n",
       "       [ 4.42256266e-03],\n",
       "       [ 3.31666693e-03],\n",
       "       [ 1.57656753e-03],\n",
       "       [ 9.43897292e-04],\n",
       "       [ 1.45023968e-03],\n",
       "       [ 3.81866703e-03],\n",
       "       [ 6.03615725e-03],\n",
       "       [ 3.84636037e-03],\n",
       "       [ 2.25007348e-03],\n",
       "       [ 1.34584028e-03],\n",
       "       [-3.69111513e-04],\n",
       "       [ 3.23946727e-03],\n",
       "       [ 4.80256788e-03],\n",
       "       [ 4.26584901e-03],\n",
       "       [ 1.45605858e-03],\n",
       "       [ 4.67290962e-03],\n",
       "       [ 8.22746195e-04],\n",
       "       [ 2.32538022e-03],\n",
       "       [ 2.27368902e-03],\n",
       "       [-8.55169026e-04],\n",
       "       [ 1.30931148e-03],\n",
       "       [ 1.41555816e-03],\n",
       "       [ 9.09154024e-03],\n",
       "       [ 2.88494281e-03],\n",
       "       [ 2.47874763e-03],\n",
       "       [ 1.23940641e-03],\n",
       "       [ 6.33308548e-04],\n",
       "       [-3.99666344e-04],\n",
       "       [ 3.10617173e-03],\n",
       "       [ 9.15609300e-04],\n",
       "       [ 3.47308512e-03],\n",
       "       [-2.56850122e-04],\n",
       "       [ 2.15832726e-03],\n",
       "       [ 2.27627344e-03],\n",
       "       [ 4.37253946e-03],\n",
       "       [ 2.64500128e-03],\n",
       "       [ 4.41160705e-03],\n",
       "       [ 2.04548705e-03],\n",
       "       [ 4.02636407e-03],\n",
       "       [ 4.20689024e-03],\n",
       "       [ 7.90976454e-04],\n",
       "       [ 1.05553912e-03],\n",
       "       [-2.50602316e-05],\n",
       "       [ 1.71074318e-03],\n",
       "       [ 8.12823046e-03],\n",
       "       [-2.35758699e-03],\n",
       "       [ 2.63306312e-03],\n",
       "       [ 1.40457251e-03],\n",
       "       [ 3.16069089e-03],\n",
       "       [ 1.53636816e-03],\n",
       "       [ 2.28056638e-03],\n",
       "       [ 2.42340285e-03],\n",
       "       [ 3.62543185e-04],\n",
       "       [ 6.61855238e-03],\n",
       "       [ 1.71330385e-03],\n",
       "       [ 1.70397037e-03],\n",
       "       [ 5.62449824e-03],\n",
       "       [ 5.17418934e-03],\n",
       "       [ 1.28400640e-03],\n",
       "       [ 2.25266861e-03],\n",
       "       [ 7.40024727e-04],\n",
       "       [ 3.35856853e-03],\n",
       "       [ 1.04178558e-03],\n",
       "       [ 1.67575572e-03],\n",
       "       [ 3.61653138e-03],\n",
       "       [-7.04822131e-04],\n",
       "       [ 1.81001564e-03],\n",
       "       [ 1.11780129e-03],\n",
       "       [ 5.23428991e-03],\n",
       "       [ 5.65770920e-03],\n",
       "       [ 2.94524292e-03],\n",
       "       [ 9.83567734e-05],\n",
       "       [ 4.61766496e-03],\n",
       "       [ 3.99199827e-03],\n",
       "       [ 3.92700592e-03],\n",
       "       [ 8.52034718e-05],\n",
       "       [ 2.07725028e-03],\n",
       "       [ 2.66122539e-03],\n",
       "       [ 2.62454897e-03],\n",
       "       [ 4.61347401e-03],\n",
       "       [-2.41348706e-03],\n",
       "       [ 1.74595416e-03],\n",
       "       [ 1.37110939e-03],\n",
       "       [ 3.71042220e-03],\n",
       "       [-1.64614961e-04],\n",
       "       [ 1.78454118e-03],\n",
       "       [ 2.54737027e-03],\n",
       "       [ 1.77172944e-03],\n",
       "       [ 1.24719739e-03],\n",
       "       [ 2.49752804e-04],\n",
       "       [-2.33563827e-03],\n",
       "       [ 1.20617216e-03],\n",
       "       [ 2.08917330e-03],\n",
       "       [ 4.89806477e-03],\n",
       "       [ 2.15099705e-03],\n",
       "       [ 7.85584329e-04],\n",
       "       [ 5.11588436e-03],\n",
       "       [ 4.26982436e-03],\n",
       "       [ 4.52599517e-04],\n",
       "       [ 3.30573833e-03],\n",
       "       [-4.14316979e-04],\n",
       "       [ 3.01737897e-03],\n",
       "       [ 3.32274707e-03],\n",
       "       [ 2.67802039e-03],\n",
       "       [ 1.39771309e-03],\n",
       "       [ 8.16050917e-04],\n",
       "       [ 2.92322086e-03],\n",
       "       [ 2.89643789e-03],\n",
       "       [ 1.65105844e-03],\n",
       "       [ 2.09317869e-03],\n",
       "       [ 3.67509527e-03],\n",
       "       [ 6.35000737e-03],\n",
       "       [-1.47425430e-03],\n",
       "       [ 2.12464156e-03],\n",
       "       [ 1.85828889e-03],\n",
       "       [ 4.49415063e-03],\n",
       "       [ 1.71765080e-03],\n",
       "       [ 3.69579857e-03],\n",
       "       [ 3.72671383e-03],\n",
       "       [ 3.45093926e-04],\n",
       "       [ 5.37334615e-03],\n",
       "       [ 2.71437224e-03],\n",
       "       [ 1.66703342e-03],\n",
       "       [-9.15752724e-04],\n",
       "       [ 3.17068800e-04],\n",
       "       [ 1.91708794e-03],\n",
       "       [ 2.35343701e-03],\n",
       "       [ 1.76831381e-03],\n",
       "       [ 1.41214998e-03],\n",
       "       [ 2.10455945e-03],\n",
       "       [ 1.24439190e-03],\n",
       "       [ 1.05906045e-02],\n",
       "       [-1.49761065e-04],\n",
       "       [ 5.16550709e-03],\n",
       "       [ 3.79382493e-03],\n",
       "       [ 3.00167501e-03],\n",
       "       [ 4.03391756e-03],\n",
       "       [ 7.49088358e-03],\n",
       "       [-9.60480655e-04],\n",
       "       [ 3.64598352e-03],\n",
       "       [ 3.09644220e-03],\n",
       "       [ 9.19413753e-04],\n",
       "       [ 2.33711954e-03],\n",
       "       [ 1.88685604e-03],\n",
       "       [ 1.12829143e-02],\n",
       "       [ 2.15972005e-03],\n",
       "       [ 2.49287393e-03],\n",
       "       [ 2.76241498e-03],\n",
       "       [ 1.63257588e-03],\n",
       "       [ 1.15258060e-02],\n",
       "       [ 2.67553912e-03],\n",
       "       [ 9.75572597e-03],\n",
       "       [ 2.57872278e-03],\n",
       "       [-3.82703403e-03],\n",
       "       [ 5.26139839e-03],\n",
       "       [-5.53783262e-04],\n",
       "       [ 1.74961425e-03],\n",
       "       [ 3.59335262e-03],\n",
       "       [ 2.25362834e-03],\n",
       "       [ 3.13412631e-03],\n",
       "       [ 8.56473576e-03],\n",
       "       [-4.00271470e-04],\n",
       "       [ 6.77291304e-03],\n",
       "       [ 3.32848681e-03],\n",
       "       [ 5.38249640e-03],\n",
       "       [ 4.41657705e-03],\n",
       "       [ 5.43027651e-04],\n",
       "       [ 2.15231394e-03],\n",
       "       [ 4.91482811e-03],\n",
       "       [ 2.90995976e-03],\n",
       "       [ 5.25224209e-03],\n",
       "       [ 4.14535357e-03],\n",
       "       [ 1.04763908e-02],\n",
       "       [ 8.65501643e-05],\n",
       "       [ 3.89210088e-03],\n",
       "       [ 3.96386348e-03],\n",
       "       [ 9.26457997e-03],\n",
       "       [ 2.42328737e-03],\n",
       "       [ 2.60884617e-03],\n",
       "       [ 1.80787314e-03],\n",
       "       [ 1.18107838e-03],\n",
       "       [ 2.08358106e-05],\n",
       "       [ 5.81232226e-03],\n",
       "       [ 4.81866580e-03],\n",
       "       [ 4.45962558e-03],\n",
       "       [ 1.74691714e-03],\n",
       "       [ 1.42421247e-03],\n",
       "       [ 4.77569224e-03],\n",
       "       [ 3.15010827e-03],\n",
       "       [ 2.29424867e-03],\n",
       "       [ 5.19395107e-04],\n",
       "       [ 1.13913482e-02],\n",
       "       [ 2.50516459e-03],\n",
       "       [ 5.98539598e-04],\n",
       "       [ 9.07191308e-04],\n",
       "       [ 2.80352775e-03],\n",
       "       [ 2.71414733e-03],\n",
       "       [ 2.94047501e-03],\n",
       "       [ 2.66139070e-03],\n",
       "       [ 3.66734457e-03],\n",
       "       [ 1.07925781e-03],\n",
       "       [ 1.49832712e-03],\n",
       "       [-2.41598836e-03],\n",
       "       [ 1.58662768e-03],\n",
       "       [-1.32703572e-03],\n",
       "       [ 1.05831446e-03],\n",
       "       [ 1.94697781e-03],\n",
       "       [ 3.06244008e-03],\n",
       "       [ 4.19425685e-03],\n",
       "       [-3.34622775e-04],\n",
       "       [ 2.83381855e-03],\n",
       "       [ 2.17310362e-03],\n",
       "       [ 5.82238799e-03],\n",
       "       [-2.60122726e-03],\n",
       "       [-7.35471956e-04],\n",
       "       [ 2.33504642e-03],\n",
       "       [-9.33703268e-04],\n",
       "       [ 1.33296521e-03],\n",
       "       [ 3.97050986e-03],\n",
       "       [ 9.24470136e-04],\n",
       "       [-1.07313599e-03],\n",
       "       [ 1.63267646e-03],\n",
       "       [ 2.12121382e-03],\n",
       "       [ 3.97290755e-03],\n",
       "       [ 6.79789577e-04],\n",
       "       [ 2.15473981e-03],\n",
       "       [ 2.06097990e-04],\n",
       "       [ 3.14386562e-03],\n",
       "       [ 3.72185884e-03],\n",
       "       [ 2.61883112e-03],\n",
       "       [ 2.80130096e-03],\n",
       "       [ 2.18758127e-03],\n",
       "       [ 4.76483582e-03],\n",
       "       [ 3.81316524e-03],\n",
       "       [ 2.52615102e-03],\n",
       "       [ 3.91108776e-03],\n",
       "       [ 5.62428264e-03],\n",
       "       [ 1.66009134e-03],\n",
       "       [ 5.87716233e-04],\n",
       "       [ 1.35889906e-03],\n",
       "       [ 2.08648830e-03],\n",
       "       [ 7.39739276e-04],\n",
       "       [ 3.21133644e-03],\n",
       "       [ 1.50554138e-03],\n",
       "       [ 1.65872625e-03],\n",
       "       [-8.77789862e-05],\n",
       "       [ 3.43451649e-03],\n",
       "       [ 2.92674900e-04],\n",
       "       [ 2.31088977e-03],\n",
       "       [ 1.10376556e-03],\n",
       "       [ 1.46001554e-03],\n",
       "       [ 2.13179999e-04],\n",
       "       [ 6.46575307e-03],\n",
       "       [ 3.94203095e-03],\n",
       "       [ 2.02098954e-03],\n",
       "       [ 3.24563007e-03],\n",
       "       [ 2.62737740e-03],\n",
       "       [ 1.71697792e-03],\n",
       "       [ 6.06857473e-03],\n",
       "       [ 4.94698761e-03],\n",
       "       [-3.07189970e-04],\n",
       "       [ 2.23253178e-03],\n",
       "       [ 3.02962522e-04],\n",
       "       [ 4.90894914e-03],\n",
       "       [ 1.78346876e-03],\n",
       "       [ 1.43259345e-03],\n",
       "       [ 7.44914054e-04],\n",
       "       [ 5.19588776e-03],\n",
       "       [ 1.22359092e-03],\n",
       "       [ 3.04527371e-03],\n",
       "       [ 2.28512008e-03],\n",
       "       [ 2.58185412e-03],\n",
       "       [ 2.12617638e-03],\n",
       "       [ 1.39901508e-03],\n",
       "       [ 1.65306521e-03],\n",
       "       [-2.48314324e-03],\n",
       "       [ 1.64806144e-03],\n",
       "       [ 2.37077358e-03],\n",
       "       [ 1.00059686e-02],\n",
       "       [ 1.44573487e-03],\n",
       "       [ 1.03985127e-02],\n",
       "       [ 1.09996204e-03],\n",
       "       [ 1.27080502e-03],\n",
       "       [ 2.66374880e-03],\n",
       "       [ 1.70293869e-03],\n",
       "       [ 5.58603508e-03],\n",
       "       [ 3.43623920e-03],\n",
       "       [ 1.18967402e-03],\n",
       "       [ 2.28338130e-03],\n",
       "       [ 2.33028317e-03],\n",
       "       [ 3.12146288e-03],\n",
       "       [ 9.19012353e-04],\n",
       "       [ 2.63081444e-03],\n",
       "       [ 3.05143883e-03],\n",
       "       [ 1.34260533e-03],\n",
       "       [ 1.44997984e-03],\n",
       "       [ 2.60194903e-03],\n",
       "       [ 1.04211923e-03],\n",
       "       [ 9.30428738e-04],\n",
       "       [ 2.16396060e-03],\n",
       "       [ 2.00197380e-03],\n",
       "       [-2.87143397e-03],\n",
       "       [ 3.51829018e-04],\n",
       "       [ 3.38698644e-03],\n",
       "       [ 2.30407133e-03],\n",
       "       [ 1.20877335e-03],\n",
       "       [ 2.59960676e-03],\n",
       "       [ 1.02975480e-02],\n",
       "       [-3.88087676e-04],\n",
       "       [ 1.64398551e-03],\n",
       "       [ 2.09318381e-03],\n",
       "       [-3.60722392e-04],\n",
       "       [ 1.52339973e-03],\n",
       "       [ 3.65895778e-03],\n",
       "       [ 3.41109885e-03],\n",
       "       [-1.39740005e-05],\n",
       "       [ 1.22684008e-03],\n",
       "       [ 1.97164365e-03],\n",
       "       [ 1.32591324e-03],\n",
       "       [ 1.57552073e-03],\n",
       "       [ 3.03805992e-03],\n",
       "       [ 4.60169977e-03],\n",
       "       [ 1.24473963e-03],\n",
       "       [ 7.33913621e-04],\n",
       "       [ 7.25252600e-03],\n",
       "       [ 3.37673770e-03],\n",
       "       [ 1.89839257e-03],\n",
       "       [ 1.10110187e-03],\n",
       "       [ 6.67127268e-03],\n",
       "       [ 2.49021128e-03],\n",
       "       [ 9.84899700e-04],\n",
       "       [ 2.65501114e-03],\n",
       "       [ 9.27248970e-04],\n",
       "       [ 2.11943733e-03],\n",
       "       [ 2.87775858e-03],\n",
       "       [ 2.95745861e-03],\n",
       "       [ 4.99991747e-03],\n",
       "       [ 2.11791042e-03],\n",
       "       [ 5.72331855e-03],\n",
       "       [ 4.40331874e-03],\n",
       "       [-1.95175846e-04],\n",
       "       [ 2.08393112e-03],\n",
       "       [ 1.67683559e-03],\n",
       "       [ 2.88983714e-03],\n",
       "       [ 4.59993863e-03],\n",
       "       [ 1.31317251e-03],\n",
       "       [ 3.05132288e-03],\n",
       "       [ 4.03619418e-03],\n",
       "       [ 6.26210030e-03],\n",
       "       [ 3.61298909e-03],\n",
       "       [ 2.74617597e-03],\n",
       "       [ 4.07337240e-04],\n",
       "       [ 2.42676679e-03],\n",
       "       [ 2.78246822e-03],\n",
       "       [ 4.28410276e-04],\n",
       "       [ 6.07105205e-03],\n",
       "       [ 4.34949994e-03],\n",
       "       [ 1.76337920e-03],\n",
       "       [ 3.89624527e-03],\n",
       "       [ 2.09046248e-03],\n",
       "       [ 3.80070647e-03],\n",
       "       [ 4.89249546e-03],\n",
       "       [ 8.15230049e-03],\n",
       "       [ 2.08322145e-03],\n",
       "       [ 2.51722196e-03],\n",
       "       [ 4.19805897e-03],\n",
       "       [-2.53717852e-04],\n",
       "       [ 1.66205782e-03],\n",
       "       [ 2.07871711e-03],\n",
       "       [ 2.56859092e-03],\n",
       "       [ 2.00241618e-03],\n",
       "       [-1.18179887e-05],\n",
       "       [-8.27979762e-04],\n",
       "       [ 3.65501782e-03],\n",
       "       [ 3.15394765e-03],\n",
       "       [ 3.05058551e-03],\n",
       "       [ 9.38598532e-04],\n",
       "       [-1.18815643e-03],\n",
       "       [ 1.11497368e-03],\n",
       "       [ 2.51807598e-03],\n",
       "       [ 6.65117754e-03],\n",
       "       [ 3.87100037e-03],\n",
       "       [ 6.16428489e-03],\n",
       "       [ 3.04527371e-03],\n",
       "       [ 4.16302402e-03],\n",
       "       [ 4.80242120e-03],\n",
       "       [ 1.21012749e-03],\n",
       "       [ 1.50622707e-03],\n",
       "       [ 1.95650896e-03],\n",
       "       [ 4.62710421e-04],\n",
       "       [ 2.51942454e-03],\n",
       "       [ 1.77920359e-04],\n",
       "       [ 3.72981792e-03],\n",
       "       [ 2.33059516e-03],\n",
       "       [ 5.98011771e-03],\n",
       "       [ 1.64719112e-03],\n",
       "       [ 7.17597641e-03],\n",
       "       [ 5.52010583e-03],\n",
       "       [ 2.41864473e-03],\n",
       "       [ 3.39221815e-03],\n",
       "       [ 1.71469129e-03],\n",
       "       [ 6.71521295e-04],\n",
       "       [ 7.84621947e-03],\n",
       "       [ 2.50814669e-03],\n",
       "       [ 2.17674626e-03],\n",
       "       [ 1.79122697e-04],\n",
       "       [ 2.18789512e-03],\n",
       "       [ 7.71611417e-03]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_1seq_tanh_loaded = create_model_a_1seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "model_a_1seq_tanh_loaded.load_weights(checkpoint_filepath)\n",
    "model_a_1seq_tanh_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_tanh_pred = model_a_1seq_tanh_loaded.predict(model_a_X_test_padded)\n",
    "model_a_1seq_tanh_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 1s 949us/sample - loss: 0.0071 - mae: 0.0356\n",
      "Training MSE: 0.07729923417970892\n",
      "Test MSE: 0.08399772207905713\n",
      "Baseline MSE: 0.08418924631944838\n"
     ]
    }
   ],
   "source": [
    "dev_loss, dev_acc = model_a_1seq_tanh_loaded.evaluate(model_a_X_test_padded,  model_a_y_test, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_train, model_a_1seq_tanh_loaded.predict(model_a_X_train_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, model_a_1seq_tanh_loaded.predict(model_a_X_test_padded)) ) }\")\n",
    "print(f\"Baseline MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, 0*model_a_y_test ) ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_a_1seq_tanh_history.history['loss'])\n",
    "plt.plot(model_a_1seq_tanh_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTMs, sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 159, 100)          522200    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 159, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 159, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 524,049\n",
      "Trainable params: 1,849\n",
      "Non-trainable params: 522,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a_2seq_sigmoid = create_model_a_2seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_2seq_sigmoid.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1372 samples, validate on 344 samples\n",
      "Epoch 1/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0602 - mae: 0.1957\n",
      "Epoch 00001: val_loss improved from inf to 0.00690, saving model to ./model_a_checkpoint/model_a_2seq_sigmoid 11112020 1858h.h5\n",
      "1372/1372 [==============================] - 9s 7ms/sample - loss: 0.0595 - mae: 0.1942 - val_loss: 0.0069 - val_mae: 0.0612\n",
      "Epoch 2/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.1116\n",
      "Epoch 00002: val_loss improved from 0.00690 to 0.00452, saving model to ./model_a_checkpoint/model_a_2seq_sigmoid 11112020 1858h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0275 - mae: 0.1123 - val_loss: 0.0045 - val_mae: 0.0346\n",
      "Epoch 3/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.1001\n",
      "Epoch 00003: val_loss improved from 0.00452 to 0.00451, saving model to ./model_a_checkpoint/model_a_2seq_sigmoid 11112020 1858h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0210 - mae: 0.0995 - val_loss: 0.0045 - val_mae: 0.0345\n",
      "Epoch 4/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0149 - mae: 0.0817\n",
      "Epoch 00004: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0149 - mae: 0.0817 - val_loss: 0.0045 - val_mae: 0.0354\n",
      "Epoch 5/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0745\n",
      "Epoch 00005: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0135 - mae: 0.0748 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 6/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0643\n",
      "Epoch 00006: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0108 - mae: 0.0646 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 7/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0634\n",
      "Epoch 00007: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0109 - mae: 0.0637 - val_loss: 0.0045 - val_mae: 0.0355\n",
      "Epoch 8/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0609\n",
      "Epoch 00008: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0103 - mae: 0.0610 - val_loss: 0.0045 - val_mae: 0.0350\n",
      "Epoch 9/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0570\n",
      "Epoch 00009: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0090 - mae: 0.0570 - val_loss: 0.0046 - val_mae: 0.0362\n",
      "Epoch 10/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0588\n",
      "Epoch 00010: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0093 - mae: 0.0584 - val_loss: 0.0045 - val_mae: 0.0348\n",
      "Epoch 11/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0096 - mae: 0.0575\n",
      "Epoch 00011: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0095 - mae: 0.0571 - val_loss: 0.0046 - val_mae: 0.0369\n",
      "Epoch 12/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0551\n",
      "Epoch 00012: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0088 - mae: 0.0551 - val_loss: 0.0046 - val_mae: 0.0358\n",
      "Epoch 13/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0528\n",
      "Epoch 00013: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0085 - mae: 0.0524 - val_loss: 0.0045 - val_mae: 0.0348\n",
      "Epoch 14/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0537\n",
      "Epoch 00014: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0085 - mae: 0.0534 - val_loss: 0.0045 - val_mae: 0.0343\n",
      "Epoch 15/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0534- ETA: 2s - loss\n",
      "Epoch 00015: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0085 - mae: 0.0530 - val_loss: 0.0046 - val_mae: 0.0357\n",
      "Epoch 16/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0516\n",
      "Epoch 00016: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0082 - mae: 0.0515 - val_loss: 0.0046 - val_mae: 0.0362\n",
      "Epoch 17/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0514\n",
      "Epoch 00017: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0084 - mae: 0.0512 - val_loss: 0.0045 - val_mae: 0.0348\n",
      "Epoch 18/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0490\n",
      "Epoch 00018: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0081 - mae: 0.0490 - val_loss: 0.0045 - val_mae: 0.0349\n",
      "Epoch 19/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0489\n",
      "Epoch 00019: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0078 - mae: 0.0491 - val_loss: 0.0046 - val_mae: 0.0353\n",
      "Epoch 20/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0503\n",
      "Epoch 00020: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0081 - mae: 0.0501 - val_loss: 0.0046 - val_mae: 0.0353\n",
      "Epoch 21/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0488\n",
      "Epoch 00021: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0077 - mae: 0.0484 - val_loss: 0.0046 - val_mae: 0.0354\n",
      "Epoch 22/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0484\n",
      "Epoch 00022: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0078 - mae: 0.0483 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 23/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0473\n",
      "Epoch 00023: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0074 - mae: 0.0472 - val_loss: 0.0045 - val_mae: 0.0344\n",
      "Epoch 24/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0466\n",
      "Epoch 00024: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0077 - mae: 0.0465 - val_loss: 0.0046 - val_mae: 0.0355\n",
      "Epoch 25/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0467\n",
      "Epoch 00025: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0073 - mae: 0.0464 - val_loss: 0.0046 - val_mae: 0.0351\n",
      "Epoch 26/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0472\n",
      "Epoch 00026: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0076 - mae: 0.0473 - val_loss: 0.0046 - val_mae: 0.0359\n",
      "Epoch 27/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0465\n",
      "Epoch 00027: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0074 - mae: 0.0463 - val_loss: 0.0046 - val_mae: 0.0352\n",
      "Epoch 28/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0465\n",
      "Epoch 00028: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0075 - mae: 0.0463 - val_loss: 0.0046 - val_mae: 0.0361\n",
      "Epoch 29/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0471\n",
      "Epoch 00029: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0076 - mae: 0.0474 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0456\n",
      "Epoch 00030: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0072 - mae: 0.0453 - val_loss: 0.0045 - val_mae: 0.0347\n",
      "Epoch 31/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0468- ETA: 1s - loss: 0.0072 - \n",
      "Epoch 00031: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0072 - mae: 0.0467 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 32/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0458\n",
      "Epoch 00032: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0072 - mae: 0.0454 - val_loss: 0.0045 - val_mae: 0.0343\n",
      "Epoch 33/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0446\n",
      "Epoch 00033: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0073 - mae: 0.0445 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 34/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0443\n",
      "Epoch 00034: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0440 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 35/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0440\n",
      "Epoch 00035: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0440 - val_loss: 0.0046 - val_mae: 0.0347\n",
      "Epoch 36/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0441\n",
      "Epoch 00036: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0070 - mae: 0.0439 - val_loss: 0.0045 - val_mae: 0.0343\n",
      "Epoch 37/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0444\n",
      "Epoch 00037: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0073 - mae: 0.0439 - val_loss: 0.0045 - val_mae: 0.0341\n",
      "Epoch 38/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0437- ETA: 0s - loss: 0.0073 - mae: 0.04\n",
      "Epoch 00038: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0434 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 39/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0438\n",
      "Epoch 00039: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0434 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 40/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0429- ETA: 2s - lo\n",
      "Epoch 00040: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0431 - val_loss: 0.0045 - val_mae: 0.0344\n",
      "Epoch 41/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0411\n",
      "Epoch 00041: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0412 - val_loss: 0.0046 - val_mae: 0.0349\n",
      "Epoch 42/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0421\n",
      "Epoch 00042: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0068 - mae: 0.0422 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 43/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0415- ETA\n",
      "Epoch 00043: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0071 - mae: 0.0424 - val_loss: 0.0045 - val_mae: 0.0341\n",
      "Epoch 44/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0424- ETA: 0s - loss: 0.0075 - mae\n",
      "Epoch 00044: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0069 - mae: 0.0422 - val_loss: 0.0046 - val_mae: 0.0347\n",
      "Epoch 45/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0423- ETA: 1s - loss: 0.008\n",
      "Epoch 00045: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0069 - mae: 0.0424 - val_loss: 0.0045 - val_mae: 0.0342\n",
      "Epoch 46/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0420\n",
      "Epoch 00046: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0068 - mae: 0.0417 - val_loss: 0.0046 - val_mae: 0.0347\n",
      "Epoch 47/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0416\n",
      "Epoch 00047: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0067 - mae: 0.0415 - val_loss: 0.0045 - val_mae: 0.0345\n",
      "Epoch 48/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0418\n",
      "Epoch 00048: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0070 - mae: 0.0422 - val_loss: 0.0046 - val_mae: 0.0346\n",
      "Epoch 49/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0422- ETA: 2s\n",
      "Epoch 00049: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0070 - mae: 0.0421 - val_loss: 0.0045 - val_mae: 0.0343\n",
      "Epoch 50/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0417\n",
      "Epoch 00050: val_loss did not improve from 0.00451\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.0068 - mae: 0.0417 - val_loss: 0.0046 - val_mae: 0.0347\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_2seq_sigmoid {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_2seq_sigmoid_history = model_a_2seq_sigmoid.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.45041466e-04],\n",
       "       [ 5.68476319e-03],\n",
       "       [ 2.97543406e-03],\n",
       "       [ 1.93116069e-03],\n",
       "       [ 1.40313804e-03],\n",
       "       [-1.86766684e-03],\n",
       "       [-2.02372670e-03],\n",
       "       [ 1.68830156e-03],\n",
       "       [-1.08802319e-03],\n",
       "       [ 1.40219927e-03],\n",
       "       [-1.41905248e-03],\n",
       "       [ 2.08939612e-03],\n",
       "       [ 4.49046493e-04],\n",
       "       [-4.36976552e-04],\n",
       "       [ 7.44998455e-04],\n",
       "       [ 1.91606581e-03],\n",
       "       [-6.47604465e-04],\n",
       "       [-4.46408987e-04],\n",
       "       [ 1.66416168e-04],\n",
       "       [ 4.64215875e-04],\n",
       "       [-1.27208233e-03],\n",
       "       [-7.66068697e-04],\n",
       "       [-3.62783670e-04],\n",
       "       [-2.22682953e-03],\n",
       "       [ 2.61202455e-04],\n",
       "       [ 1.76736712e-03],\n",
       "       [ 8.01786780e-04],\n",
       "       [ 2.58947909e-03],\n",
       "       [-3.08156013e-04],\n",
       "       [-1.50503218e-03],\n",
       "       [-1.09787285e-03],\n",
       "       [-6.94438815e-04],\n",
       "       [ 2.46748328e-03],\n",
       "       [-1.40944123e-03],\n",
       "       [ 2.44632363e-03],\n",
       "       [-1.76410377e-03],\n",
       "       [-1.98146701e-03],\n",
       "       [ 6.64131343e-03],\n",
       "       [-8.28713179e-04],\n",
       "       [ 5.64371049e-03],\n",
       "       [ 3.71594727e-03],\n",
       "       [-1.86507404e-03],\n",
       "       [-7.48932362e-05],\n",
       "       [-1.69084966e-03],\n",
       "       [ 1.70506984e-02],\n",
       "       [ 1.16750598e-04],\n",
       "       [ 5.73389232e-03],\n",
       "       [ 7.80910254e-04],\n",
       "       [ 2.16177106e-03],\n",
       "       [ 1.09954178e-03],\n",
       "       [-1.68919563e-03],\n",
       "       [-1.36721134e-03],\n",
       "       [-1.35806203e-03],\n",
       "       [-4.87640500e-04],\n",
       "       [ 9.07135010e-03],\n",
       "       [ 1.51979923e-03],\n",
       "       [-1.10013783e-03],\n",
       "       [-1.14637613e-03],\n",
       "       [-1.03813410e-03],\n",
       "       [ 2.36317515e-04],\n",
       "       [ 1.36253238e-03],\n",
       "       [ 4.42060828e-03],\n",
       "       [ 1.15494430e-02],\n",
       "       [-1.61778927e-03],\n",
       "       [-1.69019401e-03],\n",
       "       [-1.54793262e-03],\n",
       "       [ 5.63293695e-03],\n",
       "       [-1.53627992e-03],\n",
       "       [ 5.28973341e-03],\n",
       "       [ 2.88465619e-03],\n",
       "       [ 1.66460872e-03],\n",
       "       [-6.75261021e-04],\n",
       "       [ 1.15965307e-03],\n",
       "       [ 3.12162936e-03],\n",
       "       [ 1.17413700e-03],\n",
       "       [-1.26434863e-03],\n",
       "       [ 1.25344098e-03],\n",
       "       [ 3.84180248e-03],\n",
       "       [ 2.82111764e-03],\n",
       "       [ 1.53093338e-02],\n",
       "       [-1.13725662e-04],\n",
       "       [ 1.04129314e-04],\n",
       "       [-1.60880387e-03],\n",
       "       [-1.85155869e-03],\n",
       "       [-6.41256571e-04],\n",
       "       [ 1.42277777e-03],\n",
       "       [ 3.23915482e-03],\n",
       "       [ 7.81956315e-03],\n",
       "       [-1.80837512e-03],\n",
       "       [-7.69093633e-04],\n",
       "       [-2.91705132e-04],\n",
       "       [ 3.42759490e-03],\n",
       "       [ 2.00659037e-04],\n",
       "       [-1.04601681e-03],\n",
       "       [-1.98233128e-03],\n",
       "       [ 7.91996717e-04],\n",
       "       [ 1.86482072e-03],\n",
       "       [ 2.54593790e-03],\n",
       "       [ 2.63287127e-03],\n",
       "       [-2.10151076e-03],\n",
       "       [ 4.72322106e-04],\n",
       "       [-2.03903019e-03],\n",
       "       [-1.04714930e-03],\n",
       "       [ 3.07182968e-03],\n",
       "       [-1.50273740e-03],\n",
       "       [ 5.07801771e-04],\n",
       "       [-1.87933445e-03],\n",
       "       [-2.56040692e-03],\n",
       "       [ 2.47992575e-03],\n",
       "       [-1.71074271e-03],\n",
       "       [-2.42403150e-03],\n",
       "       [ 9.55134630e-04],\n",
       "       [ 5.59352338e-03],\n",
       "       [ 1.11499429e-03],\n",
       "       [-2.13797390e-03],\n",
       "       [-1.74751878e-03],\n",
       "       [-2.58578360e-03],\n",
       "       [ 1.05784386e-02],\n",
       "       [-2.03073025e-04],\n",
       "       [ 1.96807086e-03],\n",
       "       [-2.47384608e-03],\n",
       "       [ 6.86281919e-03],\n",
       "       [ 4.41655517e-03],\n",
       "       [ 4.73450124e-03],\n",
       "       [ 6.02871180e-04],\n",
       "       [-1.60585344e-03],\n",
       "       [ 3.27350199e-03],\n",
       "       [ 2.21718848e-03],\n",
       "       [ 2.27990746e-03],\n",
       "       [ 6.00689650e-03],\n",
       "       [ 4.23684716e-04],\n",
       "       [-2.25439668e-03],\n",
       "       [ 2.18254328e-03],\n",
       "       [ 3.55069339e-03],\n",
       "       [ 1.14865601e-03],\n",
       "       [-2.05248594e-03],\n",
       "       [ 2.92241573e-04],\n",
       "       [ 1.28777325e-03],\n",
       "       [-8.13260674e-04],\n",
       "       [-8.49217176e-04],\n",
       "       [ 1.10228360e-03],\n",
       "       [ 6.69418275e-03],\n",
       "       [ 2.30203569e-03],\n",
       "       [ 3.83292139e-03],\n",
       "       [-7.84546137e-05],\n",
       "       [ 6.16179407e-03],\n",
       "       [ 4.73594666e-03],\n",
       "       [-5.78299165e-04],\n",
       "       [-1.65927410e-03],\n",
       "       [ 2.20865011e-03],\n",
       "       [ 9.62182879e-04],\n",
       "       [ 8.10772181e-05],\n",
       "       [ 2.72765756e-04],\n",
       "       [ 2.12478638e-03],\n",
       "       [-2.57685781e-04],\n",
       "       [-2.39421427e-03],\n",
       "       [ 2.50414014e-04],\n",
       "       [-2.35404074e-03],\n",
       "       [-8.35075974e-04],\n",
       "       [ 2.47538090e-03],\n",
       "       [ 1.48390234e-03],\n",
       "       [ 2.62698531e-03],\n",
       "       [ 5.80400229e-05],\n",
       "       [ 5.93543053e-04],\n",
       "       [-7.84546137e-05],\n",
       "       [-3.24353576e-04],\n",
       "       [ 5.77627122e-03],\n",
       "       [ 2.06857920e-04],\n",
       "       [-1.11639500e-03],\n",
       "       [-1.56807899e-03],\n",
       "       [-5.73247671e-05],\n",
       "       [ 6.05762005e-04],\n",
       "       [ 1.17672980e-03],\n",
       "       [ 3.87592614e-03],\n",
       "       [ 4.73777950e-03],\n",
       "       [-1.34198368e-03],\n",
       "       [-7.91013241e-04],\n",
       "       [ 1.25916302e-03],\n",
       "       [ 3.91513109e-04],\n",
       "       [-2.46763229e-04],\n",
       "       [ 9.41783190e-04],\n",
       "       [ 6.31719828e-04],\n",
       "       [-8.62583518e-04],\n",
       "       [ 6.48081303e-04],\n",
       "       [-1.52544677e-03],\n",
       "       [-2.05238163e-03],\n",
       "       [ 9.45180655e-05],\n",
       "       [-2.14728713e-03],\n",
       "       [ 1.57257915e-03],\n",
       "       [-1.18814409e-03],\n",
       "       [ 5.96478581e-04],\n",
       "       [ 6.84787333e-03],\n",
       "       [ 2.28559971e-03],\n",
       "       [-1.61200762e-03],\n",
       "       [-9.57950950e-04],\n",
       "       [-2.98172235e-04],\n",
       "       [ 1.55223906e-02],\n",
       "       [ 3.32795084e-03],\n",
       "       [ 8.13975930e-04],\n",
       "       [ 1.40160322e-04],\n",
       "       [ 1.10997260e-03],\n",
       "       [ 1.67889893e-03],\n",
       "       [ 1.84950233e-03],\n",
       "       [ 1.51002407e-03],\n",
       "       [-3.50177288e-05],\n",
       "       [ 3.26012075e-03],\n",
       "       [-7.03692436e-04],\n",
       "       [ 2.78964639e-04],\n",
       "       [ 1.57662481e-02],\n",
       "       [ 2.11679935e-03],\n",
       "       [-9.91433859e-04],\n",
       "       [-9.73269343e-04],\n",
       "       [ 9.20131803e-04],\n",
       "       [ 5.91823459e-03],\n",
       "       [ 1.67280436e-03],\n",
       "       [ 2.76027620e-03],\n",
       "       [ 1.50114298e-04],\n",
       "       [-1.76016986e-03],\n",
       "       [ 2.06741691e-03],\n",
       "       [ 5.35254180e-03],\n",
       "       [ 1.40130520e-03],\n",
       "       [ 3.99784744e-03],\n",
       "       [ 2.29467452e-03],\n",
       "       [-1.26664340e-03],\n",
       "       [ 2.02032924e-03],\n",
       "       [ 1.36584044e-03],\n",
       "       [ 1.18730962e-03],\n",
       "       [ 9.60618258e-04],\n",
       "       [ 3.83409858e-03],\n",
       "       [ 3.09495628e-03],\n",
       "       [-4.55558300e-04],\n",
       "       [-2.00878084e-03],\n",
       "       [ 1.36321783e-03],\n",
       "       [ 4.43275273e-03],\n",
       "       [ 3.94538045e-04],\n",
       "       [ 1.87830627e-03],\n",
       "       [ 1.23804808e-03],\n",
       "       [ 6.08274341e-03],\n",
       "       [ 7.60555267e-05],\n",
       "       [ 4.53913212e-03],\n",
       "       [ 6.41465187e-04],\n",
       "       [ 9.90614295e-04],\n",
       "       [ 7.42942095e-04],\n",
       "       [ 6.77791238e-03],\n",
       "       [ 4.13422287e-03],\n",
       "       [ 8.21694732e-04],\n",
       "       [ 1.47895515e-03],\n",
       "       [-8.09133053e-06],\n",
       "       [ 2.99865007e-03],\n",
       "       [ 2.47442722e-03],\n",
       "       [-1.35883689e-03],\n",
       "       [-1.53414905e-03],\n",
       "       [ 7.60003924e-03],\n",
       "       [ 1.03488564e-04],\n",
       "       [ 1.91217661e-03],\n",
       "       [ 6.04425371e-03],\n",
       "       [ 1.20089948e-03],\n",
       "       [ 7.35031068e-03],\n",
       "       [-1.46061182e-03],\n",
       "       [-5.77643514e-04],\n",
       "       [-1.62495673e-03],\n",
       "       [-6.88984990e-04],\n",
       "       [ 6.91592693e-04],\n",
       "       [ 2.81126797e-03],\n",
       "       [ 2.33575702e-04],\n",
       "       [ 3.23288143e-03],\n",
       "       [-9.06616449e-04],\n",
       "       [-1.18069351e-03],\n",
       "       [-9.96038318e-04],\n",
       "       [ 1.38622522e-03],\n",
       "       [ 4.44652140e-03],\n",
       "       [-7.35938549e-04],\n",
       "       [ 3.24954093e-03],\n",
       "       [-1.59630179e-03],\n",
       "       [-1.50297582e-03],\n",
       "       [-2.51191854e-03],\n",
       "       [-1.11381710e-03],\n",
       "       [ 8.74777138e-03],\n",
       "       [ 9.30652022e-04],\n",
       "       [ 2.08440423e-03],\n",
       "       [-1.46846473e-03],\n",
       "       [ 3.95278633e-03],\n",
       "       [-2.10151076e-03],\n",
       "       [ 2.60621309e-04],\n",
       "       [ 4.47714329e-03],\n",
       "       [ 8.73669982e-04],\n",
       "       [ 1.91749632e-03],\n",
       "       [ 5.42104244e-05],\n",
       "       [ 4.58987057e-03],\n",
       "       [ 8.58812034e-03],\n",
       "       [ 2.18687952e-03],\n",
       "       [ 1.59008801e-03],\n",
       "       [ 2.81178951e-03],\n",
       "       [ 1.25943124e-03],\n",
       "       [-8.77529383e-05],\n",
       "       [ 4.51925397e-03],\n",
       "       [ 3.52503359e-03],\n",
       "       [ 3.11493874e-04],\n",
       "       [ 2.45556235e-03],\n",
       "       [-3.68669629e-04],\n",
       "       [ 4.55306470e-03],\n",
       "       [ 1.38685107e-03],\n",
       "       [ 8.34116340e-03],\n",
       "       [ 5.79595566e-04],\n",
       "       [-1.81500614e-03],\n",
       "       [ 1.64270401e-04],\n",
       "       [ 3.31948698e-03],\n",
       "       [-2.20771134e-03],\n",
       "       [ 1.55793130e-03],\n",
       "       [-1.39799714e-03],\n",
       "       [ 1.45606697e-03],\n",
       "       [ 2.28714943e-03],\n",
       "       [ 2.02938914e-04],\n",
       "       [-2.60968506e-03],\n",
       "       [ 4.03991342e-03],\n",
       "       [-2.37250328e-03],\n",
       "       [-1.34760141e-03],\n",
       "       [-1.39679015e-03],\n",
       "       [ 8.45550001e-03],\n",
       "       [ 2.29364634e-03],\n",
       "       [ 4.13811207e-03],\n",
       "       [ 2.57168710e-03],\n",
       "       [ 3.17206979e-03],\n",
       "       [-1.92071497e-03],\n",
       "       [ 4.22462821e-04],\n",
       "       [ 1.42669678e-03],\n",
       "       [ 7.64876604e-04],\n",
       "       [ 2.25788355e-03],\n",
       "       [ 2.10627913e-04],\n",
       "       [-1.43206120e-03],\n",
       "       [ 3.10777128e-03],\n",
       "       [ 6.84665143e-03],\n",
       "       [ 1.56082213e-03],\n",
       "       [ 2.22589076e-03],\n",
       "       [ 1.07146353e-02],\n",
       "       [-2.27743387e-03],\n",
       "       [ 1.13096833e-03],\n",
       "       [ 1.26034021e-04],\n",
       "       [ 1.77060068e-03],\n",
       "       [ 1.22800469e-04],\n",
       "       [-3.61680984e-04],\n",
       "       [-3.94895673e-04],\n",
       "       [ 3.70916724e-03],\n",
       "       [-2.43356824e-03],\n",
       "       [-7.41779804e-04],\n",
       "       [ 4.15565073e-03],\n",
       "       [-2.12369859e-03],\n",
       "       [-1.58698857e-03],\n",
       "       [ 2.92342901e-03],\n",
       "       [ 1.61463022e-03],\n",
       "       [ 4.44135070e-03],\n",
       "       [ 1.28659606e-03],\n",
       "       [-1.77900493e-03],\n",
       "       [ 4.65124846e-04],\n",
       "       [ 1.56864524e-04],\n",
       "       [ 2.03536451e-03],\n",
       "       [-1.02952123e-03],\n",
       "       [ 3.77742946e-03],\n",
       "       [-2.95907259e-04],\n",
       "       [ 3.84950638e-03],\n",
       "       [ 3.52320075e-03],\n",
       "       [ 4.36253846e-03],\n",
       "       [ 4.49785590e-03],\n",
       "       [-1.58096850e-03],\n",
       "       [ 2.15344131e-03],\n",
       "       [-1.78192556e-03],\n",
       "       [-5.46544790e-04],\n",
       "       [-2.16987729e-03],\n",
       "       [-6.36532903e-04],\n",
       "       [-1.49552524e-03],\n",
       "       [ 3.45849991e-03],\n",
       "       [ 4.81654704e-03],\n",
       "       [ 2.74692476e-03],\n",
       "       [-2.37250328e-03],\n",
       "       [-9.15139914e-04],\n",
       "       [ 8.87066126e-05],\n",
       "       [ 1.05581433e-02],\n",
       "       [ 5.62790036e-03],\n",
       "       [-8.16211104e-04],\n",
       "       [ 1.97058916e-03],\n",
       "       [ 2.15381384e-03],\n",
       "       [ 1.27317011e-03],\n",
       "       [ 9.83193517e-04],\n",
       "       [ 1.16737187e-03],\n",
       "       [ 9.38868523e-03],\n",
       "       [-1.78578496e-03],\n",
       "       [ 2.95373797e-03],\n",
       "       [ 9.95233655e-04],\n",
       "       [ 4.37617302e-03],\n",
       "       [-1.66253746e-03],\n",
       "       [-2.63528526e-03],\n",
       "       [-1.02742016e-03],\n",
       "       [ 3.67984176e-04],\n",
       "       [ 4.01882827e-03],\n",
       "       [ 7.30887055e-03],\n",
       "       [-4.97400761e-05],\n",
       "       [ 2.77978182e-03],\n",
       "       [ 2.29546428e-03],\n",
       "       [-3.26305628e-04],\n",
       "       [ 1.29047036e-03],\n",
       "       [ 3.57761979e-03],\n",
       "       [ 8.14586878e-04],\n",
       "       [ 2.29462981e-04],\n",
       "       [-2.85476446e-04],\n",
       "       [ 2.76726484e-03],\n",
       "       [ 5.67428768e-03],\n",
       "       [-8.33943486e-04],\n",
       "       [ 8.23582709e-03],\n",
       "       [ 1.16860867e-03],\n",
       "       [ 1.13952160e-03],\n",
       "       [-1.31356716e-03],\n",
       "       [ 7.28100538e-03],\n",
       "       [ 5.20348549e-05],\n",
       "       [-9.24393535e-04],\n",
       "       [-2.05223262e-03],\n",
       "       [-4.84332442e-04],\n",
       "       [ 3.98188829e-04],\n",
       "       [-2.35256553e-03],\n",
       "       [-1.81835890e-03],\n",
       "       [ 1.41781569e-03],\n",
       "       [ 4.80991602e-03],\n",
       "       [ 3.62545252e-05],\n",
       "       [ 2.54693627e-03],\n",
       "       [ 1.23135746e-03],\n",
       "       [-6.74873590e-04],\n",
       "       [ 5.27396798e-04],\n",
       "       [ 1.99615955e-04],\n",
       "       [ 7.64021277e-03],\n",
       "       [-1.17024779e-03],\n",
       "       [ 4.75943089e-05],\n",
       "       [ 1.54525042e-03],\n",
       "       [-9.26524401e-04],\n",
       "       [ 1.75857544e-03],\n",
       "       [ 4.58060205e-03],\n",
       "       [ 1.63255632e-03],\n",
       "       [ 5.82426786e-04],\n",
       "       [-7.95722008e-04],\n",
       "       [-2.07747519e-03],\n",
       "       [-1.68661773e-03],\n",
       "       [-1.56910717e-03],\n",
       "       [-1.88663602e-04],\n",
       "       [ 8.65384936e-04],\n",
       "       [ 3.75550985e-03],\n",
       "       [-2.29595602e-03],\n",
       "       [-1.94263458e-03],\n",
       "       [ 4.14121151e-03],\n",
       "       [ 3.49473953e-03],\n",
       "       [ 2.42662430e-03],\n",
       "       [ 1.83132291e-03],\n",
       "       [ 1.65724754e-03],\n",
       "       [-1.63039565e-03],\n",
       "       [ 1.87416375e-03],\n",
       "       [ 4.08665836e-03],\n",
       "       [-2.46714056e-03],\n",
       "       [ 9.09122825e-03],\n",
       "       [-1.42106414e-03],\n",
       "       [ 2.07182765e-03],\n",
       "       [ 2.38251686e-03],\n",
       "       [ 5.34255803e-03],\n",
       "       [ 2.41187215e-03],\n",
       "       [-6.00978732e-04],\n",
       "       [ 4.73201275e-04],\n",
       "       [ 6.73472881e-04],\n",
       "       [-9.05945897e-04],\n",
       "       [ 8.27839971e-03],\n",
       "       [ 1.22644007e-03],\n",
       "       [ 4.32923436e-04],\n",
       "       [-1.60746276e-03],\n",
       "       [-1.03281438e-03],\n",
       "       [ 9.66027379e-04],\n",
       "       [-2.06905603e-03],\n",
       "       [-9.68068838e-04],\n",
       "       [-1.05145574e-03],\n",
       "       [ 4.15742397e-06],\n",
       "       [ 4.00295854e-03],\n",
       "       [ 3.95812094e-03],\n",
       "       [-1.98131800e-03],\n",
       "       [ 1.54659152e-03],\n",
       "       [ 2.38964707e-02],\n",
       "       [ 3.96937132e-03],\n",
       "       [ 3.09056044e-03],\n",
       "       [-5.78314066e-05],\n",
       "       [ 3.13454866e-03],\n",
       "       [ 1.16750598e-04],\n",
       "       [-2.14861333e-03],\n",
       "       [ 2.62278318e-03],\n",
       "       [ 7.65547156e-03],\n",
       "       [ 3.13526392e-03],\n",
       "       [-1.12205744e-03],\n",
       "       [ 1.95173919e-03],\n",
       "       [-1.22934580e-03],\n",
       "       [ 3.84554267e-04],\n",
       "       [ 6.38958812e-03],\n",
       "       [ 8.38439167e-03],\n",
       "       [ 1.88623369e-03],\n",
       "       [-1.01338327e-03],\n",
       "       [ 1.92639232e-03],\n",
       "       [ 2.32769549e-03],\n",
       "       [ 6.67826831e-03],\n",
       "       [-1.46180391e-03],\n",
       "       [ 1.11213326e-03],\n",
       "       [ 6.94751740e-04],\n",
       "       [ 9.22179222e-03],\n",
       "       [ 3.11970711e-03],\n",
       "       [ 3.76276672e-03],\n",
       "       [ 1.51909888e-03],\n",
       "       [ 2.22684443e-03],\n",
       "       [ 1.82212889e-03],\n",
       "       [ 1.57780945e-03],\n",
       "       [ 1.21423900e-02],\n",
       "       [-1.78441405e-03],\n",
       "       [-1.62363052e-03],\n",
       "       [ 1.67427957e-03],\n",
       "       [-1.73333287e-03],\n",
       "       [ 2.34554708e-03],\n",
       "       [ 1.16522610e-03],\n",
       "       [ 2.11338699e-03],\n",
       "       [-8.46311450e-04],\n",
       "       [ 4.35501337e-04],\n",
       "       [ 1.90426409e-03],\n",
       "       [ 2.09769607e-03],\n",
       "       [-2.37897038e-04],\n",
       "       [ 5.83924353e-03],\n",
       "       [ 1.17418170e-03],\n",
       "       [ 5.47802448e-03],\n",
       "       [-8.69423151e-04],\n",
       "       [ 5.11422753e-04],\n",
       "       [-2.74389982e-04],\n",
       "       [-6.14687800e-04],\n",
       "       [ 2.30516493e-03],\n",
       "       [ 2.09450722e-04],\n",
       "       [ 5.02735376e-04],\n",
       "       [ 6.12065196e-04],\n",
       "       [ 1.57231092e-03],\n",
       "       [-2.26922333e-03],\n",
       "       [-7.72565603e-04],\n",
       "       [ 1.47743523e-03],\n",
       "       [-1.43557787e-03],\n",
       "       [ 2.54927576e-03],\n",
       "       [-7.30752945e-04],\n",
       "       [-1.06483698e-03],\n",
       "       [-1.90183520e-04],\n",
       "       [ 3.52294743e-03],\n",
       "       [ 7.64440000e-03],\n",
       "       [-1.57265365e-03],\n",
       "       [-1.37166679e-03],\n",
       "       [ 4.31886315e-03],\n",
       "       [ 5.39049506e-04],\n",
       "       [ 2.58864462e-03],\n",
       "       [-1.14409626e-03],\n",
       "       [-1.72385573e-03],\n",
       "       [ 3.59356403e-04],\n",
       "       [ 3.83621454e-03],\n",
       "       [ 2.59822607e-03],\n",
       "       [ 6.30463660e-03],\n",
       "       [-1.67421997e-03],\n",
       "       [-2.10151076e-03],\n",
       "       [ 3.42412293e-03],\n",
       "       [-1.46460533e-03],\n",
       "       [ 1.01026893e-03],\n",
       "       [ 2.54912674e-03],\n",
       "       [ 2.76482105e-03],\n",
       "       [ 4.71036136e-03],\n",
       "       [ 1.88648701e-05],\n",
       "       [ 6.65193796e-03],\n",
       "       [ 4.02776897e-03],\n",
       "       [ 4.22835350e-03],\n",
       "       [ 2.66954303e-04],\n",
       "       [ 4.24180925e-03],\n",
       "       [-4.68999147e-04],\n",
       "       [ 5.31092286e-03],\n",
       "       [-1.37700140e-03],\n",
       "       [ 2.07822025e-03],\n",
       "       [ 1.59993768e-03],\n",
       "       [-1.89773738e-03],\n",
       "       [-1.76596642e-03],\n",
       "       [ 3.25134397e-03],\n",
       "       [ 4.88990545e-03],\n",
       "       [ 2.79943645e-03],\n",
       "       [-3.34978104e-04],\n",
       "       [-1.93148851e-04],\n",
       "       [ 3.04129720e-03],\n",
       "       [ 3.67555022e-03],\n",
       "       [ 2.71037221e-04],\n",
       "       [ 3.61731648e-03],\n",
       "       [ 2.32660770e-03],\n",
       "       [-7.53536820e-04],\n",
       "       [-1.82619691e-03],\n",
       "       [ 6.27630949e-03],\n",
       "       [ 8.97124410e-04],\n",
       "       [-1.84595585e-04],\n",
       "       [-8.31216574e-04],\n",
       "       [-1.87061727e-03],\n",
       "       [-6.88746572e-04],\n",
       "       [ 2.92246044e-03],\n",
       "       [-5.85988164e-04],\n",
       "       [-1.76385045e-04],\n",
       "       [-1.62203610e-03],\n",
       "       [ 3.17707658e-04],\n",
       "       [ 4.10427153e-03],\n",
       "       [ 4.67553735e-04],\n",
       "       [ 8.21986794e-03],\n",
       "       [-1.11478567e-03],\n",
       "       [-2.21937895e-03],\n",
       "       [ 3.11434269e-04],\n",
       "       [ 9.68798995e-04],\n",
       "       [-1.60482526e-03],\n",
       "       [-1.57052279e-03],\n",
       "       [-3.01697850e-03],\n",
       "       [ 3.73280048e-03],\n",
       "       [ 2.66537070e-04],\n",
       "       [ 2.30550766e-03],\n",
       "       [ 2.83761322e-03],\n",
       "       [ 2.83898413e-03],\n",
       "       [ 1.72169507e-03],\n",
       "       [ 1.37783587e-03],\n",
       "       [-1.48360431e-03],\n",
       "       [ 1.73391402e-03],\n",
       "       [ 4.43863869e-03],\n",
       "       [ 3.70740891e-04],\n",
       "       [-7.22557306e-05],\n",
       "       [-6.00367785e-04],\n",
       "       [-2.63531506e-03],\n",
       "       [-1.19116902e-03],\n",
       "       [-6.19664788e-04],\n",
       "       [-1.80122256e-03],\n",
       "       [-2.31114030e-03],\n",
       "       [-8.67247581e-06],\n",
       "       [ 5.58552146e-03],\n",
       "       [-5.41210175e-05],\n",
       "       [-1.43064559e-03],\n",
       "       [ 3.59764695e-03],\n",
       "       [ 7.12820888e-03],\n",
       "       [-5.29587269e-04],\n",
       "       [ 1.59582496e-03],\n",
       "       [ 3.34317982e-03],\n",
       "       [ 3.99354100e-03],\n",
       "       [-1.72586739e-03],\n",
       "       [-1.19701028e-03],\n",
       "       [ 3.34914029e-03],\n",
       "       [ 3.25712562e-03],\n",
       "       [-1.05722249e-03],\n",
       "       [ 1.29131973e-03],\n",
       "       [ 9.63360071e-05],\n",
       "       [-1.45484507e-03],\n",
       "       [ 4.08440828e-05],\n",
       "       [-1.82363391e-03],\n",
       "       [ 7.45785236e-03],\n",
       "       [ 4.88908589e-03],\n",
       "       [ 2.94582546e-03],\n",
       "       [ 4.48510051e-04],\n",
       "       [ 6.70015812e-04],\n",
       "       [ 7.84546137e-05],\n",
       "       [ 1.16179883e-03],\n",
       "       [ 3.89203429e-04],\n",
       "       [ 1.04534626e-03],\n",
       "       [ 5.24826348e-03],\n",
       "       [-2.10481882e-03],\n",
       "       [-1.22405589e-03],\n",
       "       [ 4.20421362e-04],\n",
       "       [ 6.50709867e-03],\n",
       "       [-2.13302672e-03],\n",
       "       [-1.27574801e-03],\n",
       "       [ 9.06661153e-04],\n",
       "       [ 2.12731957e-03],\n",
       "       [-2.37250328e-03],\n",
       "       [ 2.63373554e-03],\n",
       "       [-6.49601221e-04],\n",
       "       [ 8.21620226e-04],\n",
       "       [-1.46089494e-03],\n",
       "       [-1.01813674e-03],\n",
       "       [-1.69019401e-03],\n",
       "       [ 7.43269920e-05],\n",
       "       [-9.09566879e-04],\n",
       "       [ 5.57452440e-05],\n",
       "       [-7.12096691e-04],\n",
       "       [-6.58765435e-04],\n",
       "       [ 6.05727732e-03],\n",
       "       [ 6.18582964e-03],\n",
       "       [ 4.57546115e-03],\n",
       "       [ 1.44670904e-03],\n",
       "       [ 2.15819478e-03],\n",
       "       [ 4.69131768e-03],\n",
       "       [ 5.71459532e-04],\n",
       "       [ 5.70319593e-03],\n",
       "       [-1.37299299e-04],\n",
       "       [-5.49241900e-04],\n",
       "       [ 3.93454731e-03],\n",
       "       [ 1.04068965e-02],\n",
       "       [ 2.15262175e-04],\n",
       "       [ 2.98793614e-03],\n",
       "       [ 9.39556956e-03],\n",
       "       [ 5.74573874e-04],\n",
       "       [-1.07520819e-03],\n",
       "       [ 2.36225128e-03],\n",
       "       [ 4.97078896e-03],\n",
       "       [-6.30438328e-04],\n",
       "       [-1.85510516e-03],\n",
       "       [ 3.34346294e-03],\n",
       "       [ 5.11802733e-03],\n",
       "       [-9.89645720e-04],\n",
       "       [-1.14884973e-03],\n",
       "       [ 5.19424677e-04],\n",
       "       [ 5.28027117e-03],\n",
       "       [ 6.54463470e-03],\n",
       "       [ 1.62094831e-04],\n",
       "       [ 4.88515198e-03],\n",
       "       [ 3.57562304e-03],\n",
       "       [ 1.14861131e-03],\n",
       "       [ 4.74095345e-04],\n",
       "       [-1.57442689e-03],\n",
       "       [ 7.49528408e-03],\n",
       "       [-7.06434250e-04],\n",
       "       [ 1.42177939e-03],\n",
       "       [ 1.24214590e-03],\n",
       "       [ 2.87081301e-03],\n",
       "       [-5.94943762e-04],\n",
       "       [ 1.00582838e-05],\n",
       "       [-2.11185217e-03],\n",
       "       [ 1.45816803e-03],\n",
       "       [ 5.92136383e-03],\n",
       "       [ 5.59648871e-03],\n",
       "       [ 8.20371509e-03],\n",
       "       [ 2.56435573e-03],\n",
       "       [-5.17666340e-05],\n",
       "       [ 5.02039492e-03],\n",
       "       [ 3.66088748e-03],\n",
       "       [-6.07147813e-04],\n",
       "       [ 7.27881491e-03],\n",
       "       [ 2.43708491e-04],\n",
       "       [ 4.83721495e-04],\n",
       "       [-1.17404759e-03],\n",
       "       [ 3.97245586e-03],\n",
       "       [ 3.21009755e-03],\n",
       "       [ 2.57338583e-03],\n",
       "       [ 3.98455560e-03],\n",
       "       [ 6.91697001e-04],\n",
       "       [ 1.32435560e-03],\n",
       "       [ 1.24849379e-03],\n",
       "       [ 2.61768699e-03],\n",
       "       [ 3.53777409e-03],\n",
       "       [ 1.22904778e-04],\n",
       "       [ 9.43228602e-04],\n",
       "       [ 5.44916093e-03],\n",
       "       [ 8.02543759e-03],\n",
       "       [ 3.26156616e-04],\n",
       "       [ 1.21326745e-03],\n",
       "       [ 1.14105642e-03],\n",
       "       [ 3.73572111e-04],\n",
       "       [ 6.72745705e-03],\n",
       "       [ 6.74083829e-04],\n",
       "       [ 9.29147005e-04],\n",
       "       [-1.00247562e-03],\n",
       "       [ 2.63455510e-03],\n",
       "       [ 1.88623369e-03],\n",
       "       [ 5.85414469e-03],\n",
       "       [ 2.43046880e-03],\n",
       "       [ 1.65531039e-03],\n",
       "       [-1.02195144e-03],\n",
       "       [-1.20356679e-04],\n",
       "       [ 1.27317011e-03],\n",
       "       [ 6.37450814e-03],\n",
       "       [ 2.31750309e-03],\n",
       "       [-8.85441899e-04],\n",
       "       [ 4.54090536e-03],\n",
       "       [ 3.96847725e-03],\n",
       "       [ 6.88001513e-04],\n",
       "       [ 2.46255100e-03],\n",
       "       [-7.42092729e-04],\n",
       "       [-3.60339880e-04],\n",
       "       [ 6.74352050e-04],\n",
       "       [ 1.33557618e-03],\n",
       "       [ 2.30185688e-03],\n",
       "       [ 6.48707151e-04],\n",
       "       [ 2.70777941e-03],\n",
       "       [ 3.99023294e-04],\n",
       "       [ 6.05523586e-04],\n",
       "       [-3.90395522e-04],\n",
       "       [ 2.53739953e-03],\n",
       "       [ 6.14561141e-03],\n",
       "       [ 2.72557139e-04],\n",
       "       [-1.73747540e-05],\n",
       "       [ 7.56070018e-04],\n",
       "       [ 2.95364857e-03],\n",
       "       [ 3.02229822e-03],\n",
       "       [-1.49905682e-04],\n",
       "       [-1.19204819e-03],\n",
       "       [-1.48563087e-03],\n",
       "       [-1.12576783e-03],\n",
       "       [ 2.12450325e-03],\n",
       "       [-2.56016850e-04],\n",
       "       [ 1.05038285e-03],\n",
       "       [ 4.10570204e-03],\n",
       "       [-1.29297376e-03],\n",
       "       [ 8.57353210e-04],\n",
       "       [ 9.37134027e-05],\n",
       "       [-7.59303570e-04],\n",
       "       [ 3.56289744e-03],\n",
       "       [-2.01883912e-03],\n",
       "       [-3.97801399e-04],\n",
       "       [-4.76241112e-05],\n",
       "       [-2.11606920e-03],\n",
       "       [-1.24041736e-03],\n",
       "       [ 5.02601266e-04],\n",
       "       [-5.12927771e-04],\n",
       "       [ 2.49475241e-04],\n",
       "       [ 7.54198432e-03],\n",
       "       [ 2.78909504e-03],\n",
       "       [ 5.30761480e-03],\n",
       "       [ 2.98053026e-03],\n",
       "       [ 3.76199186e-03],\n",
       "       [ 1.17272139e-03],\n",
       "       [ 2.57343054e-05],\n",
       "       [ 2.65184045e-03],\n",
       "       [ 5.42151928e-03],\n",
       "       [ 6.63049519e-03],\n",
       "       [ 1.36432052e-02],\n",
       "       [-1.54840946e-03],\n",
       "       [ 1.41945481e-03],\n",
       "       [ 1.11956894e-03],\n",
       "       [ 1.37439370e-03],\n",
       "       [ 3.57562304e-03],\n",
       "       [ 6.05449080e-04],\n",
       "       [ 5.80742955e-04],\n",
       "       [ 1.99696422e-03],\n",
       "       [-3.69891524e-04],\n",
       "       [ 3.21623683e-03],\n",
       "       [ 4.88208234e-03],\n",
       "       [ 3.12620401e-03],\n",
       "       [ 3.52059305e-03],\n",
       "       [ 5.39585948e-04],\n",
       "       [-6.20752573e-04],\n",
       "       [-4.52741981e-04],\n",
       "       [ 6.92969561e-03],\n",
       "       [-1.33171678e-03],\n",
       "       [ 1.12232268e-02],\n",
       "       [-5.45889139e-04],\n",
       "       [-8.45223665e-04],\n",
       "       [ 2.23277509e-03],\n",
       "       [ 8.05893540e-03],\n",
       "       [-5.50121069e-04],\n",
       "       [ 2.58845091e-03],\n",
       "       [ 1.73813105e-03],\n",
       "       [-1.38001144e-03],\n",
       "       [ 7.49641657e-03],\n",
       "       [-5.50985336e-04]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_2seq_sigmoid_loaded = create_model_a_2seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "model_a_2seq_sigmoid_loaded.load_weights(checkpoint_filepath)\n",
    "model_a_2seq_sigmoid_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_sigmoid_pred = model_a_2seq_sigmoid_loaded.predict(model_a_X_test_padded)\n",
    "model_a_2seq_sigmoid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 1s 2ms/sample - loss: 0.0071 - mae: 0.0357\n",
      "Training MSE: 0.07739601327729813\n",
      "Test MSE: 0.08426806740665316\n",
      "Baseline MSE: 0.08418924631944838\n"
     ]
    }
   ],
   "source": [
    "dev_loss, dev_acc = model_a_2seq_sigmoid_loaded.evaluate(model_a_X_test_padded,  model_a_y_test, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_train, model_a_2seq_sigmoid_loaded.predict(model_a_X_train_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, model_a_2seq_sigmoid_loaded.predict(model_a_X_test_padded)) ) }\")\n",
    "print(f\"Baseline MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, 0*model_a_y_test ) ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_a_2seq_sigmoid_history.history['loss'])\n",
    "plt.plot(model_a_2seq_sigmoid_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LSTM, sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 159, 100)          522200    \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 1680      \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 523,905\n",
      "Trainable params: 1,705\n",
      "Non-trainable params: 522,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a_1seq_sigmoid = create_model_a_1seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_1seq_sigmoid.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1372 samples, validate on 344 samples\n",
      "Epoch 1/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 1.6349 - mae: 1.2503\n",
      "Epoch 00001: val_loss improved from inf to 1.16077, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 1.6261 - mae: 1.2465 - val_loss: 1.1608 - val_mae: 1.0748\n",
      "Epoch 2/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 1.0058 - mae: 0.9727\n",
      "Epoch 00002: val_loss improved from 1.16077 to 0.69606, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 1.0002 - mae: 0.9694 - val_loss: 0.6961 - val_mae: 0.8298\n",
      "Epoch 3/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6437 - mae: 0.7705\n",
      "Epoch 00003: val_loss improved from 0.69606 to 0.43216, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.6402 - mae: 0.7686 - val_loss: 0.4322 - val_mae: 0.6514\n",
      "Epoch 4/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.4391 - mae: 0.6254\n",
      "Epoch 00004: val_loss improved from 0.43216 to 0.27563, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.4370 - mae: 0.6234 - val_loss: 0.2756 - val_mae: 0.5180\n",
      "Epoch 5/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.3036 - mae: 0.5079\n",
      "Epoch 00005: val_loss improved from 0.27563 to 0.17906, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.3014 - mae: 0.5062 - val_loss: 0.1791 - val_mae: 0.4155\n",
      "Epoch 6/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.2161 - mae: 0.4192\n",
      "Epoch 00006: val_loss improved from 0.17906 to 0.11730, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.2159 - mae: 0.4191 - val_loss: 0.1173 - val_mae: 0.3338\n",
      "Epoch 7/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.1612 - mae: 0.3522\n",
      "Epoch 00007: val_loss improved from 0.11730 to 0.07668, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.1615 - mae: 0.3523 - val_loss: 0.0767 - val_mae: 0.2671\n",
      "Epoch 8/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.1212 - mae: 0.2963\n",
      "Epoch 00008: val_loss improved from 0.07668 to 0.05034, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.1195 - mae: 0.2932 - val_loss: 0.0503 - val_mae: 0.2132\n",
      "Epoch 9/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0924 - mae: 0.2483\n",
      "Epoch 00009: val_loss improved from 0.05034 to 0.03336, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0917 - mae: 0.2474 - val_loss: 0.0334 - val_mae: 0.1699\n",
      "Epoch 10/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0779 - mae: 0.2212\n",
      "Epoch 00010: val_loss improved from 0.03336 to 0.02206, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0781 - mae: 0.2215 - val_loss: 0.0221 - val_mae: 0.1341\n",
      "Epoch 11/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0613 - mae: 0.1923\n",
      "Epoch 00011: val_loss improved from 0.02206 to 0.01463, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0611 - mae: 0.1921 - val_loss: 0.0146 - val_mae: 0.1039\n",
      "Epoch 12/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0483 - mae: 0.1684\n",
      "Epoch 00012: val_loss improved from 0.01463 to 0.01023, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0480 - mae: 0.1680 - val_loss: 0.0102 - val_mae: 0.0812\n",
      "Epoch 13/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0454 - mae: 0.1592\n",
      "Epoch 00013: val_loss improved from 0.01023 to 0.00766, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0448 - mae: 0.1583 - val_loss: 0.0077 - val_mae: 0.0642\n",
      "Epoch 14/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.1466\n",
      "Epoch 00014: val_loss improved from 0.00766 to 0.00631, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0379 - mae: 0.1462 - val_loss: 0.0063 - val_mae: 0.0530\n",
      "Epoch 15/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0363 - mae: 0.1410\n",
      "Epoch 00015: val_loss improved from 0.00631 to 0.00561, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0365 - mae: 0.1413 - val_loss: 0.0056 - val_mae: 0.0483\n",
      "Epoch 16/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0338 - mae: 0.1415\n",
      "Epoch 00016: val_loss improved from 0.00561 to 0.00539, saving model to ./model_a_checkpoint/model_a_1seq_sigmoid 11112020 1901h.h5\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0337 - mae: 0.1413 - val_loss: 0.0054 - val_mae: 0.0468\n",
      "Epoch 17/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.1320\n",
      "Epoch 00017: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0288 - mae: 0.1316 - val_loss: 0.0055 - val_mae: 0.0473\n",
      "Epoch 18/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0315 - mae: 0.1327\n",
      "Epoch 00018: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0312 - mae: 0.1326 - val_loss: 0.0057 - val_mae: 0.0487\n",
      "Epoch 19/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.1325\n",
      "Epoch 00019: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0306 - mae: 0.1328 - val_loss: 0.0060 - val_mae: 0.0512\n",
      "Epoch 20/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0297 - mae: 0.1321\n",
      "Epoch 00020: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0295 - mae: 0.1322 - val_loss: 0.0064 - val_mae: 0.0540\n",
      "Epoch 21/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.1312\n",
      "Epoch 00021: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0285 - mae: 0.1309 - val_loss: 0.0068 - val_mae: 0.0571\n",
      "Epoch 22/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0283 - mae: 0.1304\n",
      "Epoch 00022: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0279 - mae: 0.1295 - val_loss: 0.0070 - val_mae: 0.0589\n",
      "Epoch 23/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0271 - mae: 0.1258\n",
      "Epoch 00023: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0270 - mae: 0.1260 - val_loss: 0.0073 - val_mae: 0.0609\n",
      "Epoch 24/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.1245\n",
      "Epoch 00024: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0254 - mae: 0.1244 - val_loss: 0.0073 - val_mae: 0.0612\n",
      "Epoch 25/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0283 - mae: 0.1279\n",
      "Epoch 00025: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0283 - mae: 0.1282 - val_loss: 0.0075 - val_mae: 0.0626\n",
      "Epoch 26/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.1241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00026: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0264 - mae: 0.1236 - val_loss: 0.0077 - val_mae: 0.0638\n",
      "Epoch 27/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.1215\n",
      "Epoch 00027: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0249 - mae: 0.1217 - val_loss: 0.0078 - val_mae: 0.0647\n",
      "Epoch 28/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.1220\n",
      "Epoch 00028: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0251 - mae: 0.1222 - val_loss: 0.0077 - val_mae: 0.0641\n",
      "Epoch 29/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.1203\n",
      "Epoch 00029: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0236 - mae: 0.1204 - val_loss: 0.0076 - val_mae: 0.0636\n",
      "Epoch 30/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0253 - mae: 0.1239\n",
      "Epoch 00030: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0250 - mae: 0.1232 - val_loss: 0.0078 - val_mae: 0.0646\n",
      "Epoch 31/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0223 - mae: 0.1145\n",
      "Epoch 00031: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0230 - mae: 0.1159 - val_loss: 0.0076 - val_mae: 0.0637\n",
      "Epoch 32/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.1191\n",
      "Epoch 00032: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0236 - mae: 0.1192 - val_loss: 0.0075 - val_mae: 0.0628\n",
      "Epoch 33/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.1174\n",
      "Epoch 00033: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0237 - mae: 0.1174 - val_loss: 0.0074 - val_mae: 0.0620\n",
      "Epoch 34/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.1153\n",
      "Epoch 00034: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0226 - mae: 0.1156 - val_loss: 0.0073 - val_mae: 0.0611\n",
      "Epoch 35/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0229 - mae: 0.1149\n",
      "Epoch 00035: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0226 - mae: 0.1143 - val_loss: 0.0073 - val_mae: 0.0612\n",
      "Epoch 36/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0229 - mae: 0.1159\n",
      "Epoch 00036: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0227 - mae: 0.1157 - val_loss: 0.0073 - val_mae: 0.0613\n",
      "Epoch 37/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1141\n",
      "Epoch 00037: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0219 - mae: 0.1142 - val_loss: 0.0073 - val_mae: 0.0613\n",
      "Epoch 38/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1166\n",
      "Epoch 00038: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0227 - mae: 0.1164 - val_loss: 0.0073 - val_mae: 0.0613\n",
      "Epoch 39/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.1105\n",
      "Epoch 00039: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0214 - mae: 0.1108 - val_loss: 0.0073 - val_mae: 0.0615\n",
      "Epoch 40/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.1105\n",
      "Epoch 00040: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0209 - mae: 0.1115 - val_loss: 0.0072 - val_mae: 0.0605\n",
      "Epoch 41/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.1092\n",
      "Epoch 00041: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0201 - mae: 0.1087 - val_loss: 0.0070 - val_mae: 0.0594\n",
      "Epoch 42/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0215 - mae: 0.1131\n",
      "Epoch 00042: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0215 - mae: 0.1127 - val_loss: 0.0069 - val_mae: 0.0589\n",
      "Epoch 43/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0198 - mae: 0.1047\n",
      "Epoch 00043: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0196 - mae: 0.1045 - val_loss: 0.0069 - val_mae: 0.0590\n",
      "Epoch 44/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0202 - mae: 0.1078\n",
      "Epoch 00044: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0200 - mae: 0.1071 - val_loss: 0.0069 - val_mae: 0.0590\n",
      "Epoch 45/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.1073\n",
      "Epoch 00045: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0199 - mae: 0.1076 - val_loss: 0.0070 - val_mae: 0.0597\n",
      "Epoch 46/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0189 - mae: 0.1021\n",
      "Epoch 00046: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0187 - mae: 0.1016 - val_loss: 0.0068 - val_mae: 0.0583\n",
      "Epoch 47/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0196 - mae: 0.1078\n",
      "Epoch 00047: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0193 - mae: 0.1070 - val_loss: 0.0068 - val_mae: 0.0577\n",
      "Epoch 48/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0178 - mae: 0.1005\n",
      "Epoch 00048: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0177 - mae: 0.1001 - val_loss: 0.0066 - val_mae: 0.0567\n",
      "Epoch 49/50\n",
      "1312/1372 [===========================>..] - ETA: 0s - loss: 0.0173 - mae: 0.1003\n",
      "Epoch 00049: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 0.0171 - mae: 0.0996 - val_loss: 0.0065 - val_mae: 0.0553\n",
      "Epoch 50/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.0176 - mae: 0.1019\n",
      "Epoch 00050: val_loss did not improve from 0.00539\n",
      "1372/1372 [==============================] - 2s 2ms/sample - loss: 0.0176 - mae: 0.1019 - val_loss: 0.0063 - val_mae: 0.0541\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_1seq_sigmoid {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_1seq_sigmoid_history = model_a_1seq_sigmoid.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.78998613e-02],\n",
       "       [-4.34081852e-02],\n",
       "       [-3.82755101e-02],\n",
       "       [-1.06232166e-02],\n",
       "       [ 3.12184691e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [-1.22154653e-02],\n",
       "       [ 2.03187764e-02],\n",
       "       [ 2.89802551e-02],\n",
       "       [-1.92942917e-02],\n",
       "       [-3.30637991e-02],\n",
       "       [-1.33588016e-02],\n",
       "       [ 2.78736651e-02],\n",
       "       [ 3.12002301e-02],\n",
       "       [ 1.15357935e-02],\n",
       "       [ 3.11525166e-02],\n",
       "       [-3.06035876e-02],\n",
       "       [ 1.41701698e-02],\n",
       "       [ 1.67392790e-02],\n",
       "       [-2.47534215e-02],\n",
       "       [ 5.63243032e-03],\n",
       "       [-4.16976213e-03],\n",
       "       [ 1.07738376e-03],\n",
       "       [ 1.67509913e-02],\n",
       "       [ 2.67775357e-02],\n",
       "       [ 3.07455957e-02],\n",
       "       [ 2.43682563e-02],\n",
       "       [-3.51713002e-02],\n",
       "       [ 1.84684992e-04],\n",
       "       [ 8.77097249e-03],\n",
       "       [ 1.84537470e-02],\n",
       "       [-1.84258819e-03],\n",
       "       [-2.52652466e-02],\n",
       "       [ 1.89618468e-02],\n",
       "       [ 1.25130117e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [-1.39169395e-02],\n",
       "       [-5.55497408e-03],\n",
       "       [-1.71891153e-02],\n",
       "       [-2.67276168e-03],\n",
       "       [ 2.86117494e-02],\n",
       "       [ 4.80765104e-03],\n",
       "       [ 2.78495848e-02],\n",
       "       [-3.63877118e-02],\n",
       "       [-6.28197193e-03],\n",
       "       [-4.65510786e-02],\n",
       "       [ 3.05721164e-02],\n",
       "       [-1.62548721e-02],\n",
       "       [ 2.74720192e-02],\n",
       "       [-3.95207107e-02],\n",
       "       [ 2.48521566e-02],\n",
       "       [ 2.74721682e-02],\n",
       "       [ 3.04548144e-02],\n",
       "       [-4.61990237e-02],\n",
       "       [ 1.37847066e-02],\n",
       "       [ 2.00724602e-03],\n",
       "       [-4.83930111e-04],\n",
       "       [ 2.24346817e-02],\n",
       "       [-1.14688873e-02],\n",
       "       [ 3.09747756e-02],\n",
       "       [-2.69914865e-02],\n",
       "       [-5.83159328e-02],\n",
       "       [ 3.07828784e-02],\n",
       "       [ 2.65919566e-02],\n",
       "       [ 3.56677175e-03],\n",
       "       [-7.49084353e-03],\n",
       "       [ 2.37387717e-02],\n",
       "       [-4.05983627e-02],\n",
       "       [ 1.03490353e-02],\n",
       "       [ 1.96292698e-02],\n",
       "       [-1.12016201e-02],\n",
       "       [ 1.93258822e-02],\n",
       "       [ 1.97224915e-02],\n",
       "       [-3.67726386e-02],\n",
       "       [-3.30442488e-02],\n",
       "       [ 2.17535794e-02],\n",
       "       [ 8.21706653e-03],\n",
       "       [-1.52527392e-02],\n",
       "       [-3.82668972e-02],\n",
       "       [-6.28206134e-03],\n",
       "       [ 3.07443440e-02],\n",
       "       [ 3.06269825e-02],\n",
       "       [ 2.48521566e-02],\n",
       "       [-4.03787494e-02],\n",
       "       [ 2.97231376e-02],\n",
       "       [ 3.03183496e-02],\n",
       "       [-3.80836725e-02],\n",
       "       [ 2.79015601e-02],\n",
       "       [ 7.60039687e-03],\n",
       "       [-2.62731016e-02],\n",
       "       [ 2.15321779e-03],\n",
       "       [ 2.33427286e-02],\n",
       "       [ 3.06218266e-02],\n",
       "       [-1.60925090e-02],\n",
       "       [ 2.17028260e-02],\n",
       "       [ 3.10684741e-02],\n",
       "       [ 3.03387642e-04],\n",
       "       [ 3.12116146e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 3.02905440e-02],\n",
       "       [ 3.03750932e-02],\n",
       "       [ 2.04114616e-02],\n",
       "       [ 2.54257023e-02],\n",
       "       [ 2.89816558e-02],\n",
       "       [-6.98304176e-03],\n",
       "       [ 2.65611708e-02],\n",
       "       [ 9.71028209e-03],\n",
       "       [-3.83110940e-02],\n",
       "       [ 2.74812877e-02],\n",
       "       [ 2.89817750e-02],\n",
       "       [ 3.05721462e-02],\n",
       "       [ 3.11199725e-02],\n",
       "       [ 2.34016776e-02],\n",
       "       [ 1.67399943e-02],\n",
       "       [-8.85060430e-03],\n",
       "       [ 1.63108408e-02],\n",
       "       [-1.02137625e-02],\n",
       "       [ 1.99123323e-02],\n",
       "       [-9.58088040e-03],\n",
       "       [ 2.88909972e-02],\n",
       "       [-1.76890194e-02],\n",
       "       [ 2.91754305e-02],\n",
       "       [ 3.12079191e-02],\n",
       "       [-1.95559859e-02],\n",
       "       [ 2.99482048e-02],\n",
       "       [ 1.08018816e-02],\n",
       "       [-3.32328975e-02],\n",
       "       [ 2.78575122e-02],\n",
       "       [-2.55930722e-02],\n",
       "       [ 2.94145942e-03],\n",
       "       [ 2.86400914e-02],\n",
       "       [-1.85216367e-02],\n",
       "       [-4.22269404e-02],\n",
       "       [-1.35429502e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 3.06219757e-02],\n",
       "       [-1.47402585e-02],\n",
       "       [-2.94641554e-02],\n",
       "       [-5.29685616e-03],\n",
       "       [ 2.94312537e-02],\n",
       "       [-5.76060414e-02],\n",
       "       [ 3.09475362e-02],\n",
       "       [-3.65911126e-02],\n",
       "       [-1.86495483e-02],\n",
       "       [-4.97452915e-02],\n",
       "       [ 2.35392749e-02],\n",
       "       [-3.63546312e-02],\n",
       "       [ 2.71962583e-02],\n",
       "       [ 2.95431018e-02],\n",
       "       [ 3.06195915e-02],\n",
       "       [ 2.84400880e-02],\n",
       "       [-2.74035931e-02],\n",
       "       [-6.88433647e-06],\n",
       "       [-2.24193931e-03],\n",
       "       [ 2.86402404e-02],\n",
       "       [-3.92919481e-02],\n",
       "       [ 2.87927389e-02],\n",
       "       [ 2.91041434e-02],\n",
       "       [ 3.11982632e-02],\n",
       "       [-1.42785907e-03],\n",
       "       [-4.26928103e-02],\n",
       "       [-2.16889083e-02],\n",
       "       [-1.85745955e-03],\n",
       "       [-1.86495483e-02],\n",
       "       [-9.86406207e-03],\n",
       "       [-2.44857371e-02],\n",
       "       [-6.36431575e-03],\n",
       "       [ 3.05951536e-02],\n",
       "       [ 9.29445028e-04],\n",
       "       [ 2.86057889e-02],\n",
       "       [-4.55356538e-02],\n",
       "       [-2.26113498e-02],\n",
       "       [ 3.09341848e-02],\n",
       "       [ 2.35390663e-02],\n",
       "       [ 1.46090686e-02],\n",
       "       [ 2.21779943e-03],\n",
       "       [-1.26518905e-02],\n",
       "       [-6.28122687e-03],\n",
       "       [-2.58638561e-02],\n",
       "       [ 2.73285210e-02],\n",
       "       [ 3.08389366e-02],\n",
       "       [ 3.37883830e-03],\n",
       "       [ 3.11906636e-02],\n",
       "       [ 2.48521864e-02],\n",
       "       [ 2.34192610e-02],\n",
       "       [ 2.98111439e-02],\n",
       "       [ 9.71019268e-03],\n",
       "       [-3.18181515e-02],\n",
       "       [ 7.75101781e-03],\n",
       "       [-1.06938183e-02],\n",
       "       [-4.44999933e-02],\n",
       "       [-2.34394968e-02],\n",
       "       [ 3.03023756e-02],\n",
       "       [-1.90258026e-03],\n",
       "       [ 4.75499034e-03],\n",
       "       [-4.24821377e-02],\n",
       "       [ 2.67483294e-02],\n",
       "       [ 3.08124721e-02],\n",
       "       [ 2.50068307e-02],\n",
       "       [-6.29970431e-03],\n",
       "       [ 1.86198354e-02],\n",
       "       [-2.51987278e-02],\n",
       "       [ 3.03625166e-02],\n",
       "       [ 2.94804275e-02],\n",
       "       [ 3.04636955e-02],\n",
       "       [ 2.43306458e-02],\n",
       "       [-6.36416674e-03],\n",
       "       [-5.31482697e-02],\n",
       "       [-9.83521342e-03],\n",
       "       [ 2.87924707e-02],\n",
       "       [ 3.05455625e-02],\n",
       "       [-5.01975417e-03],\n",
       "       [ 3.12055349e-02],\n",
       "       [-2.84892619e-02],\n",
       "       [ 3.00817788e-02],\n",
       "       [ 3.05787623e-02],\n",
       "       [ 1.28543675e-02],\n",
       "       [-1.98262632e-02],\n",
       "       [-5.19400835e-03],\n",
       "       [-1.65636241e-02],\n",
       "       [-2.41834223e-02],\n",
       "       [-1.24043226e-02],\n",
       "       [ 3.03023160e-02],\n",
       "       [-3.38559151e-02],\n",
       "       [-3.63048017e-02],\n",
       "       [-6.98363781e-03],\n",
       "       [ 2.24649608e-02],\n",
       "       [-1.28927231e-02],\n",
       "       [-1.49306953e-02],\n",
       "       [ 3.11637819e-02],\n",
       "       [-1.94728374e-04],\n",
       "       [ 3.11456621e-02],\n",
       "       [-4.69365716e-03],\n",
       "       [ 3.10887396e-02],\n",
       "       [-2.34591067e-02],\n",
       "       [-9.42206383e-03],\n",
       "       [-3.90230715e-02],\n",
       "       [ 2.98062265e-02],\n",
       "       [-4.90030944e-02],\n",
       "       [ 2.71957815e-02],\n",
       "       [-3.20594311e-02],\n",
       "       [-9.13286209e-03],\n",
       "       [-5.47873080e-02],\n",
       "       [ 3.11810076e-02],\n",
       "       [-6.28724694e-03],\n",
       "       [ 7.92053342e-03],\n",
       "       [-4.63071465e-03],\n",
       "       [-4.19997573e-02],\n",
       "       [-6.64263964e-04],\n",
       "       [-4.71451879e-03],\n",
       "       [-3.06676328e-02],\n",
       "       [-5.34473360e-02],\n",
       "       [ 1.00500286e-02],\n",
       "       [ 3.11179757e-02],\n",
       "       [ 1.71371698e-02],\n",
       "       [ 2.91084349e-02],\n",
       "       [-3.98366153e-02],\n",
       "       [ 2.75150836e-02],\n",
       "       [ 2.51164734e-02],\n",
       "       [-1.14751756e-02],\n",
       "       [ 1.67075694e-02],\n",
       "       [-1.25466287e-02],\n",
       "       [-2.12094784e-02],\n",
       "       [-4.80562449e-04],\n",
       "       [ 3.08486521e-02],\n",
       "       [ 4.69040871e-03],\n",
       "       [-1.91865265e-02],\n",
       "       [-7.25767016e-03],\n",
       "       [-4.23513949e-02],\n",
       "       [ 7.21251965e-03],\n",
       "       [-1.33461952e-02],\n",
       "       [-1.19025707e-02],\n",
       "       [ 2.07222998e-02],\n",
       "       [ 1.17457807e-02],\n",
       "       [ 9.97802615e-03],\n",
       "       [-2.46807933e-03],\n",
       "       [-6.23681247e-02],\n",
       "       [-5.01915812e-03],\n",
       "       [ 1.09909475e-02],\n",
       "       [ 2.87927687e-02],\n",
       "       [ 3.11686695e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [-3.67783010e-02],\n",
       "       [ 3.11880708e-02],\n",
       "       [ 2.84067988e-02],\n",
       "       [ 1.31186843e-03],\n",
       "       [ 2.77869999e-02],\n",
       "       [-2.93518603e-02],\n",
       "       [ 1.30649507e-02],\n",
       "       [-2.75788009e-02],\n",
       "       [-3.56243551e-02],\n",
       "       [-2.16721594e-02],\n",
       "       [-3.28585804e-02],\n",
       "       [-2.83717811e-02],\n",
       "       [ 2.64588892e-02],\n",
       "       [ 7.16987252e-03],\n",
       "       [ 2.71931291e-02],\n",
       "       [-1.54035091e-02],\n",
       "       [ 2.87921131e-02],\n",
       "       [-4.29549515e-02],\n",
       "       [-1.65134370e-02],\n",
       "       [-5.38702309e-02],\n",
       "       [-2.77721584e-02],\n",
       "       [ 2.74917185e-02],\n",
       "       [ 2.53103077e-02],\n",
       "       [ 7.17243552e-03],\n",
       "       [ 2.72101462e-02],\n",
       "       [ 2.95436382e-02],\n",
       "       [ 3.00713480e-02],\n",
       "       [ 1.54539943e-02],\n",
       "       [-2.37366259e-02],\n",
       "       [-1.75734460e-02],\n",
       "       [ 1.59122050e-02],\n",
       "       [-2.05717385e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 2.87883580e-02],\n",
       "       [ 2.91041434e-02],\n",
       "       [-2.45637000e-02],\n",
       "       [-3.30849588e-02],\n",
       "       [ 1.31292045e-02],\n",
       "       [ 1.80210769e-02],\n",
       "       [ 9.85106826e-03],\n",
       "       [ 3.03052366e-02],\n",
       "       [ 1.84873044e-02],\n",
       "       [ 3.10645998e-02],\n",
       "       [ 3.06210518e-02],\n",
       "       [-2.41652727e-02],\n",
       "       [-3.00752521e-02],\n",
       "       [ 8.20428133e-04],\n",
       "       [-2.53401101e-02],\n",
       "       [-2.20800340e-02],\n",
       "       [ 1.25584304e-02],\n",
       "       [-9.59101319e-03],\n",
       "       [-4.40251529e-02],\n",
       "       [ 2.61553228e-02],\n",
       "       [-3.78836989e-02],\n",
       "       [-6.69091940e-04],\n",
       "       [-1.03248954e-02],\n",
       "       [ 2.97201574e-02],\n",
       "       [ 2.43977010e-02],\n",
       "       [ 2.50070095e-02],\n",
       "       [ 3.79037857e-03],\n",
       "       [-1.00612640e-04],\n",
       "       [ 3.04548740e-02],\n",
       "       [-3.96057665e-02],\n",
       "       [ 4.75242734e-03],\n",
       "       [ 3.03023756e-02],\n",
       "       [-4.00697291e-02],\n",
       "       [-7.27126002e-03],\n",
       "       [ 3.12021375e-02],\n",
       "       [ 3.06210518e-02],\n",
       "       [ 3.03023160e-02],\n",
       "       [-4.24713492e-02],\n",
       "       [-2.12193429e-02],\n",
       "       [ 3.12002301e-02],\n",
       "       [ 3.53875756e-03],\n",
       "       [ 3.05975974e-02],\n",
       "       [ 3.03886235e-02],\n",
       "       [ 4.06539440e-03],\n",
       "       [-3.31918895e-02],\n",
       "       [-5.60755432e-02],\n",
       "       [ 3.11789215e-02],\n",
       "       [ 3.09782028e-02],\n",
       "       [ 2.61479318e-02],\n",
       "       [-9.35053825e-03],\n",
       "       [-3.28578651e-02],\n",
       "       [ 2.97231972e-02],\n",
       "       [-2.09808350e-04],\n",
       "       [ 3.08516324e-02],\n",
       "       [-3.34898829e-02],\n",
       "       [ 8.46025348e-03],\n",
       "       [-1.22450590e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [-2.68299282e-02],\n",
       "       [ 2.02843845e-02],\n",
       "       [-4.68283296e-02],\n",
       "       [-4.60893214e-02],\n",
       "       [ 8.61814618e-03],\n",
       "       [-3.78004909e-02],\n",
       "       [-3.60954106e-02],\n",
       "       [-1.47402585e-02],\n",
       "       [-3.20591033e-02],\n",
       "       [-6.89533353e-03],\n",
       "       [-2.07353532e-02],\n",
       "       [ 2.79015601e-02],\n",
       "       [-1.96240246e-02],\n",
       "       [ 3.08731496e-02],\n",
       "       [ 3.11867595e-02],\n",
       "       [ 2.79015601e-02],\n",
       "       [ 2.19853818e-02],\n",
       "       [ 2.08812058e-02],\n",
       "       [-6.36437535e-03],\n",
       "       [-4.49027717e-02],\n",
       "       [-5.34424484e-02],\n",
       "       [ 3.10295820e-02],\n",
       "       [ 3.21775675e-04],\n",
       "       [ 9.81098413e-03],\n",
       "       [-1.11426115e-02],\n",
       "       [ 1.12130046e-02],\n",
       "       [-2.02754140e-03],\n",
       "       [ 3.07422280e-02],\n",
       "       [ 2.02393234e-02],\n",
       "       [ 3.08410227e-02],\n",
       "       [ 1.00128353e-02],\n",
       "       [ 2.75504291e-02],\n",
       "       [ 1.88353062e-02],\n",
       "       [-4.81765866e-02],\n",
       "       [-3.53621244e-02],\n",
       "       [ 3.05721462e-02],\n",
       "       [ 3.06287408e-02],\n",
       "       [-4.66564596e-02],\n",
       "       [ 1.62117481e-02],\n",
       "       [-3.54447663e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 2.59347558e-02],\n",
       "       [ 3.07446420e-02],\n",
       "       [ 2.03017592e-02],\n",
       "       [-8.25664401e-03],\n",
       "       [ 3.06212902e-02],\n",
       "       [-4.20116484e-02],\n",
       "       [-9.44933295e-03],\n",
       "       [-1.52392089e-02],\n",
       "       [ 3.08183730e-02],\n",
       "       [-3.37125361e-02],\n",
       "       [ 3.02680731e-02],\n",
       "       [ 3.12002599e-02],\n",
       "       [ 1.89824998e-02],\n",
       "       [ 2.48483121e-02],\n",
       "       [ 2.94224918e-02],\n",
       "       [-3.37086022e-02],\n",
       "       [-4.04301286e-02],\n",
       "       [ 3.03519964e-02],\n",
       "       [-1.81213319e-02],\n",
       "       [ 2.43670940e-02],\n",
       "       [ 2.02302635e-02],\n",
       "       [ 3.03014219e-02],\n",
       "       [ 1.65909529e-03],\n",
       "       [ 2.65920758e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 1.41878426e-02],\n",
       "       [ 2.46876776e-02],\n",
       "       [ 3.07570398e-02],\n",
       "       [ 2.74999142e-02],\n",
       "       [ 2.97231972e-02],\n",
       "       [ 3.10510397e-02],\n",
       "       [-9.86704230e-03],\n",
       "       [-3.25524509e-02],\n",
       "       [-1.74969435e-03],\n",
       "       [ 2.53058374e-02],\n",
       "       [ 2.25537717e-02],\n",
       "       [ 2.35498846e-02],\n",
       "       [-9.24766064e-04],\n",
       "       [ 2.66134739e-02],\n",
       "       [ 1.41651630e-02],\n",
       "       [-3.09562981e-02],\n",
       "       [-3.56278121e-02],\n",
       "       [ 8.92800093e-03],\n",
       "       [ 3.00145447e-02],\n",
       "       [-4.14553285e-02],\n",
       "       [ 3.06213200e-02],\n",
       "       [-3.30944955e-02],\n",
       "       [-2.72970498e-02],\n",
       "       [ 2.65578330e-02],\n",
       "       [ 2.84686685e-03],\n",
       "       [ 8.42779875e-04],\n",
       "       [ 3.05930078e-02],\n",
       "       [-1.29020214e-03],\n",
       "       [ 1.89086795e-03],\n",
       "       [-6.28057122e-03],\n",
       "       [ 1.42277479e-02],\n",
       "       [-2.38407552e-02],\n",
       "       [ 2.80674994e-02],\n",
       "       [-2.34941542e-02],\n",
       "       [-4.08973098e-02],\n",
       "       [-2.37478912e-02],\n",
       "       [ 2.88909972e-02],\n",
       "       [-3.85995209e-02],\n",
       "       [-9.10574496e-02],\n",
       "       [-2.42484510e-02],\n",
       "       [-4.23553288e-02],\n",
       "       [-3.36438715e-02],\n",
       "       [ 3.09695899e-02],\n",
       "       [-6.28197193e-03],\n",
       "       [ 9.71028209e-03],\n",
       "       [-7.84543157e-03],\n",
       "       [-3.95441949e-02],\n",
       "       [ 2.80293822e-03],\n",
       "       [ 2.79010236e-02],\n",
       "       [-3.53393257e-02],\n",
       "       [ 2.55207717e-02],\n",
       "       [ 9.79417562e-03],\n",
       "       [-1.60751045e-02],\n",
       "       [-1.44273043e-03],\n",
       "       [ 3.11821401e-02],\n",
       "       [-1.13040209e-02],\n",
       "       [ 1.03759468e-02],\n",
       "       [ 9.62421298e-03],\n",
       "       [-4.57502902e-02],\n",
       "       [ 2.89814174e-02],\n",
       "       [-2.00630724e-02],\n",
       "       [ 2.51525640e-03],\n",
       "       [-4.41384017e-02],\n",
       "       [-1.19057596e-02],\n",
       "       [-2.65315175e-03],\n",
       "       [-3.12578380e-02],\n",
       "       [-7.70577788e-03],\n",
       "       [ 2.63375342e-02],\n",
       "       [ 3.02998424e-02],\n",
       "       [-5.34372032e-02],\n",
       "       [ 3.03028822e-02],\n",
       "       [ 2.79015601e-02],\n",
       "       [ 3.08139026e-02],\n",
       "       [-1.55994296e-03],\n",
       "       [-3.15638483e-02],\n",
       "       [-3.54412198e-03],\n",
       "       [ 2.78220177e-02],\n",
       "       [ 2.97231376e-02],\n",
       "       [-6.98074698e-03],\n",
       "       [ 3.11248899e-02],\n",
       "       [-1.75953805e-02],\n",
       "       [ 3.05957198e-02],\n",
       "       [-1.48452520e-02],\n",
       "       [-1.52002871e-02],\n",
       "       [ 8.80745053e-03],\n",
       "       [ 3.03497910e-02],\n",
       "       [ 3.45918536e-03],\n",
       "       [ 2.43956149e-02],\n",
       "       [ 3.03886831e-02],\n",
       "       [-3.47279310e-02],\n",
       "       [-3.01015973e-02],\n",
       "       [ 3.10681760e-02],\n",
       "       [-1.32171810e-02],\n",
       "       [-2.92322934e-02],\n",
       "       [-1.36453807e-02],\n",
       "       [ 3.05959284e-02],\n",
       "       [ 1.93736851e-02],\n",
       "       [ 2.79015601e-02],\n",
       "       [ 8.79290700e-03],\n",
       "       [-4.54232097e-03],\n",
       "       [-7.71805644e-03],\n",
       "       [ 2.52971351e-02],\n",
       "       [-1.81005299e-02],\n",
       "       [-5.50645888e-02],\n",
       "       [-2.38329172e-04],\n",
       "       [ 2.46513486e-02],\n",
       "       [-1.70177221e-02],\n",
       "       [ 2.33572423e-02],\n",
       "       [-3.67154181e-02],\n",
       "       [ 2.91086137e-02],\n",
       "       [ 2.95022726e-02],\n",
       "       [ 1.94264948e-02],\n",
       "       [-4.09735739e-02],\n",
       "       [ 2.01841295e-02],\n",
       "       [-3.90523076e-02],\n",
       "       [ 8.61820579e-03],\n",
       "       [ 2.79015005e-02],\n",
       "       [-3.62261832e-02],\n",
       "       [ 3.04590762e-02],\n",
       "       [-8.27822089e-03],\n",
       "       [-1.92945302e-02],\n",
       "       [-3.83822620e-02],\n",
       "       [-4.00671661e-02],\n",
       "       [ 3.40920687e-03],\n",
       "       [-2.24764049e-02],\n",
       "       [-3.18032801e-02],\n",
       "       [ 4.38988209e-03],\n",
       "       [ 1.14942789e-02],\n",
       "       [-2.00698078e-02],\n",
       "       [ 3.11635435e-02],\n",
       "       [-4.08039391e-02],\n",
       "       [ 3.05462778e-02],\n",
       "       [ 5.57881594e-03],\n",
       "       [ 1.50081217e-02],\n",
       "       [ 8.61820579e-03],\n",
       "       [ 1.88355446e-02],\n",
       "       [-1.19029582e-02],\n",
       "       [-3.54001820e-02],\n",
       "       [ 1.24121010e-02],\n",
       "       [ 2.43923366e-02],\n",
       "       [-1.53356493e-02],\n",
       "       [-4.26374078e-02],\n",
       "       [ 3.04332078e-02],\n",
       "       [-2.41738260e-02],\n",
       "       [ 2.16576755e-02],\n",
       "       [-1.95711553e-02],\n",
       "       [ 1.12325847e-02],\n",
       "       [ 2.43305266e-02],\n",
       "       [-3.40553522e-02],\n",
       "       [ 1.99012160e-02],\n",
       "       [-2.22036541e-02],\n",
       "       [ 3.03887725e-02],\n",
       "       [ 2.86402404e-02],\n",
       "       [ 2.99495757e-02],\n",
       "       [-1.37010217e-03],\n",
       "       [ 4.47544456e-03],\n",
       "       [ 2.07214057e-02],\n",
       "       [ 2.07499564e-02],\n",
       "       [ 3.06214392e-02],\n",
       "       [-4.04050946e-03],\n",
       "       [ 3.12205851e-02],\n",
       "       [-2.28736997e-02],\n",
       "       [ 3.03023756e-02],\n",
       "       [ 2.87927687e-02],\n",
       "       [-5.68702817e-03],\n",
       "       [-2.64867246e-02],\n",
       "       [ 2.86397338e-02],\n",
       "       [ 2.88885534e-02],\n",
       "       [-1.85737014e-03],\n",
       "       [ 3.07487249e-02],\n",
       "       [ 3.11635435e-02],\n",
       "       [-1.78843439e-02],\n",
       "       [-1.51654184e-02],\n",
       "       [-3.73803675e-02],\n",
       "       [ 1.94081664e-03],\n",
       "       [-2.73028910e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 1.92048252e-02],\n",
       "       [-4.43390012e-03],\n",
       "       [-1.25474036e-02],\n",
       "       [ 3.12002301e-02],\n",
       "       [-3.78700197e-02],\n",
       "       [ 2.19853818e-02],\n",
       "       [ 3.03389728e-02],\n",
       "       [ 2.77862847e-02],\n",
       "       [ 2.96568871e-02],\n",
       "       [ 2.49327123e-02],\n",
       "       [ 3.03498209e-02],\n",
       "       [-4.08586860e-03],\n",
       "       [-4.08014953e-02],\n",
       "       [ 2.53281891e-02],\n",
       "       [ 3.11009884e-02],\n",
       "       [ 6.69774413e-03],\n",
       "       [ 3.10728848e-02],\n",
       "       [-2.84892619e-02],\n",
       "       [-4.54983413e-02],\n",
       "       [-3.53516638e-02],\n",
       "       [ 1.89618468e-02],\n",
       "       [-1.22154653e-02],\n",
       "       [ 3.07900310e-02],\n",
       "       [ 2.75403559e-02],\n",
       "       [ 1.46256685e-02],\n",
       "       [-1.16610825e-02],\n",
       "       [-1.43602490e-03],\n",
       "       [ 1.80259049e-02],\n",
       "       [ 2.48739719e-02],\n",
       "       [ 3.03052366e-02],\n",
       "       [-3.04132998e-02],\n",
       "       [-1.27753317e-02],\n",
       "       [-1.85213089e-02],\n",
       "       [ 3.12002003e-02],\n",
       "       [-6.98438287e-03],\n",
       "       [ 3.03898752e-02],\n",
       "       [-1.81338787e-02],\n",
       "       [-3.84706557e-02],\n",
       "       [ 1.30391717e-02],\n",
       "       [-1.63443983e-02],\n",
       "       [ 1.01534724e-02],\n",
       "       [ 3.03888321e-02],\n",
       "       [-1.01861954e-02],\n",
       "       [-4.29108441e-02],\n",
       "       [-3.81323397e-02],\n",
       "       [ 2.77869403e-02],\n",
       "       [ 2.19461620e-02],\n",
       "       [ 1.89676583e-02],\n",
       "       [ 2.79015005e-02],\n",
       "       [ 3.15934420e-04],\n",
       "       [ 2.50537097e-02],\n",
       "       [ 2.56923139e-02],\n",
       "       [ 7.90789723e-03],\n",
       "       [ 2.84380913e-02],\n",
       "       [ 2.65919566e-02],\n",
       "       [-8.67819786e-03],\n",
       "       [-2.08913684e-02],\n",
       "       [ 1.29784644e-02],\n",
       "       [ 3.66243720e-03],\n",
       "       [-2.33105123e-02],\n",
       "       [-5.52060306e-02],\n",
       "       [ 2.51180232e-02],\n",
       "       [ 3.10402811e-02],\n",
       "       [ 1.78861320e-02],\n",
       "       [ 2.15615332e-02],\n",
       "       [-3.48018706e-02],\n",
       "       [-3.28495800e-02],\n",
       "       [ 3.10394764e-02],\n",
       "       [-5.02756238e-03],\n",
       "       [-2.27202475e-02],\n",
       "       [-5.07430136e-02],\n",
       "       [-4.61857915e-02],\n",
       "       [-1.70673430e-02],\n",
       "       [ 2.91279256e-02],\n",
       "       [-4.51916456e-02],\n",
       "       [ 3.03375125e-02],\n",
       "       [ 1.89614594e-02],\n",
       "       [-9.45559144e-03],\n",
       "       [-2.24305391e-02],\n",
       "       [ 3.05957198e-02],\n",
       "       [ 2.29918659e-02],\n",
       "       [-4.32735085e-02],\n",
       "       [-1.96243823e-02],\n",
       "       [-1.28547847e-02],\n",
       "       [ 8.61832500e-03],\n",
       "       [-1.59363449e-02],\n",
       "       [ 3.11831534e-02],\n",
       "       [-2.40098536e-02],\n",
       "       [ 3.12002301e-02],\n",
       "       [-3.42766941e-02],\n",
       "       [-4.67790067e-02],\n",
       "       [-3.41512859e-02],\n",
       "       [-4.89321351e-03],\n",
       "       [ 2.07214653e-02],\n",
       "       [ 2.59931386e-02],\n",
       "       [-1.90410018e-03],\n",
       "       [-6.28313422e-03],\n",
       "       [-1.70038640e-02],\n",
       "       [ 3.07419896e-02],\n",
       "       [ 3.12215388e-02],\n",
       "       [-4.25026119e-02],\n",
       "       [ 1.99119151e-02],\n",
       "       [-1.47402585e-02],\n",
       "       [-2.26798654e-03],\n",
       "       [-2.99363136e-02],\n",
       "       [-1.50516629e-02],\n",
       "       [ 3.05721462e-02],\n",
       "       [ 2.97192037e-02],\n",
       "       [ 1.46611631e-02],\n",
       "       [-1.12234652e-02],\n",
       "       [ 2.34114230e-02],\n",
       "       [-5.72878122e-02],\n",
       "       [ 2.75372565e-02],\n",
       "       [ 4.76744771e-03],\n",
       "       [ 3.84867191e-04],\n",
       "       [-4.39859331e-02],\n",
       "       [ 2.02159286e-02],\n",
       "       [-3.22004855e-02],\n",
       "       [-8.92743468e-03],\n",
       "       [-1.65177584e-02],\n",
       "       [-3.93821299e-02],\n",
       "       [ 3.78221273e-04],\n",
       "       [ 6.95428252e-03],\n",
       "       [-1.22338831e-02],\n",
       "       [ 2.91120112e-02],\n",
       "       [-3.73479426e-02],\n",
       "       [-4.71404493e-02],\n",
       "       [-4.59882915e-02],\n",
       "       [ 3.11633646e-02],\n",
       "       [ 2.43476033e-03],\n",
       "       [ 1.93332434e-02],\n",
       "       [-7.16826320e-03],\n",
       "       [-3.59908640e-02],\n",
       "       [-2.71439254e-02],\n",
       "       [-9.92059708e-04],\n",
       "       [ 2.87927389e-02],\n",
       "       [-3.37126553e-02],\n",
       "       [-1.47402585e-02],\n",
       "       [-2.63859928e-02],\n",
       "       [ 3.02665532e-02],\n",
       "       [ 2.88783312e-02],\n",
       "       [ 2.87922621e-02],\n",
       "       [ 2.80674994e-02],\n",
       "       [-1.47402585e-02],\n",
       "       [-9.45866108e-03],\n",
       "       [ 2.93331444e-02],\n",
       "       [ 2.87882984e-02],\n",
       "       [-1.90558434e-02],\n",
       "       [-5.07566035e-02],\n",
       "       [ 3.12002003e-02],\n",
       "       [-2.34993994e-02],\n",
       "       [ 2.13098824e-02],\n",
       "       [-2.24394500e-02],\n",
       "       [-1.14708245e-02],\n",
       "       [ 2.91032493e-02],\n",
       "       [ 1.55119002e-02],\n",
       "       [-1.44873857e-02],\n",
       "       [-1.09139383e-02],\n",
       "       [ 9.97358561e-03],\n",
       "       [ 2.94916332e-02],\n",
       "       [ 2.67768800e-02],\n",
       "       [ 3.02728117e-02],\n",
       "       [-1.06872022e-02],\n",
       "       [ 2.58151591e-02],\n",
       "       [ 2.43868530e-02],\n",
       "       [ 2.84393132e-02],\n",
       "       [ 2.72793770e-02],\n",
       "       [ 8.86538625e-03],\n",
       "       [-4.35128808e-03],\n",
       "       [ 3.06218266e-02],\n",
       "       [ 1.47159398e-02],\n",
       "       [-2.61148512e-02],\n",
       "       [ 3.03447545e-02],\n",
       "       [ 3.08392942e-02],\n",
       "       [-3.47307324e-03],\n",
       "       [ 1.54516101e-02],\n",
       "       [ 3.05956304e-02],\n",
       "       [ 3.11995447e-02],\n",
       "       [ 3.07353139e-02],\n",
       "       [-1.23287737e-02],\n",
       "       [ 3.11774313e-02],\n",
       "       [ 2.78752446e-02],\n",
       "       [ 2.32678056e-02],\n",
       "       [ 3.06212008e-02],\n",
       "       [ 2.80279219e-02],\n",
       "       [ 2.07225084e-02],\n",
       "       [ 8.84190202e-03],\n",
       "       [ 2.77845562e-02],\n",
       "       [-2.86514759e-02],\n",
       "       [-4.23381627e-02],\n",
       "       [ 1.26108229e-02],\n",
       "       [-4.10918295e-02],\n",
       "       [-3.26127410e-02],\n",
       "       [ 9.85100865e-03],\n",
       "       [ 1.99868679e-02],\n",
       "       [ 2.78726518e-02],\n",
       "       [ 7.72467256e-03],\n",
       "       [-5.16311824e-02],\n",
       "       [-2.18982399e-02],\n",
       "       [-2.24265456e-03],\n",
       "       [-1.06842220e-02],\n",
       "       [ 3.12214792e-02],\n",
       "       [-3.89018655e-03],\n",
       "       [ 3.12002003e-02],\n",
       "       [-4.67790067e-02],\n",
       "       [-3.71773243e-02],\n",
       "       [-6.06000423e-03],\n",
       "       [ 2.34330595e-02],\n",
       "       [-2.60312855e-02],\n",
       "       [ 9.66462493e-03],\n",
       "       [ 3.04434896e-02],\n",
       "       [-2.78684497e-03],\n",
       "       [ 2.29874849e-02],\n",
       "       [-8.21694732e-03],\n",
       "       [ 9.29003954e-03],\n",
       "       [ 1.15233958e-02],\n",
       "       [-2.44840682e-02],\n",
       "       [ 3.06272209e-02],\n",
       "       [-4.22126949e-02],\n",
       "       [-2.27201581e-02],\n",
       "       [ 2.60122120e-02],\n",
       "       [-1.08807981e-02],\n",
       "       [ 7.34484196e-03],\n",
       "       [ 3.06217372e-02],\n",
       "       [ 8.32858682e-03],\n",
       "       [ 3.05002928e-02],\n",
       "       [ 1.10010803e-02],\n",
       "       [-2.10660100e-02],\n",
       "       [ 3.10274065e-02]], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_1seq_sigmoid_loaded = create_model_a_1seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "model_a_1seq_sigmoid_loaded.load_weights(checkpoint_filepath)\n",
    "model_a_1seq_sigmoid_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_sigmoid_pred = model_a_1seq_sigmoid_loaded.predict(model_a_X_test_padded)\n",
    "model_a_1seq_sigmoid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 1s 949us/sample - loss: 0.0076 - mae: 0.0465\n",
      "Training MSE: 0.08219251292312667\n",
      "Test MSE: 0.08698438348431432\n",
      "Baseline MSE: 0.08418924631944838\n"
     ]
    }
   ],
   "source": [
    "dev_loss, dev_acc = model_a_1seq_sigmoid_loaded.evaluate(model_a_X_test_padded,  model_a_y_test, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_train, model_a_1seq_sigmoid_loaded.predict(model_a_X_train_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, model_a_1seq_sigmoid_loaded.predict(model_a_X_test_padded)) ) }\")\n",
    "print(f\"Baseline MSE: {np.sqrt( metrics.mean_squared_error(model_a_y_test, 0*model_a_y_test ) ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_a_1seq_sigmoid_history.history['loss'])\n",
    "plt.plot(model_a_1seq_sigmoid_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classification_df['15mins_price_diff_positive'] = classification_df['15mins_price_diff_perc'].apply(lambda x: 1 if x >= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>datetime_15mins_after</th>\n",
       "      <th>price_15mins_after</th>\n",
       "      <th>datetime_now</th>\n",
       "      <th>price_now</th>\n",
       "      <th>15mins_price_diff_abs</th>\n",
       "      <th>15mins_price_diff_perc</th>\n",
       "      <th>prev_30mins_prices</th>\n",
       "      <th>15mins_price_diff_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-11-28 11:05:00</td>\n",
       "      <td>261.150000</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>261.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>[260.96, 261.03, 261.01, 261.015, 261.04, 261....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>join me live from fort myer in arlington virginia</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>2017-08-22 09:15:00</td>\n",
       "      <td>243.630000</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>[243.76, 243.79, 243.85, 243.86, 243.81, 243.7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-05-09 07:16:00</td>\n",
       "      <td>239.940000</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>239.875000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>[239.73, 239.73, 239.73, 239.73, 239.73, 239.7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-08 04:59:00</td>\n",
       "      <td>236.915000</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>236.880000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>[236.84, 236.84, 236.84, 236.85333333333332, 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.778460e+17</td>\n",
       "      <td>great night in iowa special people thank you</td>\n",
       "      <td>56446</td>\n",
       "      <td>8039</td>\n",
       "      <td>2017-06-22 11:11:00</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>19:11:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-22 19:26:00</td>\n",
       "      <td>242.893333</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>242.880000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>[242.85, 242.85, 242.85, 242.85, 242.85, 242.8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-05-23 09:28:00</td>\n",
       "      <td>271.110000</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>271.040000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>[271.18, 271.16, 271.18, 271.15, 271.08, 271.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>so general michael flynns life can be totally ...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-20 18:49:00</td>\n",
       "      <td>266.890000</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>266.820000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>[266.74625000000003, 266.7475, 266.74875, 266....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-20 06:45:00</td>\n",
       "      <td>268.910000</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>268.620000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>[268.77, 268.74333333333334, 268.7166666666666...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>todays court decision means that congress must...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-18 05:49:00</td>\n",
       "      <td>270.710000</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>[270.73, 270.7, 270.72749999999996, 270.755, 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-29 05:46:00</td>\n",
       "      <td>261.065000</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>260.982857</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>[260.82, 260.89, 260.9, 260.87666666666667, 26...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0     9.353400e+17                                     thank you rand   \n",
       "1     8.997980e+17  join me live from fort myer in arlington virginia   \n",
       "2     8.939700e+17                                   thank you nicole   \n",
       "3     8.819770e+17     thank you to shawn steel for the nice words on   \n",
       "4     8.778460e+17       great night in iowa special people thank you   \n",
       "...            ...                                                ...   \n",
       "2607  9.990960e+17  if the person placed very early into my campai...   \n",
       "2608  9.874600e+17  so general michael flynns life can be totally ...   \n",
       "2609  9.870960e+17  my thoughts prayers and condolences are with t...   \n",
       "2610  9.863570e+17  todays court decision means that congress must...   \n",
       "2611  9.791090e+17  i am pleased to announce that i intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4         56446      8039  2017-06-22 11:11:00  2017-06-22 19:11:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2607      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2608      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2609      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2610      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2611      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month datetime_15mins_after  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   2017-11-28 11:05:00   \n",
       "1     2017-08-22  09:00:00     9  2017      8   2017-08-22 09:15:00   \n",
       "2     2017-05-09  07:01:00     7  2017      5   2017-05-09 07:16:00   \n",
       "3     2017-03-08  04:44:00     4  2017      3   2017-03-08 04:59:00   \n",
       "4     2017-06-22  19:11:00    19  2017      6   2017-06-22 19:26:00   \n",
       "...          ...       ...   ...   ...    ...                   ...   \n",
       "2607  2018-05-23  09:13:00     9  2018      5   2018-05-23 09:28:00   \n",
       "2608  2018-04-20  18:34:00    18  2018      4   2018-04-20 18:49:00   \n",
       "2609  2018-04-20  06:30:00     6  2018      4   2018-04-20 06:45:00   \n",
       "2610  2018-04-18  05:34:00     5  2018      4   2018-04-18 05:49:00   \n",
       "2611  2018-03-29  05:31:00     5  2018      3   2018-03-29 05:46:00   \n",
       "\n",
       "      price_15mins_after         datetime_now   price_now  \\\n",
       "0             261.150000  2017-11-28 10:50:00  261.100000   \n",
       "1             243.630000  2017-08-22 09:00:00  243.670000   \n",
       "2             239.940000  2017-05-09 07:01:00  239.875000   \n",
       "3             236.915000  2017-03-08 04:44:00  236.880000   \n",
       "4             242.893333  2017-06-22 19:11:00  242.880000   \n",
       "...                  ...                  ...         ...   \n",
       "2607          271.110000  2018-05-23 09:13:00  271.040000   \n",
       "2608          266.890000  2018-04-20 18:34:00  266.820000   \n",
       "2609          268.910000  2018-04-20 06:30:00  268.620000   \n",
       "2610          270.710000  2018-04-18 05:34:00  270.600000   \n",
       "2611          261.065000  2018-03-29 05:31:00  260.982857   \n",
       "\n",
       "      15mins_price_diff_abs  15mins_price_diff_perc  \\\n",
       "0                  0.050000                0.000191   \n",
       "1                 -0.040000               -0.000164   \n",
       "2                  0.065000                0.000271   \n",
       "3                  0.035000                0.000148   \n",
       "4                  0.013333                0.000055   \n",
       "...                     ...                     ...   \n",
       "2607               0.070000                0.000258   \n",
       "2608               0.070000                0.000262   \n",
       "2609               0.290000                0.001080   \n",
       "2610               0.110000                0.000407   \n",
       "2611               0.082143                0.000315   \n",
       "\n",
       "                                     prev_30mins_prices  \\\n",
       "0     [260.96, 261.03, 261.01, 261.015, 261.04, 261....   \n",
       "1     [243.76, 243.79, 243.85, 243.86, 243.81, 243.7...   \n",
       "2     [239.73, 239.73, 239.73, 239.73, 239.73, 239.7...   \n",
       "3     [236.84, 236.84, 236.84, 236.85333333333332, 2...   \n",
       "4     [242.85, 242.85, 242.85, 242.85, 242.85, 242.8...   \n",
       "...                                                 ...   \n",
       "2607  [271.18, 271.16, 271.18, 271.15, 271.08, 271.0...   \n",
       "2608  [266.74625000000003, 266.7475, 266.74875, 266....   \n",
       "2609  [268.77, 268.74333333333334, 268.7166666666666...   \n",
       "2610  [270.73, 270.7, 270.72749999999996, 270.755, 2...   \n",
       "2611  [260.82, 260.89, 260.9, 260.87666666666667, 26...   \n",
       "\n",
       "      15mins_price_diff_positive  \n",
       "0                              1  \n",
       "1                              0  \n",
       "2                              1  \n",
       "3                              1  \n",
       "4                              1  \n",
       "...                          ...  \n",
       "2607                           1  \n",
       "2608                           1  \n",
       "2609                           1  \n",
       "2610                           1  \n",
       "2611                           1  \n",
       "\n",
       "[2562 rows x 19 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "classification_X = classification_df.loc[:, 'text']\n",
    "classification_y = classification_df.loc[:, '15mins_price_diff_positive']\n",
    "# classification_y = to_categorical(classification_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_X_train, classification_X_test, classification_y_train, classification_y_test = train_test_split(classification_X, classification_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_X_train_padded = word2vec_sentence_to_indices_padded(classification_X_train, model_a_longest_sentence_len, model_a_word2vec_model)\n",
    "classification_X_test_padded = word2vec_sentence_to_indices_padded(classification_X_test, model_a_longest_sentence_len, model_a_word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_classification(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 159, 100)          522200    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 159, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 159, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 524,054\n",
      "Trainable params: 1,854\n",
      "Non-trainable params: 522,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_model_classification(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "classification_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1372 samples, validate on 344 samples\n",
      "Epoch 1/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 1.3375 - accuracy: 0.5394\n",
      "Epoch 00001: val_loss improved from inf to 0.69218, saving model to ./model_a_checkpoint/classification 11112020 1926h.h5\n",
      "1372/1372 [==============================] - 9s 7ms/sample - loss: 1.3389 - accuracy: 0.5372 - val_loss: 0.6922 - val_accuracy: 0.5843\n",
      "Epoch 2/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.8404 - accuracy: 0.5208\n",
      "Epoch 00002: val_loss improved from 0.69218 to 0.67904, saving model to ./model_a_checkpoint/classification 11112020 1926h.h5\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.8369 - accuracy: 0.5226 - val_loss: 0.6790 - val_accuracy: 0.5843\n",
      "Epoch 3/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7906 - accuracy: 0.5260\n",
      "Epoch 00003: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7972 - accuracy: 0.5241 - val_loss: 0.6835 - val_accuracy: 0.5843\n",
      "Epoch 4/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7590 - accuracy: 0.5253\n",
      "Epoch 00004: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7589 - accuracy: 0.5226 - val_loss: 0.6880 - val_accuracy: 0.5843\n",
      "Epoch 5/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.5506 ETA: 0s - loss: 0.702\n",
      "Epoch 00005: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6980 - accuracy: 0.5510 - val_loss: 0.6870 - val_accuracy: 0.5843\n",
      "Epoch 6/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7029 - accuracy: 0.5193\n",
      "Epoch 00006: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7054 - accuracy: 0.5168 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 7/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5283\n",
      "Epoch 00007: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7083 - accuracy: 0.5277 - val_loss: 0.6863 - val_accuracy: 0.5843\n",
      "Epoch 8/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7012 - accuracy: 0.5275\n",
      "Epoch 00008: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7017 - accuracy: 0.5255 - val_loss: 0.6862 - val_accuracy: 0.5843\n",
      "Epoch 9/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.5350\n",
      "Epoch 00009: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6938 - accuracy: 0.5364 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 10/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7196 - accuracy: 0.5409\n",
      "Epoch 00010: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7204 - accuracy: 0.5386 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 11/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7143 - accuracy: 0.5461\n",
      "Epoch 00011: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7137 - accuracy: 0.5459 - val_loss: 0.6862 - val_accuracy: 0.5843\n",
      "Epoch 12/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7135 - accuracy: 0.5409\n",
      "Epoch 00012: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7127 - accuracy: 0.5423 - val_loss: 0.6865 - val_accuracy: 0.5843\n",
      "Epoch 13/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5283 ETA: 0s - loss: 0.6933 - accuracy: 0.\n",
      "Epoch 00013: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6927 - accuracy: 0.5321 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 14/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7270 - accuracy: 0.5253\n",
      "Epoch 00014: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7267 - accuracy: 0.5233 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 15/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7335 - accuracy: 0.5327\n",
      "Epoch 00015: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7388 - accuracy: 0.5321 - val_loss: 0.6865 - val_accuracy: 0.5843\n",
      "Epoch 16/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7013 - accuracy: 0.5439\n",
      "Epoch 00016: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7013 - accuracy: 0.5445 - val_loss: 0.6881 - val_accuracy: 0.5843\n",
      "Epoch 17/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7044 - accuracy: 0.5335\n",
      "Epoch 00017: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7044 - accuracy: 0.5313 - val_loss: 0.6879 - val_accuracy: 0.5843\n",
      "Epoch 18/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5231\n",
      "Epoch 00018: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6977 - accuracy: 0.5233 - val_loss: 0.6878 - val_accuracy: 0.5843\n",
      "Epoch 19/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.5446\n",
      "Epoch 00019: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6906 - accuracy: 0.5452 - val_loss: 0.6870 - val_accuracy: 0.5843\n",
      "Epoch 20/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5446\n",
      "Epoch 00020: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7080 - accuracy: 0.5459 - val_loss: 0.6856 - val_accuracy: 0.5843\n",
      "Epoch 21/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6957 - accuracy: 0.5260 ETA: 0s - loss: 0.6975 - accuracy\n",
      "Epoch 00021: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6953 - accuracy: 0.5270 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 22/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7171 - accuracy: 0.5320\n",
      "Epoch 00022: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7164 - accuracy: 0.5357 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 23/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7134 - accuracy: 0.5320\n",
      "Epoch 00023: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7125 - accuracy: 0.5335 - val_loss: 0.6854 - val_accuracy: 0.5843\n",
      "Epoch 24/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7112 - accuracy: 0.5268\n",
      "Epoch 00024: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7106 - accuracy: 0.5277 - val_loss: 0.6866 - val_accuracy: 0.5843\n",
      "Epoch 25/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7126 - accuracy: 0.5305\n",
      "Epoch 00025: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7116 - accuracy: 0.5306 - val_loss: 0.6859 - val_accuracy: 0.5843\n",
      "Epoch 26/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7005 - accuracy: 0.5342 ETA\n",
      "Epoch 00026: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7006 - accuracy: 0.5335 - val_loss: 0.6856 - val_accuracy: 0.5843\n",
      "Epoch 27/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6969 - accuracy: 0.5327\n",
      "Epoch 00027: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6963 - accuracy: 0.5357 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 28/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7047 - accuracy: 0.5223\n",
      "Epoch 00028: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7041 - accuracy: 0.5241 - val_loss: 0.6852 - val_accuracy: 0.5843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5365\n",
      "Epoch 00029: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6940 - accuracy: 0.5357 - val_loss: 0.6855 - val_accuracy: 0.5843\n",
      "Epoch 30/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.5335\n",
      "Epoch 00030: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7056 - accuracy: 0.5328 - val_loss: 0.6862 - val_accuracy: 0.5843\n",
      "Epoch 31/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6979 - accuracy: 0.5365\n",
      "Epoch 00031: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6983 - accuracy: 0.5386 - val_loss: 0.6864 - val_accuracy: 0.5843\n",
      "Epoch 32/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7148 - accuracy: 0.52 - ETA: 0s - loss: 0.7140 - accuracy: 0.5231\n",
      "Epoch 00032: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7133 - accuracy: 0.5241 - val_loss: 0.6854 - val_accuracy: 0.5843\n",
      "Epoch 33/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5357\n",
      "Epoch 00033: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6913 - accuracy: 0.5343 - val_loss: 0.6847 - val_accuracy: 0.5843\n",
      "Epoch 34/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.5335\n",
      "Epoch 00034: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7032 - accuracy: 0.5343 - val_loss: 0.6851 - val_accuracy: 0.5843\n",
      "Epoch 35/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5320\n",
      "Epoch 00035: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6922 - accuracy: 0.5364 - val_loss: 0.6858 - val_accuracy: 0.5843\n",
      "Epoch 36/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7104 - accuracy: 0.5335\n",
      "Epoch 00036: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7104 - accuracy: 0.5321 - val_loss: 0.6845 - val_accuracy: 0.5843\n",
      "Epoch 37/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6972 - accuracy: 0.5201\n",
      "Epoch 00037: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6972 - accuracy: 0.5204 - val_loss: 0.6846 - val_accuracy: 0.5843\n",
      "Epoch 38/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5268\n",
      "Epoch 00038: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6930 - accuracy: 0.5277 - val_loss: 0.6845 - val_accuracy: 0.5843\n",
      "Epoch 39/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6958 - accuracy: 0.5238\n",
      "Epoch 00039: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6954 - accuracy: 0.5262 - val_loss: 0.6844 - val_accuracy: 0.5843\n",
      "Epoch 40/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7143 - accuracy: 0.5164\n",
      "Epoch 00040: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7141 - accuracy: 0.5168 - val_loss: 0.6849 - val_accuracy: 0.5843\n",
      "Epoch 41/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5327\n",
      "Epoch 00041: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6925 - accuracy: 0.5335 - val_loss: 0.6848 - val_accuracy: 0.5843\n",
      "Epoch 42/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.5236 ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.6968 - accuracy: 0.5268\n",
      "Epoch 00042: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6971 - accuracy: 0.5248 - val_loss: 0.6843 - val_accuracy: 0.5843\n",
      "Epoch 43/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7039 - accuracy: 0.5268\n",
      "Epoch 00043: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7036 - accuracy: 0.5270 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 44/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.5223\n",
      "Epoch 00044: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6955 - accuracy: 0.5204 - val_loss: 0.6865 - val_accuracy: 0.5843\n",
      "Epoch 45/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7017 - accuracy: 0.5231\n",
      "Epoch 00045: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7017 - accuracy: 0.5233 - val_loss: 0.6856 - val_accuracy: 0.5843\n",
      "Epoch 46/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.5231\n",
      "Epoch 00046: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.7039 - accuracy: 0.5233 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 47/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.5350\n",
      "Epoch 00047: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6943 - accuracy: 0.5372 - val_loss: 0.6862 - val_accuracy: 0.5843\n",
      "Epoch 48/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5350\n",
      "Epoch 00048: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6941 - accuracy: 0.5343 - val_loss: 0.6857 - val_accuracy: 0.5843\n",
      "Epoch 49/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6968 - accuracy: 0.5186\n",
      "Epoch 00049: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6970 - accuracy: 0.5190 - val_loss: 0.6862 - val_accuracy: 0.5843\n",
      "Epoch 50/50\n",
      "1344/1372 [============================>.] - ETA: 0s - loss: 0.6964 - accuracy: 0.5231\n",
      "Epoch 00050: val_loss did not improve from 0.67904\n",
      "1372/1372 [==============================] - 4s 3ms/sample - loss: 0.6963 - accuracy: 0.5211 - val_loss: 0.6854 - val_accuracy: 0.5843\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/classification {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "classification_history = classification_model.fit(classification_X_train_padded, classification_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58781165, 0.80068463],\n",
       "       [0.57438266, 0.78770894],\n",
       "       [0.58350235, 0.7963762 ],\n",
       "       ...,\n",
       "       [0.59073013, 0.80347854],\n",
       "       [0.578751  , 0.7915312 ],\n",
       "       [0.59304094, 0.8057902 ]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_loaded = create_model_classification(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "classification_loaded.load_weights(checkpoint_filepath)\n",
    "classification_loaded.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classification_pred = classification_loaded.predict(model_a_X_test_padded)\n",
    "classification_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.argmax(i) for i in classification_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.value_counts(classification_y).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (word vectors + price history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_df = df.dropna(subset=['cleaned_text', 'prev_60mins_prices'])\n",
    "\n",
    "model_b_X = model_b_df.loc[:, ['cleaned_text_2', 'prev_60mins_prices']]\n",
    "model_b_y = model_b_df.loc[:, '60mins_price_diff_perc']\n",
    "\n",
    "model_b_X_train, model_b_X_test, model_b_y_train, model_b_y_test = train_test_split(model_b_X, model_b_y, test_size=0.33, random_state=42)\n",
    "\n",
    "model_b_X_train_text = model_b_X_train.iloc[:, 0]\n",
    "model_b_X_train_price_history = model_b_X_train.iloc[:, 1].apply(lambda x: split(strip()))\n",
    "model_b_X_test_text = model_b_X_test.iloc[:, 0]\n",
    "model_b_X_test_price_history = model_b_X_test.iloc[:, 1]\n",
    "                           \n",
    "model_b_corpus_list = []\n",
    "\n",
    "for i in model_b_X_train_text:\n",
    "    model_b_corpus_list.append(i.split())\n",
    "    \n",
    "model_b_word2vec_model = Word2Vec(model_b_corpus_list, min_count=1, size=100)\n",
    "model_b_pretrained_weights = model_b_word2vec_model.wv.vectors\n",
    "\n",
    "model_b_num_words = [len(i) for i in model_b_corpus_list]\n",
    "model_b_longest_sentence_len = max(model_b_num_words)\n",
    "\n",
    "model_b_X_train_padded = sentence_to_indices_padded(model_b_X_train_text, model_b_longest_sentence_len)\n",
    "model_b_X_test_padded = sentence_to_indices_padded(model_b_X_test_text, model_b_longest_sentence_len)\n",
    "\n",
    "model_b_X_train_input = [model_b_X_train_padded, np.array(model_b_X_train_price_history)]\n",
    "model_b_X_test_input = [model_b_X_test_padded, model_b_X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.array(model_b_X_train_input[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='relu')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = create_model_b(model_b_pretrained_weights, model_b_longest_sentence_len, (30,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_b_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_b.fit(model_b_X_train_input, model_b_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_X_train_input[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
