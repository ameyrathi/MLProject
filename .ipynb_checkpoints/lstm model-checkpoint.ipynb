{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_5mins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, 'cleaned_text']\n",
    "y = df.loc[:, '5mins_price_diff_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "\n",
    "for i in X_train:\n",
    "    corpus_list.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=9195, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2Vec(corpus_list, min_count=1, size=100)\n",
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [len(i) for i in corpus_list]\n",
    "longest_sentence_len = max(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702    kim jong un of north korea proclaims “unwaveri...\n",
       "2164    prime minister trudeau is being so indignant, ...\n",
       "1281    “trump gets no credit for what he’s done in th...\n",
       "1866    ....china, which is for the first time doing p...\n",
       "306     two dozen nfl players continue to kneel during...\n",
       "                              ...                        \n",
       "1638    best economic numbers in decades. if the democ...\n",
       "1095    \"is it legal for a sitting president to be \"\"w...\n",
       "1130    the fake news media (failing @nytimes, @cnn, @...\n",
       "1294    as i predicted all along, obamacare has been s...\n",
       "860     karen handel for congress. she will fight for ...\n",
       "Name: cleaned_text, Length: 1788, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = sentence_to_indices_padded(X_train, longest_sentence_len)\n",
    "X_test_padded = sentence_to_indices_padded(X_test, longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9195"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(pretrained_weights, longest_sentence_len):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 158, 100)          919500    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 158, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 158, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 921,349\n",
      "Trainable params: 921,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_LSTM_model(pretrained_weights, longest_sentence_len)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1430 samples, validate on 358 samples\n",
      "Epoch 1/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0247 - mean_absolute_error: 0.0984\n",
      "Epoch 00001: val_loss improved from inf to 0.00001, saving model to ./model_a_checkpoint/08112020 15h58m.h5\n",
      "1430/1430 [==============================] - 24s 17ms/sample - loss: 0.0245 - mean_absolute_error: 0.0978 - val_loss: 1.3004e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 2/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0750\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "1430/1430 [==============================] - 21s 14ms/sample - loss: 0.0159 - mean_absolute_error: 0.0752 - val_loss: 1.3048e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 3/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0656\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "1430/1430 [==============================] - 21s 15ms/sample - loss: 0.0114 - mean_absolute_error: 0.0655 - val_loss: 1.7308e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 4/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0568\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "1430/1430 [==============================] - 22s 15ms/sample - loss: 0.0082 - mean_absolute_error: 0.0565 - val_loss: 1.8371e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 5/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0053 - mean_absolute_error: 0.0486\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "1430/1430 [==============================] - 24s 16ms/sample - loss: 0.0052 - mean_absolute_error: 0.0482 - val_loss: 2.5094e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 6/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0410\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00000, saving model to ./model_a_checkpoint/08112020 15h58m.h5\n",
      "1430/1430 [==============================] - 22s 15ms/sample - loss: 0.0040 - mean_absolute_error: 0.0411 - val_loss: 8.1892e-07 - val_mean_absolute_error: 7.5084e-04\n",
      "Epoch 7/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0031 - mean_absolute_error: 0.0355\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 26s 18ms/sample - loss: 0.0030 - mean_absolute_error: 0.0353 - val_loss: 1.0454e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 8/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0029 - mean_absolute_error: 0.0339\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 27s 19ms/sample - loss: 0.0029 - mean_absolute_error: 0.0337 - val_loss: 2.9798e-06 - val_mean_absolute_error: 0.0017\n",
      "Epoch 9/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0279\n",
      "Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to ./model_a_checkpoint/08112020 15h58m.h5\n",
      "1430/1430 [==============================] - 30s 21ms/sample - loss: 0.0023 - mean_absolute_error: 0.0280 - val_loss: 3.9207e-07 - val_mean_absolute_error: 3.5183e-04\n",
      "Epoch 10/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0251\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 28s 20ms/sample - loss: 0.0019 - mean_absolute_error: 0.0250 - val_loss: 8.3061e-07 - val_mean_absolute_error: 7.5865e-04\n",
      "Epoch 11/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0217\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 23s 16ms/sample - loss: 0.0014 - mean_absolute_error: 0.0217 - val_loss: 5.3972e-07 - val_mean_absolute_error: 5.2736e-04\n",
      "Epoch 12/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0195\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 26s 18ms/sample - loss: 0.0013 - mean_absolute_error: 0.0195 - val_loss: 2.4354e-06 - val_mean_absolute_error: 0.0015\n",
      "Epoch 13/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0169 \n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 25s 17ms/sample - loss: 0.0010 - mean_absolute_error: 0.0170 - val_loss: 2.3965e-06 - val_mean_absolute_error: 0.0015\n",
      "Epoch 14/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 9.7904e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 27s 19ms/sample - loss: 9.6615e-04 - mean_absolute_error: 0.0158 - val_loss: 1.4018e-06 - val_mean_absolute_error: 0.0011\n",
      "Epoch 15/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0160\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 27s 19ms/sample - loss: 0.0012 - mean_absolute_error: 0.0161 - val_loss: 2.1607e-06 - val_mean_absolute_error: 0.0014\n",
      "Epoch 16/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 8.9245e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 30s 21ms/sample - loss: 8.8523e-04 - mean_absolute_error: 0.0142 - val_loss: 4.3949e-07 - val_mean_absolute_error: 4.2184e-04\n",
      "Epoch 17/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 8.0043e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 38s 27ms/sample - loss: 8.1907e-04 - mean_absolute_error: 0.0140 - val_loss: 3.9535e-07 - val_mean_absolute_error: 3.5729e-04\n",
      "Epoch 18/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 7.9068e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 31s 21ms/sample - loss: 7.9646e-04 - mean_absolute_error: 0.0129 - val_loss: 1.9682e-06 - val_mean_absolute_error: 0.0013\n",
      "Epoch 19/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 6.4822e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 34s 24ms/sample - loss: 6.3879e-04 - mean_absolute_error: 0.0115 - val_loss: 1.2743e-06 - val_mean_absolute_error: 0.0010\n",
      "Epoch 20/50\n",
      "1408/1430 [============================>.] - ETA: 0s - loss: 7.8664e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "1430/1430 [==============================] - 38s 27ms/sample - loss: 8.0283e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0034e-06 - val_mean_absolute_error: 8.7446e-04\n",
      "Epoch 21/50\n",
      "1312/1430 [==========================>...] - ETA: 2s - loss: 5.9772e-04 - mean_absolute_error: 0.0109"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %Hh%Mm\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(X_train_padded, y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
