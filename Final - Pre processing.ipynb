{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README: This is the final notebook. Use this notebook to paste the final code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('raw_trump_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >= 20th jan 2017, <= 31st Dec 2018\n",
    "tweets['date']= pd.to_datetime(tweets['date'])\n",
    "tweets = tweets[tweets['date'] >= np.datetime64('2017-01-20')]\n",
    "tweets = tweets[tweets['date'] <= np.datetime64('2018-12-31')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_remove_duplicates(tweets, date):\n",
    "    duplicated_rows = tweets[tweets['date'] == date]\n",
    "    id = max(duplicated_rows['id'])\n",
    "    max_favorite = max(duplicated_rows['favorites'])\n",
    "    max_retweet = max(duplicated_rows['retweets'])\n",
    "    combined_tweet = \"\"\n",
    "    device = duplicated_rows['device'].iloc[0]\n",
    "    for index, row in duplicated_rows.iterrows():\n",
    "        combined_tweet = row['text'] + ' ' + combined_tweet.strip('.')\n",
    "    tweets = tweets[tweets.date != date]\n",
    "    row = {'id': id, 'text': combined_tweet, 'isRetweet': 'f', 'isDeleted': 'f', 'device': device, 'favorites': max_favorite, 'retweets': max_retweet, 'date': date}\n",
    "    tweets = tweets.append(row, ignore_index = True)   \n",
    "    print(f\"Combined {len(duplicated_rows)} tweets on {date}\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-11-21T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T21:59:00.000000000\n",
      "Combined 2 tweets on 2018-08-14T01:37:00.000000000\n",
      "Combined 2 tweets on 2018-05-31T10:56:00.000000000\n",
      "Combined 2 tweets on 2018-02-01T22:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T17:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:48:00.000000000\n",
      "Combined 3 tweets on 2017-12-28T22:17:00.000000000\n",
      "Combined 2 tweets on 2017-12-22T20:47:00.000000000\n",
      "Combined 2 tweets on 2017-12-16T03:09:00.000000000\n",
      "Combined 2 tweets on 2017-11-18T13:49:00.000000000\n",
      "Combined 2 tweets on 2017-11-15T10:22:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:53:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-28T21:09:00.000000000\n",
      "Combined 2 tweets on 2017-10-26T01:47:00.000000000\n",
      "Combined 2 tweets on 2017-10-14T11:08:00.000000000\n",
      "Combined 2 tweets on 2017-05-10T10:31:00.000000000\n",
      "Combined 2 tweets on 2017-09-30T01:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-22T10:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-20T10:00:00.000000000\n",
      "Combined 2 tweets on 2017-09-17T12:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:27:00.000000000\n",
      "Combined 2 tweets on 2017-07-09T12:51:00.000000000\n",
      "Combined 2 tweets on 2017-08-29T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-28T13:08:00.000000000\n",
      "Combined 2 tweets on 2017-08-24T13:15:00.000000000\n",
      "Combined 2 tweets on 2017-08-19T20:41:00.000000000\n",
      "Combined 2 tweets on 2017-08-18T15:27:00.000000000\n",
      "Combined 2 tweets on 2017-08-16T11:32:00.000000000\n",
      "Combined 2 tweets on 2017-08-15T11:03:00.000000000\n",
      "Combined 2 tweets on 2017-11-08T11:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-08T01:18:00.000000000\n",
      "Combined 2 tweets on 2017-09-08T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-08T10:59:00.000000000\n",
      "Combined 3 tweets on 2017-04-08T13:18:00.000000000\n",
      "Combined 2 tweets on 2017-10-07T11:54:00.000000000\n",
      "Combined 2 tweets on 2017-06-27T11:00:00.000000000\n",
      "Combined 2 tweets on 2017-06-26T18:16:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T08:29:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T20:27:00.000000000\n",
      "Combined 2 tweets on 2017-02-06T10:32:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T12:45:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T11:57:00.000000000\n",
      "Combined 2 tweets on 2017-04-27T14:39:00.000000000\n",
      "Combined 3 tweets on 2017-04-27T14:37:00.000000000\n",
      "Combined 2 tweets on 2017-04-22T15:18:00.000000000\n",
      "Combined 2 tweets on 2017-03-30T22:16:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:54:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:51:00.000000000\n",
      "Combined 2 tweets on 2018-12-27T21:04:00.000000000\n",
      "Combined 2 tweets on 2018-12-20T22:21:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T02:07:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-12-16T20:29:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T18:17:00.000000000\n",
      "Combined 2 tweets on 2018-12-13T17:34:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T18:58:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T14:19:00.000000000\n",
      "Combined 2 tweets on 2018-07-12T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-04-12T22:56:00.000000000\n",
      "Combined 2 tweets on 2018-01-12T15:21:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T22:14:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T16:34:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:41:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T19:05:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T04:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T20:20:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T19:47:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T16:42:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T00:43:00.000000000\n",
      "Combined 2 tweets on 2018-07-11T19:44:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:26:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:25:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:23:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:20:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:19:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:41:00.000000000\n",
      "Combined 3 tweets on 2018-04-11T15:40:00.000000000\n",
      "Combined 4 tweets on 2018-04-11T15:39:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:32:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:28:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:27:00.000000000\n",
      "Combined 2 tweets on 2018-03-11T21:12:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T18:37:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T03:34:00.000000000\n",
      "Combined 2 tweets on 2018-10-30T17:37:00.000000000\n",
      "Combined 2 tweets on 2018-10-27T21:41:00.000000000\n",
      "Combined 2 tweets on 2018-10-25T18:47:00.000000000\n",
      "Combined 2 tweets on 2018-10-22T19:18:00.000000000\n",
      "Combined 2 tweets on 2018-10-18T15:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-17T17:11:00.000000000\n",
      "Combined 2 tweets on 2018-10-16T18:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:50:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T22:33:00.000000000\n",
      "Combined 4 tweets on 2018-10-10T12:48:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:47:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T16:00:00.000000000\n",
      "Combined 2 tweets on 2018-05-10T21:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-26T19:23:00.000000000\n",
      "Combined 2 tweets on 2018-09-22T03:09:00.000000000\n",
      "Combined 2 tweets on 2018-09-21T20:24:00.000000000\n",
      "Combined 2 tweets on 2018-09-20T18:10:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:33:00.000000000\n",
      "Combined 2 tweets on 2018-09-18T15:50:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:47:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:46:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:45:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:44:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:43:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T13:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:35:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:32:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:31:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:28:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T00:11:00.000000000\n",
      "Combined 2 tweets on 2018-09-13T12:16:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:08:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-08-09T01:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-09T20:41:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T11:05:00.000000000\n",
      "Combined 2 tweets on 2018-02-09T01:23:00.000000000\n",
      "Combined 4 tweets on 2018-08-29T21:23:00.000000000\n",
      "Combined 2 tweets on 2018-08-28T15:02:00.000000000\n",
      "Combined 2 tweets on 2018-08-27T22:07:00.000000000\n",
      "Combined 2 tweets on 2018-08-26T13:21:00.000000000\n",
      "Combined 3 tweets on 2018-08-24T17:36:00.000000000\n",
      "Combined 2 tweets on 2018-08-23T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-08-17T19:25:00.000000000\n",
      "Combined 2 tweets on 2018-08-13T17:14:00.000000000\n",
      "Combined 2 tweets on 2018-05-08T01:43:00.000000000\n",
      "Combined 4 tweets on 2018-04-08T03:05:00.000000000\n",
      "Combined 2 tweets on 2018-07-31T17:33:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:29:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:28:00.000000000\n",
      "Combined 4 tweets on 2018-07-29T19:09:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-29T11:59:00.000000000\n",
      "Combined 2 tweets on 2018-07-27T15:53:00.000000000\n",
      "Combined 2 tweets on 2018-07-26T22:48:00.000000000\n",
      "Combined 2 tweets on 2018-07-25T23:42:00.000000000\n",
      "Combined 3 tweets on 2018-07-15T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-07-24T11:01:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:39:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:35:00.000000000\n",
      "Combined 2 tweets on 2018-07-15T13:40:00.000000000\n",
      "Combined 2 tweets on 2018-07-16T04:28:00.000000000\n",
      "Combined 6 tweets on 2018-07-15T13:33:00.000000000\n",
      "Combined 2 tweets on 2018-12-07T15:00:00.000000000\n",
      "Combined 3 tweets on 2018-11-07T21:11:00.000000000\n",
      "Combined 4 tweets on 2018-11-07T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-11-07T12:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-07T09:00:00.000000000\n",
      "Combined 4 tweets on 2018-07-07T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-05-07T19:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:52:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-03-07T23:13:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:59:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:49:00.000000000\n",
      "Combined 2 tweets on 2018-06-28T03:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-25T16:58:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T14:00:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:21:00.000000000\n",
      "Combined 3 tweets on 2018-06-23T11:19:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:16:00.000000000\n",
      "Combined 2 tweets on 2018-06-21T20:46:00.000000000\n",
      "Combined 4 tweets on 2018-06-19T13:52:00.000000000\n",
      "Combined 2 tweets on 2018-06-18T13:50:00.000000000\n",
      "Combined 3 tweets on 2018-06-17T14:36:00.000000000\n",
      "Combined 2 tweets on 2018-06-15T11:56:00.000000000\n",
      "Combined 4 tweets on 2018-06-15T11:55:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T08:53:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:09:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:08:00.000000000\n",
      "Combined 2 tweets on 2018-06-13T09:40:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T20:40:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:58:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:56:00.000000000\n",
      "Combined 2 tweets on 2018-06-06T00:40:00.000000000\n",
      "Combined 2 tweets on 2018-05-30T12:47:00.000000000\n",
      "Combined 4 tweets on 2018-05-25T12:04:00.000000000\n",
      "Combined 2 tweets on 2018-05-23T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-05-16T18:40:00.000000000\n",
      "Combined 3 tweets on 2018-05-16T13:09:00.000000000\n",
      "Combined 2 tweets on 2018-08-05T20:33:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T13:10:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T12:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-20T10:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-19T22:30:00.000000000\n",
      "Combined 2 tweets on 2018-04-18T12:05:00.000000000\n",
      "Combined 3 tweets on 2018-04-18T09:42:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:55:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:26:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:25:00.000000000\n",
      "Combined 4 tweets on 2018-04-17T12:24:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:57:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:56:00.000000000\n",
      "Combined 2 tweets on 2018-03-28T21:31:00.000000000\n",
      "Combined 2 tweets on 2018-03-15T17:47:00.000000000\n",
      "Combined 2 tweets on 2018-03-02T14:40:00.000000000\n",
      "Combined 2 tweets on 2018-01-28T13:18:00.000000000\n",
      "Combined 2 tweets on 2018-01-21T02:31:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:53:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:52:00.000000000\n",
      "Combined 2 tweets on 2018-01-13T13:14:00.000000000\n"
     ]
    }
   ],
   "source": [
    "dates = tweets[\"date\"]\n",
    "duplicated = tweets[dates.isin(dates[dates.duplicated()])]\n",
    "duplicated_dates = duplicated['date'].unique()\n",
    "\n",
    "for date in duplicated_dates:\n",
    "    tweets = process_and_remove_duplicates(tweets, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tweets that are Trump's\n",
    "tweets1 = tweets[tweets['isRetweet'] == 'f'].reset_index(drop=True)\n",
    "tweets1 = tweets1[tweets1['text'].str.contains('RT @') == False]\n",
    "tweets1['cleaned_text'] = tweets1['text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S*', '', x, flags=re.MULTILINE))\n",
    "tweets1 = tweets1[tweets1['cleaned_text'].str.strip() != '']\n",
    "\n",
    "tweets2 = tweets1.loc[:, ['id', 'cleaned_text', 'favorites', 'retweets', 'date']]\n",
    "tweets2['date_new'] = [i + timedelta(hours = 8) for i in tweets2['date']]\n",
    "tweets2['date_part'] = [i.date() for i in tweets2['date_new']]\n",
    "tweets2['time_part'] = [i.time() for i in tweets2['date_new']]\n",
    "tweets2['hour'] = [int(str(i).split(\":\")[0]) for i in tweets2['time_part']]\n",
    "tweets2['year'] = [int(str(i).split(\"-\")[0]) for i in tweets2['date_part']]\n",
    "tweets2['month'] = [int(str(i).split(\"-\")[1]) for i in tweets2['date_part']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_mentions(string):\n",
    "    return re.sub(\"@[A-Za-z0-9]+\",\"\",string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2[\"cleaned_text\"] = tweets2[\"cleaned_text\"].str.lower()\n",
    "# for i in range(0,5088):\n",
    "#     tweets2.loc[i,'cleaned_text2'] = remove_emoji(df.loc[i,'cleaned_text'])\n",
    "# can be made into :\n",
    "tweets2[\"cleaned_text2\"] = tweets2.cleaned_text.apply(remove_emoji)\n",
    "# need to test\n",
    "\n",
    "# for i in range(0,5088):\n",
    "#     df.loc[i,'cleaned_text2'] = re.sub(\"@[A-Za-z0-9]+\",\"\",df.loc[i,'cleaned_text2'])\n",
    "\n",
    "tweets2.cleaned_text2 = tweets2.cleaned_text.apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>date_new</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cleaned_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.384230e+17</td>\n",
       "      <td>make america great again!</td>\n",
       "      <td>157963</td>\n",
       "      <td>37189</td>\n",
       "      <td>2017-06-12 15:00:00</td>\n",
       "      <td>2017-06-12 23:00:00</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>make america great again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.391890e+17</td>\n",
       "      <td>make america great again!</td>\n",
       "      <td>56596</td>\n",
       "      <td>11433</td>\n",
       "      <td>2017-08-12 17:46:00</td>\n",
       "      <td>2017-08-13 01:46:00</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>01:46:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>make america great again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>thank you rand!</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>thank you rand!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.253890e+17</td>\n",
       "      <td>thank you @luisriveramarin!</td>\n",
       "      <td>23521</td>\n",
       "      <td>4574</td>\n",
       "      <td>2017-10-31 15:48:00</td>\n",
       "      <td>2017-10-31 23:48:00</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>23:48:00</td>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>thank you @luisriveramarin!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>join me live from fort myer in arlington, virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>join me live from fort myer in arlington, virg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>9.862190e+17</td>\n",
       "      <td>employment is up, taxes are down. enjoy! i am ...</td>\n",
       "      <td>106514</td>\n",
       "      <td>19661</td>\n",
       "      <td>2018-04-17 12:24:00</td>\n",
       "      <td>2018-04-17 20:24:00</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>20:24:00</td>\n",
       "      <td>20</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>employment is up, taxes are down. enjoy! i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>9.597990e+17</td>\n",
       "      <td>rasmussen just announced that my approval rati...</td>\n",
       "      <td>132165</td>\n",
       "      <td>30235</td>\n",
       "      <td>2018-03-02 14:40:00</td>\n",
       "      <td>2018-03-02 22:40:00</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>22:40:00</td>\n",
       "      <td>22</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>rasmussen just announced that my approval rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5255</th>\n",
       "      <td>9.576040e+17</td>\n",
       "      <td>somebody please inform jay-z that because of m...</td>\n",
       "      <td>202727</td>\n",
       "      <td>49876</td>\n",
       "      <td>2018-01-28 13:18:00</td>\n",
       "      <td>2018-01-28 21:18:00</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>21:18:00</td>\n",
       "      <td>21</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>somebody please inform jay-z that because of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5259</th>\n",
       "      <td>9.521670e+17</td>\n",
       "      <td>the democrats are all talk and no action. they...</td>\n",
       "      <td>127182</td>\n",
       "      <td>28594</td>\n",
       "      <td>2018-01-13 13:14:00</td>\n",
       "      <td>2018-01-13 21:14:00</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>21:14:00</td>\n",
       "      <td>21</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>the democrats are all talk and no action. they...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5088 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.384230e+17                          make america great again!   \n",
       "1     9.391890e+17                         make america great again!    \n",
       "2     9.353400e+17                                   thank you rand!    \n",
       "3     9.253890e+17                       thank you @luisriveramarin!    \n",
       "4     8.997980e+17  join me live from fort myer in arlington, virg...   \n",
       "...            ...                                                ...   \n",
       "5249  9.862190e+17  employment is up, taxes are down. enjoy! i am ...   \n",
       "5252  9.791090e+17  i am pleased to announce that i intend to nomi...   \n",
       "5254  9.597990e+17  rasmussen just announced that my approval rati...   \n",
       "5255  9.576040e+17  somebody please inform jay-z that because of m...   \n",
       "5259  9.521670e+17  the democrats are all talk and no action. they...   \n",
       "\n",
       "      favorites  retweets                date            date_new   date_part  \\\n",
       "0        157963     37189 2017-06-12 15:00:00 2017-06-12 23:00:00  2017-06-12   \n",
       "1         56596     11433 2017-08-12 17:46:00 2017-08-13 01:46:00  2017-08-13   \n",
       "2         42793      9125 2017-11-28 02:50:00 2017-11-28 10:50:00  2017-11-28   \n",
       "3         23521      4574 2017-10-31 15:48:00 2017-10-31 23:48:00  2017-10-31   \n",
       "4         36009      4891 2017-08-22 01:00:00 2017-08-22 09:00:00  2017-08-22   \n",
       "...         ...       ...                 ...                 ...         ...   \n",
       "5249     106514     19661 2018-04-17 12:24:00 2018-04-17 20:24:00  2018-04-17   \n",
       "5252      66173     13399 2018-03-28 21:31:00 2018-03-29 05:31:00  2018-03-29   \n",
       "5254     132165     30235 2018-03-02 14:40:00 2018-03-02 22:40:00  2018-03-02   \n",
       "5255     202727     49876 2018-01-28 13:18:00 2018-01-28 21:18:00  2018-01-28   \n",
       "5259     127182     28594 2018-01-13 13:14:00 2018-01-13 21:14:00  2018-01-13   \n",
       "\n",
       "     time_part  hour  year  month  \\\n",
       "0     23:00:00    23  2017      6   \n",
       "1     01:46:00     1  2017      8   \n",
       "2     10:50:00    10  2017     11   \n",
       "3     23:48:00    23  2017     10   \n",
       "4     09:00:00     9  2017      8   \n",
       "...        ...   ...   ...    ...   \n",
       "5249  20:24:00    20  2018      4   \n",
       "5252  05:31:00     5  2018      3   \n",
       "5254  22:40:00    22  2018      3   \n",
       "5255  21:18:00    21  2018      1   \n",
       "5259  21:14:00    21  2018      1   \n",
       "\n",
       "                                          cleaned_text2  \n",
       "0                             make america great again!  \n",
       "1                            make america great again!   \n",
       "2                                      thank you rand!   \n",
       "3                          thank you @luisriveramarin!   \n",
       "4     join me live from fort myer in arlington, virg...  \n",
       "...                                                 ...  \n",
       "5249  employment is up, taxes are down. enjoy! i am ...  \n",
       "5252  i am pleased to announce that i intend to nomi...  \n",
       "5254  rasmussen just announced that my approval rati...  \n",
       "5255  somebody please inform jay-z that because of m...  \n",
       "5259  the democrats are all talk and no action. they...  \n",
       "\n",
       "[5088 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are combined after pre processing! (else kernel crashes)\n",
    "stock_price_2017 = pd.read_csv('yr2017.csv')\n",
    "stock_price_2018 = pd.read_csv('yr2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "stock_price_2017.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)\n",
    "stock_price_2018.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datetime objects\n",
    "stock_price_2017['TIME_M'] = pd.to_datetime(stock_price_2017['TIME_M'], format='%H:%M:%S.%f')\n",
    "stock_price_2018['TIME_M'] = pd.to_datetime(stock_price_2018['TIME_M'], format='%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, milliseconds from data\n",
    "stock_price_2017['TIME_M'] = stock_price_2017['TIME_M'].dt.floor('Min').dt.time\n",
    "stock_price_2018['TIME_M'] = stock_price_2018['TIME_M'].dt.floor('Min').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minute to minute data (one entry for each minute)\n",
    "stock_price_2017.drop_duplicates(subset=['TIME_M','DATE'],inplace=True)\n",
    "stock_price_2018.drop_duplicates(subset=['TIME_M', 'DATE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv, will be used for preprocessing later!\n",
    "stock_price = pd.concat([stock_price_2017,stock_price_2018], ignore_index=True, sort=False)\n",
    "stock_price.to_csv(\"stock_price.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolation of missing values\n",
    "idx = pd.date_range(stock_price.iloc[0, 0], stock_price.iloc[-1, 0], freq='1min')\n",
    "stock_price.index = pd.DatetimeIndex(stock_price.loc[:, 'DATE_TIME_M'])\n",
    "stock_price = stock_price.reindex(idx, fill_value='NaN')\n",
    "stock_price['DATE_TIME_M'] = stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_price.astype({'PRICE': 'float'})\n",
    "stock_price['PRICE'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_csv(\"stock_price.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
