{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README: This is the final notebook. Use this notebook to paste the final code.\n",
    "Do not test code here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('raw_trump_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >= 20th jan 2017, <= 31st Dec 2018\n",
    "tweets['date']= pd.to_datetime(tweets['date'])\n",
    "tweets = tweets[tweets['date'] >= np.datetime64('2017-01-20')]\n",
    "tweets = tweets[tweets['date'] <= np.datetime64('2018-12-31')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_remove_duplicates(tweets, date):\n",
    "    duplicated_rows = tweets[tweets['date'] == date]\n",
    "    id = max(duplicated_rows['id'])\n",
    "    max_favorite = max(duplicated_rows['favorites'])\n",
    "    max_retweet = max(duplicated_rows['retweets'])\n",
    "    combined_tweet = \"\"\n",
    "    device = duplicated_rows['device'].iloc[0]\n",
    "    for index, row in duplicated_rows.iterrows():\n",
    "        combined_tweet = row['text'] + ' ' + combined_tweet.strip('.')\n",
    "    tweets = tweets[tweets.date != date]\n",
    "    row = {'id': id, 'text': combined_tweet, 'isRetweet': 'f', 'isDeleted': 'f', 'device': device, 'favorites': max_favorite, 'retweets': max_retweet, 'date': date}\n",
    "    tweets = tweets.append(row, ignore_index = True)   \n",
    "    print(f\"Combined {len(duplicated_rows)} tweets on {date}\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-11-21T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T21:59:00.000000000\n",
      "Combined 2 tweets on 2018-08-14T01:37:00.000000000\n",
      "Combined 2 tweets on 2018-05-31T10:56:00.000000000\n",
      "Combined 2 tweets on 2018-02-01T22:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T17:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:48:00.000000000\n",
      "Combined 3 tweets on 2017-12-28T22:17:00.000000000\n",
      "Combined 2 tweets on 2017-12-22T20:47:00.000000000\n",
      "Combined 2 tweets on 2017-12-16T03:09:00.000000000\n",
      "Combined 2 tweets on 2017-11-18T13:49:00.000000000\n",
      "Combined 2 tweets on 2017-11-15T10:22:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:53:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-28T21:09:00.000000000\n",
      "Combined 2 tweets on 2017-10-26T01:47:00.000000000\n",
      "Combined 2 tweets on 2017-10-14T11:08:00.000000000\n",
      "Combined 2 tweets on 2017-05-10T10:31:00.000000000\n",
      "Combined 2 tweets on 2017-09-30T01:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-22T10:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-20T10:00:00.000000000\n",
      "Combined 2 tweets on 2017-09-17T12:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:27:00.000000000\n",
      "Combined 2 tweets on 2017-07-09T12:51:00.000000000\n",
      "Combined 2 tweets on 2017-08-29T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-28T13:08:00.000000000\n",
      "Combined 2 tweets on 2017-08-24T13:15:00.000000000\n",
      "Combined 2 tweets on 2017-08-19T20:41:00.000000000\n",
      "Combined 2 tweets on 2017-08-18T15:27:00.000000000\n",
      "Combined 2 tweets on 2017-08-16T11:32:00.000000000\n",
      "Combined 2 tweets on 2017-08-15T11:03:00.000000000\n",
      "Combined 2 tweets on 2017-11-08T11:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-08T01:18:00.000000000\n",
      "Combined 2 tweets on 2017-09-08T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-08T10:59:00.000000000\n",
      "Combined 3 tweets on 2017-04-08T13:18:00.000000000\n",
      "Combined 2 tweets on 2017-10-07T11:54:00.000000000\n",
      "Combined 2 tweets on 2017-06-27T11:00:00.000000000\n",
      "Combined 2 tweets on 2017-06-26T18:16:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T08:29:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T20:27:00.000000000\n",
      "Combined 2 tweets on 2017-02-06T10:32:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T12:45:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T11:57:00.000000000\n",
      "Combined 2 tweets on 2017-04-27T14:39:00.000000000\n",
      "Combined 3 tweets on 2017-04-27T14:37:00.000000000\n",
      "Combined 2 tweets on 2017-04-22T15:18:00.000000000\n",
      "Combined 2 tweets on 2017-03-30T22:16:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:54:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:51:00.000000000\n",
      "Combined 2 tweets on 2018-12-27T21:04:00.000000000\n",
      "Combined 2 tweets on 2018-12-20T22:21:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T02:07:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-12-16T20:29:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T18:17:00.000000000\n",
      "Combined 2 tweets on 2018-12-13T17:34:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T18:58:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T14:19:00.000000000\n",
      "Combined 2 tweets on 2018-07-12T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-04-12T22:56:00.000000000\n",
      "Combined 2 tweets on 2018-01-12T15:21:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T22:14:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T16:34:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:41:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T19:05:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T04:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T20:20:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T19:47:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T16:42:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T00:43:00.000000000\n",
      "Combined 2 tweets on 2018-07-11T19:44:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:26:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:25:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:23:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:20:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:19:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:41:00.000000000\n",
      "Combined 3 tweets on 2018-04-11T15:40:00.000000000\n",
      "Combined 4 tweets on 2018-04-11T15:39:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:32:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:28:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:27:00.000000000\n",
      "Combined 2 tweets on 2018-03-11T21:12:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T18:37:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T03:34:00.000000000\n",
      "Combined 2 tweets on 2018-10-30T17:37:00.000000000\n",
      "Combined 2 tweets on 2018-10-27T21:41:00.000000000\n",
      "Combined 2 tweets on 2018-10-25T18:47:00.000000000\n",
      "Combined 2 tweets on 2018-10-22T19:18:00.000000000\n",
      "Combined 2 tweets on 2018-10-18T15:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-17T17:11:00.000000000\n",
      "Combined 2 tweets on 2018-10-16T18:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:50:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T22:33:00.000000000\n",
      "Combined 4 tweets on 2018-10-10T12:48:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:47:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T16:00:00.000000000\n",
      "Combined 2 tweets on 2018-05-10T21:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-26T19:23:00.000000000\n",
      "Combined 2 tweets on 2018-09-22T03:09:00.000000000\n",
      "Combined 2 tweets on 2018-09-21T20:24:00.000000000\n",
      "Combined 2 tweets on 2018-09-20T18:10:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:33:00.000000000\n",
      "Combined 2 tweets on 2018-09-18T15:50:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:47:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:46:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:45:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:44:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:43:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T13:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:35:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:32:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:31:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:28:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T00:11:00.000000000\n",
      "Combined 2 tweets on 2018-09-13T12:16:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:08:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-08-09T01:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-09T20:41:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T11:05:00.000000000\n",
      "Combined 2 tweets on 2018-02-09T01:23:00.000000000\n",
      "Combined 4 tweets on 2018-08-29T21:23:00.000000000\n",
      "Combined 2 tweets on 2018-08-28T15:02:00.000000000\n",
      "Combined 2 tweets on 2018-08-27T22:07:00.000000000\n",
      "Combined 2 tweets on 2018-08-26T13:21:00.000000000\n",
      "Combined 3 tweets on 2018-08-24T17:36:00.000000000\n",
      "Combined 2 tweets on 2018-08-23T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-08-17T19:25:00.000000000\n",
      "Combined 2 tweets on 2018-08-13T17:14:00.000000000\n",
      "Combined 2 tweets on 2018-05-08T01:43:00.000000000\n",
      "Combined 4 tweets on 2018-04-08T03:05:00.000000000\n",
      "Combined 2 tweets on 2018-07-31T17:33:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:29:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:28:00.000000000\n",
      "Combined 4 tweets on 2018-07-29T19:09:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-29T11:59:00.000000000\n",
      "Combined 2 tweets on 2018-07-27T15:53:00.000000000\n",
      "Combined 2 tweets on 2018-07-26T22:48:00.000000000\n",
      "Combined 2 tweets on 2018-07-25T23:42:00.000000000\n",
      "Combined 3 tweets on 2018-07-15T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-07-24T11:01:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:39:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:35:00.000000000\n",
      "Combined 2 tweets on 2018-07-15T13:40:00.000000000\n",
      "Combined 2 tweets on 2018-07-16T04:28:00.000000000\n",
      "Combined 6 tweets on 2018-07-15T13:33:00.000000000\n",
      "Combined 2 tweets on 2018-12-07T15:00:00.000000000\n",
      "Combined 3 tweets on 2018-11-07T21:11:00.000000000\n",
      "Combined 4 tweets on 2018-11-07T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-11-07T12:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-07T09:00:00.000000000\n",
      "Combined 4 tweets on 2018-07-07T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-05-07T19:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:52:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:13:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:59:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:49:00.000000000\n",
      "Combined 2 tweets on 2018-06-28T03:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-25T16:58:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T14:00:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:21:00.000000000\n",
      "Combined 3 tweets on 2018-06-23T11:19:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:16:00.000000000\n",
      "Combined 2 tweets on 2018-06-21T20:46:00.000000000\n",
      "Combined 4 tweets on 2018-06-19T13:52:00.000000000\n",
      "Combined 2 tweets on 2018-06-18T13:50:00.000000000\n",
      "Combined 3 tweets on 2018-06-17T14:36:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-06-15T11:56:00.000000000\n",
      "Combined 4 tweets on 2018-06-15T11:55:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T08:53:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:09:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:08:00.000000000\n",
      "Combined 2 tweets on 2018-06-13T09:40:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T20:40:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:58:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:56:00.000000000\n",
      "Combined 2 tweets on 2018-06-06T00:40:00.000000000\n",
      "Combined 2 tweets on 2018-05-30T12:47:00.000000000\n",
      "Combined 4 tweets on 2018-05-25T12:04:00.000000000\n",
      "Combined 2 tweets on 2018-05-23T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-05-16T18:40:00.000000000\n",
      "Combined 3 tweets on 2018-05-16T13:09:00.000000000\n",
      "Combined 2 tweets on 2018-08-05T20:33:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T13:10:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T12:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-20T10:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-19T22:30:00.000000000\n",
      "Combined 2 tweets on 2018-04-18T12:05:00.000000000\n",
      "Combined 3 tweets on 2018-04-18T09:42:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:55:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:26:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:25:00.000000000\n",
      "Combined 4 tweets on 2018-04-17T12:24:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:57:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:56:00.000000000\n",
      "Combined 2 tweets on 2018-03-28T21:31:00.000000000\n",
      "Combined 2 tweets on 2018-03-15T17:47:00.000000000\n",
      "Combined 2 tweets on 2018-03-02T14:40:00.000000000\n",
      "Combined 2 tweets on 2018-01-28T13:18:00.000000000\n",
      "Combined 2 tweets on 2018-01-21T02:31:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:53:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:52:00.000000000\n",
      "Combined 2 tweets on 2018-01-13T13:14:00.000000000\n"
     ]
    }
   ],
   "source": [
    "dates = tweets[\"date\"]\n",
    "duplicated = tweets[dates.isin(dates[dates.duplicated()])]\n",
    "duplicated_dates = duplicated['date'].unique()\n",
    "\n",
    "for date in duplicated_dates:\n",
    "    tweets = process_and_remove_duplicates(tweets, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tweets that are Trump's\n",
    "tweets1 = tweets[tweets['isRetweet'] == 'f'].reset_index(drop=True)\n",
    "tweets1 = tweets1[tweets1['text'].str.contains('RT @') == False]\n",
    "tweets1['cleaned_text'] = tweets1['text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S*', '', x, flags=re.MULTILINE))\n",
    "tweets1 = tweets1[tweets1['cleaned_text'].str.strip() != '']\n",
    "\n",
    "tweets2 = tweets1.loc[:, ['id', 'cleaned_text', 'favorites', 'retweets', 'date']]\n",
    "tweets2['date_new'] = [i + timedelta(hours = 8) for i in tweets2['date']]\n",
    "tweets2['date_part'] = [i.date() for i in tweets2['date_new']]\n",
    "tweets2['time_part'] = [i.time() for i in tweets2['date_new']]\n",
    "tweets2['hour'] = [int(str(i).split(\":\")[0]) for i in tweets2['time_part']]\n",
    "tweets2['year'] = [int(str(i).split(\"-\")[0]) for i in tweets2['date_part']]\n",
    "tweets2['month'] = [int(str(i).split(\"-\")[1]) for i in tweets2['date_part']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_mentions(string):\n",
    "    return re.sub(\"@[A-Za-z0-9]+\",\"\",string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2[\"cleaned_text\"] = tweets2[\"cleaned_text\"].str.lower()\n",
    "tweets2[\"cleaned_text2\"] = tweets2.cleaned_text.apply(remove_emoji)\n",
    "tweets2.cleaned_text2 = tweets2.cleaned_text.apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the function above and get tweets free of emoji's\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)\n",
    "\n",
    "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
    "tweets2['emoji_free_tweets'] = tweets2['cleaned_text'].apply(call_emoji_free)\n",
    "\n",
    "#Create a new column with url free tweets\n",
    "tweets2['url_free_tweets'] = tweets2['emoji_free_tweets'].apply(url_free_text)\n",
    "df = tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5088):\n",
    "    df.iloc[i,-3] = re.sub('[%s]' % re.escape(string.punctuation), '',df.iloc[i,-1])\n",
    "    df.iloc[i,-3] = re.sub(\"@[A-Za-z0-9]+\",\"\",df.iloc[i,-3])\n",
    "    df.iloc[i,-3]=re.sub('\\w*\\d\\w*', '', df.iloc[i,-3])\n",
    "    df.iloc[i,-3] = df.iloc[i,-3].strip(\"'\")\n",
    "    df.iloc[i,-3]=df.iloc[i,-3].replace(\"#\", \"\")\n",
    "    df.iloc[i,-3].strip('\"') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"trump_tweets_cleaned_2.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are combined after pre processing! (else kernel crashes)\n",
    "stock_price_2017 = pd.read_csv('yr2017.csv')\n",
    "stock_price_2018 = pd.read_csv('yr2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "stock_price_2017.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)\n",
    "stock_price_2018.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datetime objects\n",
    "stock_price_2017['TIME_M'] = pd.to_datetime(stock_price_2017['TIME_M'], format='%H:%M:%S.%f')\n",
    "stock_price_2018['TIME_M'] = pd.to_datetime(stock_price_2018['TIME_M'], format='%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, milliseconds from data\n",
    "stock_price_2017['TIME_M'] = stock_price_2017['TIME_M'].dt.floor('Min').dt.time\n",
    "stock_price_2018['TIME_M'] = stock_price_2018['TIME_M'].dt.floor('Min').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minute to minute data (one entry for each minute)\n",
    "stock_price_2017.drop_duplicates(subset=['TIME_M','DATE'],inplace=True)\n",
    "stock_price_2018.drop_duplicates(subset=['TIME_M', 'DATE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv, will be used for preprocessing later!\n",
    "stock_price = pd.concat([stock_price_2017,stock_price_2018], ignore_index=True, sort=False)\n",
    "stock_price.to_csv(\"stock_price.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolation of missing values\n",
    "idx = pd.date_range(stock_price.iloc[0, 0], stock_price.iloc[-1, 0], freq='1min')\n",
    "stock_price.index = pd.DatetimeIndex(stock_price.loc[:, 'DATE_TIME_M'])\n",
    "stock_price = stock_price.reindex(idx, fill_value='NaN')\n",
    "stock_price['DATE_TIME_M'] = stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_price.astype({'PRICE': 'float'})\n",
    "stock_price['PRICE'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_csv(\"stock_price.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
