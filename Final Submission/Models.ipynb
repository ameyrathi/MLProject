{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-95b28ce23f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#from tensorflow.keras.utils import plot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "# Import all libraries \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('processed_data.csv')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_topics = pd.get_dummies(master_df['topic'])\n",
    "\n",
    "X_lr = pd.concat([X.loc[:, ['compound', 'favorites', 'retweets']], dummy_topics], axis=1)\n",
    "y_lr = master_df.loc[:, '60mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing correlated features\n",
    "\n",
    "correlated_features = set()\n",
    "correlation_matrix = X_lr.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "            \n",
    "X_lr.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split and resampling\n",
    "\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = model_selection.train_test_split(X_lr, y_lr, test_size = 0.33, random_state = 2020)\n",
    "\n",
    "smote = SMOTE(random_state=0, sampling_strategy='not majority')\n",
    "X_train_lr, y_train_lr = smote.fit_sample(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all values\n",
    "X_train_lr = preprocessing.scale(X_train_lr)\n",
    "y_train_lr = preprocessing.scale(y_train_lr)\n",
    "X_test_lr = preprocessing.scale(X_test_lr)\n",
    "y_test_lr = preprocessing.scale(y_test_lr)\n",
    "\n",
    "# Linear regression\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_lr = regr.predict(X_test_lr)\n",
    "print('Linear metrics for stock price 60 mins after tweet')\n",
    "print('Linear score:', regr.score(X_test_lr, y_test_lr))\n",
    "print('Linear MSE', metrics.mean_squared_error(y_test_lr, y_pred_lr))\n",
    "print(regr.coef_, '\\n')\n",
    "\n",
    "print('-'*64)\n",
    "\n",
    "num_C = 5\n",
    "C = [1.0] * num_C\n",
    "for i in range(num_C):\n",
    "    C[i] = pow(10, i-5)\n",
    "print('Ridge and Lasso metrics for stock price 60 mins after tweet')\n",
    "for i in range(5):\n",
    "    # Lasso Regression\n",
    "    lasso = linear_model.Lasso(alpha = C[i])\n",
    "    lasso.fit(X_train_lr, y_train_lr)\n",
    "    y_pred = regr.predict(X_test_lr)\n",
    "    print('Alpha = ', C[i])\n",
    "    print('Lasso score:', lasso.score(X_test_lr, y_test_lr))\n",
    "    print('Lasso MSE', metrics.mean_squared_error(y_test_lr, y_pred_lr))\n",
    "    print('Lasso coefs:', lasso.coef_, '\\n')\n",
    "    \n",
    "    # Ridge regression\n",
    "    ridge = linear_model.Ridge(alpha = C[i])\n",
    "    ridge.fit(X_train_lr, y_train_lr)\n",
    "    y_pred_lr = regr.predict(X_test_lr)\n",
    "    print('Ridge score:', ridge.score(X_test_lr, y_test_lr))\n",
    "    print('Ridge MSE', metrics.mean_squared_error(y_test_lr, y_pred_lr))\n",
    "    print('Ridge coefs:', ridge.coef_, '\\n')\n",
    "    print('-'*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: Word Vectors as Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_reg_a_w2v = master_df.loc[:, 'lstm_text']\n",
    "y_lstm_reg_a_w2v = master_df.loc[:, '60mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_reg_a_w2v, X_test_lstm_reg_a_w2v, y_train_lstm_reg_a_w2v, y_test_lstm_reg_a_w2v = train_test_split(X_lstm_reg_a_w2v, y_lstm_reg_a_w2v, test_size=0.33, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words into corpus list\n",
    "\n",
    "lstm_reg_a_corpus_list = []\n",
    "\n",
    "for i in X_train_lstm_reg_a_w2v:\n",
    "    lstm_reg_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec\n",
    "\n",
    "lstm_reg_a_w2v_model = Word2Vec(lstm_reg_a_corpus_list, min_count=1, size=100)\n",
    "lstm_reg_a_w2v_weights = lstm_reg_a_w2v_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of longest sentence (for padding later on)\n",
    "\n",
    "lstm_reg_a_w2v_num_words = [len(i) for i in lstm_reg_a_w2v_corpus_list]\n",
    "lstm_reg_a_w2v_longest_sentence_len = max(lstm_reg_a_w2v_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sentences that are shorter than length of longest sentence in training data\n",
    "\n",
    "def word2vec_sentence_to_indices_padded(sentences, longest_sentence_len, word2vec_model):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_reg_a_w2v_padded = word2vec_sentence_to_indices_padded(X_train_lstm_reg_a_w2v, lstm_reg_a_w2v_longest_sentence_len, lstm_reg_a_w2v_model)\n",
    "X_test_lstm_reg_a_w2v_padded = word2vec_sentence_to_indices_padded(X_test_lstm_reg_a_w2v, lstm_reg_a_w2v_longest_sentence_len, lstm_reg_a_w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_reg_a_w2v(pretrained_weights, longest_sentence_len):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.2,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.2,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_a_w2v_ann = create_lstm_reg_a_w2v(lstm_reg_a_w2v_weights, lstm_reg_a_w2v_longest_sentence_len)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_reg_a_w2v_ann.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "lstm_reg_a_w2v_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    lstm_reg_a_w2v_ann,\n",
    "    to_file=\"lstm_reg_a_w2v_ann.png\",\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "lstm_reg_a_w2v_checkpoint_filepath = f'./lstm_reg_a_w2v/lstm_reg_a_w2v_{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "lstm_reg_a_w2v_history = lstm_reg_a_w2v_ann.fit(X_train_lstm_reg_a_w2v_padded, y_train_lstm_reg_a_w2v, validation_split=0.33, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_a_w2v_ann_loaded = create_lstm_reg_a_w2v(lstm_reg_a_w2v_weights, lstm_reg_a_w2v_longest_sentence_len)\n",
    "lstm_reg_a_w2v_ann_loaded.load_weights(lstm_reg_a_w2v_checkpoint_filepath)\n",
    "lstm_reg_a_w2v_ann_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc = lstm_reg_a_w2v_ann_loaded.evaluate(X_test_lstm_reg_a_w2v_padded, y_test_lstm_reg_a_w2v, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt(metrics.mean_squared_error(y_train_lstm_reg_a_w2v, lstm_reg_a_w2v_ann_loaded.predict(X_train_lstm_reg_a_w2v_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_a_w2v, lstm_reg_a_w2v_ann_loaded.predict(X_test_lstm_reg_a_w2v_padded)))}\")\n",
    "print(f\"Test R^2: {metrics.r2_score(y_test_lstm_reg_a_w2v, lstm_reg_a_w2v_ann_loaded.predict(X_test_lstm_reg_a_w2v_padded))}\")\n",
    "print(f\"Baseline MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_a_w2v, 0*y_test_lstm_reg_a_w2v))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(lstm_reg_a_w2v_history.history['loss'])\n",
    "plt.plot(lstm_reg_a_w2v_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_reg_a_glove = master_df.loc[:, 'lstm_text']\n",
    "y_lstm_reg_a_glove = master_df.loc[:, '60mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_reg_a_glove, X_test_lstm_reg_a_glove, y_train_lstm_reg_a_glove, y_test_lstm_reg_a_glove = train_test_split(X_lstm_reg_a_glove, y_lstm_reg_a_glove, test_size=0.33, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.50d.txt', encoding='utf8')\n",
    "glove_vocab = []\n",
    "glove_vocab_index = {}\n",
    "count = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    glove_vocab.append(word)\n",
    "    glove_vocab_index[word] = count\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    count += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(X_train_lstm_reg_a_glove)\n",
    "sequences = tokenizer.texts_to_sequences(X_train_lstm_reg_a_glove)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    global glove_vocab\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        try:\n",
    "            sentence_splitted = sentence.split()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        for word in sentence_splitted:\n",
    "            if word in glove_vocab:\n",
    "                indices.append(glove_vocab_index[word])\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_reg_a_glove_padded = glove_sentence_to_indices_padded(X_train_lstm_reg_a_glove, lstm_reg_a_w2v_longest_sentence_len)\n",
    "X_test_lstm_reg_a_glove_padded = glove_sentence_to_indices_padded(X_test_lstm_reg_a_glove, lstm_reg_a_w2v_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "        \n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            50,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=lstm_reg_a_w2v_longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_reg_a_glove(longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(embedding_layer_glove)\n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.2,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.2,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_a_glove_ann = create_lstm_reg_a_glove(lstm_reg_a_w2v_longest_sentence_len)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_reg_a_glove_ann.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "lstm_reg_a_glove_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    lstm_reg_a_glove_ann,\n",
    "    to_file=\"lstm_reg_a_glove_ann.png\",\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "lstm_reg_a_glove_checkpoint_filepath = f'./lstm_reg_a_glove/lstm_reg_a_glove_{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "lstm_reg_a_glove_history = lstm_reg_a_glove_ann.fit(X_train_lstm_reg_a_glove_padded, y_train_lstm_reg_a_glove, validation_split=0.33, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_a_glove_ann_loaded = create_lstm_reg_a_glove(lstm_reg_a_w2v_longest_sentence_len)\n",
    "lstm_reg_a_glove_ann_loaded.load_weights(lstm_reg_a_glove_checkpoint_filepath)\n",
    "lstm_reg_a_glove_ann_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc = lstm_reg_a_glove_ann_loaded.evaluate(X_test_lstm_reg_a_glove_padded, y_test_lstm_reg_a_glove, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt(metrics.mean_squared_error(y_train_lstm_reg_a_glove, lstm_reg_a_glove_ann_loaded.predict(X_train_lstm_reg_a_glove_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_a_glove, lstm_reg_a_glove_ann_loaded.predict(X_test_lstm_reg_a_glove_padded)))}\")\n",
    "print(f\"Test R^2: {metrics.r2_score(y_test_lstm_reg_a_glove, lstm_reg_a_glove_ann_loaded.predict(X_test_lstm_reg_a_glove_padded))}\")\n",
    "print(f\"Baseline MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_a_glove, 0*y_test_lstm_reg_a_glove))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(lstm_reg_a_glove_history.history['loss'])\n",
    "plt.plot(lstm_reg_a_glove_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: Word Vectors + 30min Price History as Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_reg_b_w2v = master_df.loc[:, ['lstm_text', 'prev_30_min_prices']\n",
    "y_lstm_reg_b_w2v = master_df.loc[:, '60mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_reg_b_w2v, X_test_lstm_reg_b_w2v, y_train_lstm_reg_b_w2v, y_test_lstm_reg_b_w2v = train_test_split(X_lstm_reg_b_w2v, y_lstm_reg_b_w2v, test_size=0.33, random_state=2020)\n",
    "\n",
    "def parse_price_history(price_history):\n",
    "    result = [float(i) for i in price_history.strip('[').strip(']').replace(' ', '').split(',')]\n",
    "    return result\n",
    "\n",
    "X_train_price_history = X_train_lstm_reg_b_w2v.iloc[:, 1].apply(parse_price_history)\n",
    "X_train_price_history = np.stack(X_train_price_history)\n",
    "X_train_price_history = X_train_price_history.reshape(X_train_price_history.shape[0], X_train_price_history.shape[1],1)\n",
    "\n",
    "X_test_price_history = X_test_lstm_reg_b_w2v.iloc[:, 1].apply(parse_price_history)\n",
    "X_test_price_history = np.stack(X_test_price_history)\n",
    "X_test_price_history = X_test_price_history.reshape(X_test_price_history.shape[0], X_test_price_history.shape[1],1)\n",
    "    \n",
    "# Word2Vec embeddings are the same as the one used in Model A\n",
    "X_train_lstm_reg_b_w2v = [X_train_lstm_reg_a_w2v_padded, np.array(X_train_price_history)]\n",
    "X_test_lstm_reg_b_w2v = [X_test_lstm_reg_a_w2v_padded, X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_reg_b_w2v(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='tanh')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_b_w2v_ann = create_lstm_reg_b_w2v(lstm_reg_a_w2v_weights, lstm_reg_a_w2v_longest_sentence_len)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_reg_b_w2v_ann.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "lstm_reg_b_w2v_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    lstm_reg_b_w2v_ann,\n",
    "    to_file=\"lstm_reg_b_w2v_ann.png\",\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "lstm_reg_b_w2v_checkpoint_filepath = f'./lstm_reg_b_w2v/lstm_reg_b_w2v_{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "lstm_reg_b_w2v_history = lstm_reg_b_w2v_ann.fit(X_train_lstm_reg_a_w2v_padded, y_train_lstm_reg_b_w2v, validation_split=0.33, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_b_w2v_ann_loaded = create_lstm_reg_b_w2v(lstm_reg_a_w2v_weights, lstm_reg_a_w2v_longest_sentence_len, (30,1,))\n",
    "lstm_reg_b_w2v_ann_loaded.load_weights(lstm_reg_b_w2v_checkpoint_filepath)\n",
    "lstm_reg_b_w2v_ann_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc = lstm_reg_b_w2v_ann_loaded.evaluate(X_test_lstm_reg_a_w2v_padded, y_test_lstm_reg_b_w2v, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt(metrics.mean_squared_error(y_train_lstm_reg_b_w2v, lstm_reg_b_w2v_ann_loaded.predict(X_train_lstm_reg_a_w2v_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_b_w2v, lstm_reg_b_w2v_ann_loaded.predict(X_test_lstm_reg_a_w2v_padded)))}\")\n",
    "print(f\"Test R^2: {metrics.r2_score(y_test_lstm_reg_b_w2v, lstm_reg_b_w2v_ann_loaded.predict(X_test_lstm_reg_a_w2v_padded))}\")\n",
    "print(f\"Baseline MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_b_w2v, 0*y_test_lstm_reg_b_w2v))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(lstm_reg_b_w2v_history.history['loss'])\n",
    "plt.plot(lstm_reg_b_w2v_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove embeddings are the same as the one used in Model A\n",
    "X_train_lstm_reg_b_glove = [X_train_lstm_reg_a_glove_padded, np.array(X_train_price_history)]\n",
    "X_test_lstm_reg_b_glove = [X_test_lstm_reg_a_glove_padded, X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_reg_b_glove(longest_sentence_len, price_history_shape):\n",
    "    global embedding_layer_glove\n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = embedding_layer_glove(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='tanh')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_b_glove_ann = create_lstm_reg_b_glove(lstm_reg_a_glove_longest_sentence_len, (30,1,))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_reg_b_glove_ann.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "lstm_reg_b_glove_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    lstm_reg_b_glove_ann,\n",
    "    to_file=\"lstm_reg_b_glove_ann.png\",\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "lstm_reg_b_glove_checkpoint_filepath = f'./lstm_reg_b_glove/lstm_reg_b_glove_{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "lstm_reg_b_glove_history = lstm_reg_b_glove_ann.fit(X_train_lstm_reg_a_glove_padded, y_train_lstm_reg_b_glove, validation_split=0.33, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_reg_b_glove_ann_loaded = create_lstm_reg_b_glove(lstm_reg_b_glove_longest_sentence_len, (30,1,))\n",
    "lstm_reg_b_glove_ann_loaded.load_weights(lstm_reg_b_glove_checkpoint_filepath)\n",
    "lstm_reg_b_glove_ann_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc = lstm_reg_b_glove_ann_loaded.evaluate(X_test_lstm_reg_a_glove_padded, y_test_lstm_reg_b_glove, verbose=1)\n",
    "\n",
    "print(f\"Training MSE: {np.sqrt(metrics.mean_squared_error(y_train_lstm_reg_b_glove, lstm_reg_b_glove_ann_loaded.predict(X_train_lstm_reg_a_glove_padded)))}\")\n",
    "print(f\"Test MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_b_glove, lstm_reg_b_glove_ann_loaded.predict(X_test_lstm_reg_a_glove_padded)))}\")\n",
    "print(f\"Test R^2: {metrics.r2_score(y_test_lstm_reg_b_glove, lstm_reg_b_glove_ann_loaded.predict(X_test_lstm_reg_a_glove_padded))}\")\n",
    "print(f\"Baseline MSE: {np.sqrt(metrics.mean_squared_error(y_test_lstm_reg_b_glove, 0*y_test_lstm_reg_b_glove))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(lstm_reg_b_glove_history.history['loss'])\n",
    "plt.plot(lstm_reg_b_glove_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Text-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/Users/lavanyajindal/Documents/GitHub/MLProject/master_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 23)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>text_sentiment_analysis</th>\n",
       "      <th>text_LSTM</th>\n",
       "      <th>60mins_price_diff_abs</th>\n",
       "      <th>60mins_price_diff_perc</th>\n",
       "      <th>45mins_price_diff_abs</th>\n",
       "      <th>45mins_price_diff_perc</th>\n",
       "      <th>30mins_price_diff_abs</th>\n",
       "      <th>30mins_price_diff_perc</th>\n",
       "      <th>15mins_price_diff_abs</th>\n",
       "      <th>15mins_price_diff_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand! https://t.co/NvPeleVmub</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you Rand!</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>join me live from fort myer in arlington virginia</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole! https://t.co/KlWN05uFOx</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you Nicole!</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on .</td>\n",
       "      <td>thank you to shawn steel for the nice word on</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.787250e+17</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>134210</td>\n",
       "      <td>36346</td>\n",
       "      <td>2017-06-24 21:23:00</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>make america great again</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  favorites  \\\n",
       "0  9.353400e+17            Thank you Rand! https://t.co/NvPeleVmub      42793   \n",
       "1  8.997980e+17  Join me live from Fort Myer in Arlington, Virg...      36009   \n",
       "2  8.939700e+17          Thank you Nicole! https://t.co/KlWN05uFOx      43367   \n",
       "3  8.819770e+17  Thank you to Shawn Steel for the nice words on...      50956   \n",
       "4  8.787250e+17                          MAKE AMERICA GREAT AGAIN!     134210   \n",
       "\n",
       "   retweets                 date       tweet_datetime   date_part time_part  \\\n",
       "0      9125  2017-11-28 02:50:00  2017-11-28 10:50:00  2017-11-28  10:50:00   \n",
       "1      4891  2017-08-22 01:00:00  2017-08-22 09:00:00  2017-08-22  09:00:00   \n",
       "2      8275  2017-05-08 23:01:00  2017-05-09 07:01:00  2017-05-09  07:01:00   \n",
       "3      7465  2017-03-07 20:44:00  2017-03-08 04:44:00  2017-03-08  04:44:00   \n",
       "4     36346  2017-06-24 21:23:00  2017-06-25 05:23:00  2017-06-25  05:23:00   \n",
       "\n",
       "   hour  year           ...            \\\n",
       "0    10  2017           ...             \n",
       "1     9  2017           ...             \n",
       "2     7  2017           ...             \n",
       "3     4  2017           ...             \n",
       "4     5  2017           ...             \n",
       "\n",
       "                             text_sentiment_analysis  \\\n",
       "0                                    Thank you Rand!   \n",
       "1  Join me live from Fort Myer in Arlington, Virg...   \n",
       "2                                  Thank you Nicole!   \n",
       "3   Thank you to Shawn Steel for the nice words on .   \n",
       "4                          MAKE AMERICA GREAT AGAIN!   \n",
       "\n",
       "                                           text_LSTM 60mins_price_diff_abs  \\\n",
       "0                                     thank you rand              0.385000   \n",
       "1  join me live from fort myer in arlington virginia              0.590000   \n",
       "2                                   thank you nicole              0.065000   \n",
       "3      thank you to shawn steel for the nice word on              0.142857   \n",
       "4                           make america great again              0.005714   \n",
       "\n",
       "  60mins_price_diff_perc 45mins_price_diff_abs  45mins_price_diff_perc  \\\n",
       "0               0.001475              0.115000                0.000440   \n",
       "1               0.002421              0.590000                0.002421   \n",
       "2               0.000271              0.015000                0.000063   \n",
       "3               0.000603              0.096000                0.000405   \n",
       "4               0.000023              0.004286                0.000018   \n",
       "\n",
       "   30mins_price_diff_abs  30mins_price_diff_perc  15mins_price_diff_abs  \\\n",
       "0              -0.010000               -0.000038               0.050000   \n",
       "1              -0.090000               -0.000369              -0.040000   \n",
       "2               0.085000                0.000354               0.065000   \n",
       "3               0.130000                0.000549               0.035000   \n",
       "4               0.002857                0.000012               0.001429   \n",
       "\n",
       "   15mins_price_diff_perc  \n",
       "0                0.000191  \n",
       "1               -0.000164  \n",
       "2                0.000271  \n",
       "3                0.000148  \n",
       "4                0.000006  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the range of the absolute diff in price change after tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.989999999999952"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df1.loc[:,\"60mins_price_diff_abs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9699999999999704"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df1.loc[:,\"60mins_price_diff_abs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a target column for the y label {-1,0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = []\n",
    "\n",
    "for i in range(0, len(df1)):\n",
    "    \n",
    "    if df1.loc[i,'60mins_price_diff_abs'] >= 0.020000:\n",
    "        target_value.append(1)\n",
    "\n",
    "    elif df1.loc[i,'60mins_price_diff_abs'] <= -0.003393:\n",
    "        target_value.append(-1)\n",
    "\n",
    "    else:\n",
    "        target_value.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:, 'target'] = target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>text_LSTM</th>\n",
       "      <th>60mins_price_diff_abs</th>\n",
       "      <th>60mins_price_diff_perc</th>\n",
       "      <th>45mins_price_diff_abs</th>\n",
       "      <th>45mins_price_diff_perc</th>\n",
       "      <th>30mins_price_diff_abs</th>\n",
       "      <th>30mins_price_diff_perc</th>\n",
       "      <th>15mins_price_diff_abs</th>\n",
       "      <th>15mins_price_diff_perc</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand! https://t.co/NvPeleVmub</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>join me live from fort myer in arlington virginia</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole! https://t.co/KlWN05uFOx</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>thank you to shawn steel for the nice word on</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.787250e+17</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>134210</td>\n",
       "      <td>36346</td>\n",
       "      <td>2017-06-24 21:23:00</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>make america great again</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  favorites  \\\n",
       "0  9.353400e+17            Thank you Rand! https://t.co/NvPeleVmub      42793   \n",
       "1  8.997980e+17  Join me live from Fort Myer in Arlington, Virg...      36009   \n",
       "2  8.939700e+17          Thank you Nicole! https://t.co/KlWN05uFOx      43367   \n",
       "3  8.819770e+17  Thank you to Shawn Steel for the nice words on...      50956   \n",
       "4  8.787250e+17                          MAKE AMERICA GREAT AGAIN!     134210   \n",
       "\n",
       "   retweets                 date       tweet_datetime   date_part time_part  \\\n",
       "0      9125  2017-11-28 02:50:00  2017-11-28 10:50:00  2017-11-28  10:50:00   \n",
       "1      4891  2017-08-22 01:00:00  2017-08-22 09:00:00  2017-08-22  09:00:00   \n",
       "2      8275  2017-05-08 23:01:00  2017-05-09 07:01:00  2017-05-09  07:01:00   \n",
       "3      7465  2017-03-07 20:44:00  2017-03-08 04:44:00  2017-03-08  04:44:00   \n",
       "4     36346  2017-06-24 21:23:00  2017-06-25 05:23:00  2017-06-25  05:23:00   \n",
       "\n",
       "   hour  year   ...                                            text_LSTM  \\\n",
       "0    10  2017   ...                                       thank you rand   \n",
       "1     9  2017   ...    join me live from fort myer in arlington virginia   \n",
       "2     7  2017   ...                                     thank you nicole   \n",
       "3     4  2017   ...        thank you to shawn steel for the nice word on   \n",
       "4     5  2017   ...                             make america great again   \n",
       "\n",
       "  60mins_price_diff_abs 60mins_price_diff_perc 45mins_price_diff_abs  \\\n",
       "0              0.385000               0.001475              0.115000   \n",
       "1              0.590000               0.002421              0.590000   \n",
       "2              0.065000               0.000271              0.015000   \n",
       "3              0.142857               0.000603              0.096000   \n",
       "4              0.005714               0.000023              0.004286   \n",
       "\n",
       "  45mins_price_diff_perc  30mins_price_diff_abs  30mins_price_diff_perc  \\\n",
       "0               0.000440              -0.010000               -0.000038   \n",
       "1               0.002421              -0.090000               -0.000369   \n",
       "2               0.000063               0.085000                0.000354   \n",
       "3               0.000405               0.130000                0.000549   \n",
       "4               0.000018               0.002857                0.000012   \n",
       "\n",
       "   15mins_price_diff_abs  15mins_price_diff_perc  target  \n",
       "0               0.050000                0.000191       1  \n",
       "1              -0.040000               -0.000164       1  \n",
       "2               0.065000                0.000271       1  \n",
       "3               0.035000                0.000148       1  \n",
       "4               0.001429                0.000006       0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing values before fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197, 24)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the x and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.text_LSTM\n",
    "y =df1.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "#test size is 20% of the dataset\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an understanding of the class distribution in training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    706\n",
       " 1    698\n",
       " 0    353\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    175\n",
       "-1    171\n",
       " 0     94\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Classifier: 37.5%\n",
      "\n",
      "Confusion Matrix of Naive Bayes Classifier:\n",
      "\n",
      "[[75  5 91]\n",
      " [50  1 43]\n",
      " [82  4 89]]\n",
      "\n",
      "Classification Report of Naive Bayes Classifier:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.44      0.40       171\n",
      "           0       0.10      0.01      0.02        94\n",
      "           1       0.40      0.51      0.45       175\n",
      "\n",
      "    accuracy                           0.38       440\n",
      "   macro avg       0.29      0.32      0.29       440\n",
      "weighted avg       0.32      0.38      0.34       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive-Bayes classification\n",
    "#more no. of features doesnt seem to improve accuracy after 5000\n",
    "#better results using cv + tfidftrransformer\n",
    "pipe1_ = Pipeline([('vect', CountVectorizer(stop_words=\"english\",max_features=1000)), ('tfidf', TfidfTransformer()), ('model1', MultinomialNB())])\n",
    "#pipe3 = Pipeline([('vect', CountVectorizer(stop_words=\"english\")),  ('model', MultinomialNB())])\n",
    "model_nb1 = pipe1_.fit(x_train, y_train)\n",
    "nb_pred1 = model_nb1.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of Naive Bayes Classifier: {}%\".format(round(accuracy_score(y_test, nb_pred1)*100,2)))\n",
    "print(\"\\nConfusion Matrix of Naive Bayes Classifier:\\n\")\n",
    "print(confusion_matrix(y_test, nb_pred1))\n",
    "print(\"\\nClassification Report of Naive Bayes Classifier:\\n\")\n",
    "print(classification_report(y_test, nb_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model1__alpha': 0.01}\n",
      "\n",
      "Accuracy: 37.27272727272727\n",
      "\n",
      "Confusion Matrix:\n",
      " [[67 17 87]\n",
      " [51  4 39]\n",
      " [69 13 93]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.39      0.37       171\n",
      "           0       0.12      0.04      0.06        94\n",
      "           1       0.42      0.53      0.47       175\n",
      "\n",
      "    accuracy                           0.37       440\n",
      "   macro avg       0.30      0.32      0.30       440\n",
      "weighted avg       0.33      0.37      0.35       440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model1__alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(pipe1_, \n",
    "                           n_jobs = -1,cv = 3,param_grid=param_grid, verbose = 2, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy * 100)\n",
    "    print()\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate(best_grid,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Feature-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vedant to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Text-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Classifier: 41.14%\n",
      "\n",
      "Confusion Matrix of Logistic Regression Classifier:\n",
      "\n",
      "[[84  9 78]\n",
      " [40  3 51]\n",
      " [75  6 94]]\n",
      "\n",
      "CLassification Report of Logistic Regression Classifier:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.49      0.45       171\n",
      "           0       0.17      0.03      0.05        94\n",
      "           1       0.42      0.54      0.47       175\n",
      "\n",
      "    accuracy                           0.41       440\n",
      "   macro avg       0.34      0.35      0.33       440\n",
      "weighted avg       0.37      0.41      0.38       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression classification\n",
    "pipe2_ = Pipeline([('vect', CountVectorizer(stop_words=\"english\", min_df=3, ngram_range=(1,1), max_features=200)), ('tfidf', TfidfTransformer()), ('model2', LogisticRegression())])\n",
    "#models must be in the correct order, where previous models must have fit and transform functionality while \n",
    "#last model must only have fit\n",
    "model_lr1 = pipe2_.fit(x_train, y_train)\n",
    "lr_pred1 = model_lr1.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression Classifier: {}%\".format(round(accuracy_score(y_test, lr_pred1)*100,2)))\n",
    "print(\"\\nConfusion Matrix of Logistic Regression Classifier:\\n\")\n",
    "print(confusion_matrix(y_test, lr_pred1))\n",
    "print(\"\\nCLassification Report of Logistic Regression Classifier:\\n\")\n",
    "print(classification_report(y_test, lr_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    8.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model2__C': 1, 'model2__max_iter': 500, 'model2__multi_class': 'auto', 'model2__penalty': 'l2', 'model2__solver': 'newton-cg'}\n",
      "\n",
      "Accuracy: 41.13636363636364\n",
      "\n",
      "Confusion Matrix:\n",
      " [[84  9 78]\n",
      " [40  3 51]\n",
      " [75  6 94]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.49      0.45       171\n",
      "           0       0.17      0.03      0.05        94\n",
      "           1       0.42      0.54      0.47       175\n",
      "\n",
      "    accuracy                           0.41       440\n",
      "   macro avg       0.34      0.35      0.33       440\n",
      "weighted avg       0.37      0.41      0.38       440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 336 out of 336 | elapsed:   10.4s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model2__penalty': ['l1', 'l2'],\n",
    "    'model2__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'model2__solver': ['newton-cg', 'lbfgs'],\n",
    "    'model2__max_iter': [500, 1000],\n",
    "    'model2__multi_class': ['auto', 'multinomial']\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(pipe2_, \n",
    "                           n_jobs = -1,cv=3,param_grid=param_grid, verbose = 2, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy * 100)\n",
    "    print()\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate(best_grid,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Feature-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vedant to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Text-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Classifier: 40.0%\n",
      "\n",
      "Confusion Matrix of Logistic Regression Classifier:\n",
      "\n",
      "[[84 14 73]\n",
      " [47  9 38]\n",
      " [80 12 83]]\n",
      "\n",
      "CLassification Report of Logistic Regression Classifier:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.49      0.44       171\n",
      "           0       0.26      0.10      0.14        94\n",
      "           1       0.43      0.47      0.45       175\n",
      "\n",
      "    accuracy                           0.40       440\n",
      "   macro avg       0.36      0.35      0.34       440\n",
      "weighted avg       0.38      0.40      0.38       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest classification\n",
    "pipe3_ = Pipeline([('vect', CountVectorizer(stop_words=\"english\", min_df=3, ngram_range=(1,1), max_features=200)), ('tfidf', TfidfTransformer()), ('model3', RandomForestClassifier())])\n",
    "#models must be in the correct order, where previous models must have fit and transform functionality while \n",
    "#last model must only have fit\n",
    "model_lr1 = pipe3_.fit(x_train, y_train)\n",
    "lr_pred1 = model_lr1.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression Classifier: {}%\".format(round(accuracy_score(y_test, lr_pred1)*100,2)))\n",
    "print(\"\\nConfusion Matrix of Logistic Regression Classifier:\\n\")\n",
    "print(confusion_matrix(y_test, lr_pred1))\n",
    "print(\"\\nCLassification Report of Logistic Regression Classifier:\\n\")\n",
    "print(classification_report(y_test, lr_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'model3', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'model3__bootstrap', 'model3__ccp_alpha', 'model3__class_weight', 'model3__criterion', 'model3__max_depth', 'model3__max_features', 'model3__max_leaf_nodes', 'model3__max_samples', 'model3__min_impurity_decrease', 'model3__min_impurity_split', 'model3__min_samples_leaf', 'model3__min_samples_split', 'model3__min_weight_fraction_leaf', 'model3__n_estimators', 'model3__n_jobs', 'model3__oob_score', 'model3__random_state', 'model3__verbose', 'model3__warm_start'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3_.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model3__bootstrap': True, 'model3__max_depth': 10, 'model3__max_features': 3, 'model3__max_leaf_nodes': 5, 'model3__min_samples_leaf': 6, 'model3__n_estimators': 1000}\n",
      "\n",
      "Accuracy: 40.909090909090914\n",
      "\n",
      "Confusion Matrix:\n",
      " [[108   0  63]\n",
      " [ 51   0  43]\n",
      " [103   0  72]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.63      0.50       171\n",
      "           0       0.00      0.00      0.00        94\n",
      "           1       0.40      0.41      0.41       175\n",
      "\n",
      "    accuracy                           0.41       440\n",
      "   macro avg       0.27      0.35      0.30       440\n",
      "weighted avg       0.32      0.41      0.36       440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lavanyajindal/Desktop/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model3__bootstrap': [True],\n",
    "    'model3__max_leaf_nodes':[4,5,6],\n",
    "    'model3__max_depth': [10,15],\n",
    "    'model3__max_features': [2, 3],\n",
    "    'model3__min_samples_leaf': [3, 4, 5, 6],\n",
    "    #'model3__min_samples_split': [3, 4, 5, 6],\n",
    "    'model3__n_estimators': [1000, 1300]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(pipe3_, \n",
    "                           n_jobs = -1,cv=3,param_grid=param_grid, verbose = 2, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy * 100)\n",
    "    print()\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate(best_grid,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Feature-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vedant to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Text-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Classifier: 41.36%\n",
      "\n",
      "Confusion Matrix of SVM Classifier:\n",
      "\n",
      "[[88  3 80]\n",
      " [57  1 36]\n",
      " [81  1 93]]\n",
      "\n",
      "Classification Report of SVM Classifier:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.51      0.44       171\n",
      "           0       0.20      0.01      0.02        94\n",
      "           1       0.44      0.53      0.48       175\n",
      "\n",
      "    accuracy                           0.41       440\n",
      "   macro avg       0.34      0.35      0.32       440\n",
      "weighted avg       0.37      0.41      0.37       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Support Vector classification\n",
    "#accuracy score is better without applying the TFIDFTRANSFORMER()\n",
    "pipe4_ = Pipeline([('vect', CountVectorizer(stop_words=\"english\")), ('tfidf', TfidfTransformer()), ('model4', SVC())])\n",
    "#pipe2 = Pipeline([('vect', CountVectorizer(stop_words=\"english\")),  ('model', LinearSVC())])\n",
    "model_svc1 = pipe4_.fit(x_train, y_train)\n",
    "svc_pred1 = model_svc1.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of SVM Classifier: {}%\".format(round(accuracy_score(y_test, svc_pred1)*100,2)))\n",
    "print(\"\\nConfusion Matrix of SVM Classifier:\\n\")\n",
    "print(confusion_matrix(y_test, svc_pred1))\n",
    "print(\"\\nClassification Report of SVM Classifier:\\n\")\n",
    "print(classification_report(y_test, svc_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'model4', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'model4__C', 'model4__break_ties', 'model4__cache_size', 'model4__class_weight', 'model4__coef0', 'model4__decision_function_shape', 'model4__degree', 'model4__gamma', 'model4__kernel', 'model4__max_iter', 'model4__probability', 'model4__random_state', 'model4__shrinking', 'model4__tol', 'model4__verbose'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4_.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 56 candidates, totalling 168 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 out of 168 | elapsed:   45.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model4__C': 10, 'model4__gamma': 'scale', 'model4__kernel': 'linear'}\n",
      "\n",
      "Accuracy: 37.72727272727273\n",
      "\n",
      "Confusion Matrix:\n",
      " [[76 25 70]\n",
      " [43 16 35]\n",
      " [72 29 74]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.44      0.42       171\n",
      "           0       0.23      0.17      0.20        94\n",
      "           1       0.41      0.42      0.42       175\n",
      "\n",
      "    accuracy                           0.38       440\n",
      "   macro avg       0.35      0.35      0.34       440\n",
      "weighted avg       0.37      0.38      0.37       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model4__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'model4__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'model4__gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search = GridSearchCV(pipe4_, \n",
    "                           n_jobs = -1,cv=3,param_grid=param_grid, verbose = 2, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy * 100)\n",
    "    print()\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate(best_grid,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Feature-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vedant to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brian to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
