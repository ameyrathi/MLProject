{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2246 entries, 0 to 2245\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   id                        2246 non-null   float64\n",
      " 1   text                      2199 non-null   object \n",
      " 2   favorites                 2246 non-null   int64  \n",
      " 3   retweets                  2246 non-null   int64  \n",
      " 4   date                      2246 non-null   object \n",
      " 5   tweet_datetime            2246 non-null   object \n",
      " 6   date_part                 2246 non-null   object \n",
      " 7   time_part                 2246 non-null   object \n",
      " 8   hour                      2246 non-null   int64  \n",
      " 9   year                      2246 non-null   int64  \n",
      " 10  month                     2246 non-null   int64  \n",
      " 11  datetime_60mins_after     2246 non-null   object \n",
      " 12  price_60mins_after        2246 non-null   float64\n",
      " 13  datetime_20mins_before_x  2246 non-null   object \n",
      " 14  datetime_now              2246 non-null   object \n",
      " 15  price_now                 2246 non-null   float64\n",
      " 16  datetime_20mins_before_y  2246 non-null   object \n",
      " 17  60mins_price_diff_abs     2246 non-null   float64\n",
      " 18  60mins_price_diff_perc    2246 non-null   float64\n",
      "dtypes: float64(5), int64(5), object(9)\n",
      "memory usage: 333.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.dropna(subset=['text'])\n",
    "vals = np.round(model_a_df[\"60mins_price_diff_abs\"].values,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_a_df['text'], model_a_df['60mins_price_diff_abs'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0035844155844756594\n",
      "0.0181071428572295\n"
     ]
    }
   ],
   "source": [
    "benchmark_1 = np.percentile(y_train,40)\n",
    "benchmark_2 = np.percentile(y_train,60)\n",
    "print(benchmark_1)\n",
    "print(benchmark_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.apply(lambda x: 2 if x > benchmark_2 else 0 if x<benchmark_1 else 1)\n",
    "y_test = y_test.apply(lambda x: 2 if x > benchmark_2 else 0 if x<benchmark_1 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in x_train:\n",
    "    corpus.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(corpus, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dimension = 100\n",
    "embeddings_index = {}\n",
    "f = open(f'glove/glove.twitter.27B.{dimension}d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4823 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#text to integers\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "longest_sentence_len = 30\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=longest_sentence_len, padding='post')\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(word_index.keys())\n",
    "def prepare_test_x_glove(x):\n",
    "    global unique_words\n",
    "    global word_index\n",
    "    global longest_sentence_len\n",
    "    \n",
    "    result = []\n",
    "    for tweet in x:\n",
    "        indices = []\n",
    "        for word in tweet.split():\n",
    "            if word in unique_words:\n",
    "                indices.append(word_index[word])\n",
    "            else:\n",
    "                indices.append(0)\n",
    "            \n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')\n",
    "\n",
    "x_test_padded = prepare_test_x_glove(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.19755600814664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(726, 30)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean([len(sequence) for sequence in sequences]))\n",
    "x_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, dimension))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer_glove = Embedding(vocab_size,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_classification():\n",
    "    global embedding_layer_glove\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(embedding_layer_glove)\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(3, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 30, 100)           482400    \n",
      "_________________________________________________________________\n",
      "lstm_88 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten_67 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 524,835\n",
      "Trainable params: 42,435\n",
      "Non-trainable params: 482,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_model_classification()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "classification_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473, 30)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1264    0.390000\n",
       "1098    0.007679\n",
       "721     0.006667\n",
       "18      0.060000\n",
       "958    -0.007375\n",
       "          ...   \n",
       "1680    0.007143\n",
       "1114   -0.020000\n",
       "1149    0.010536\n",
       "1319    0.003750\n",
       "877    -0.031667\n",
       "Name: 60mins_price_diff_abs, Length: 1473, dtype: float64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1178 samples, validate on 295 samples\n",
      "Epoch 1/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.2147 - accuracy: 0.3973\n",
      "Epoch 00001: val_loss improved from inf to 1.05218, saving model to ./model_a_checkpoint/classification 19112020 1648h.h5\n",
      "1178/1178 [==============================] - 3s 3ms/sample - loss: 1.2092 - accuracy: 0.3973 - val_loss: 1.0522 - val_accuracy: 0.4000\n",
      "Epoch 2/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0515 - accuracy: 0.4184\n",
      "Epoch 00002: val_loss improved from 1.05218 to 1.04590, saving model to ./model_a_checkpoint/classification 19112020 1648h.h5\n",
      "1178/1178 [==============================] - 1s 883us/sample - loss: 1.0521 - accuracy: 0.4168 - val_loss: 1.0459 - val_accuracy: 0.4339\n",
      "Epoch 3/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0299 - accuracy: 0.4514\n",
      "Epoch 00003: val_loss did not improve from 1.04590\n",
      "1178/1178 [==============================] - 1s 824us/sample - loss: 1.0320 - accuracy: 0.4499 - val_loss: 1.0513 - val_accuracy: 0.4441\n",
      "Epoch 4/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0182 - accuracy: 0.4446\n",
      "Epoch 00004: val_loss improved from 1.04590 to 1.04447, saving model to ./model_a_checkpoint/classification 19112020 1648h.h5\n",
      "1178/1178 [==============================] - 1s 851us/sample - loss: 1.0211 - accuracy: 0.4431 - val_loss: 1.0445 - val_accuracy: 0.4169\n",
      "Epoch 5/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0085 - accuracy: 0.4696\n",
      "Epoch 00005: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 942us/sample - loss: 1.0078 - accuracy: 0.4703 - val_loss: 1.0570 - val_accuracy: 0.4000\n",
      "Epoch 6/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.9858 - accuracy: 0.4878\n",
      "Epoch 00006: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 920us/sample - loss: 0.9844 - accuracy: 0.4873 - val_loss: 1.0587 - val_accuracy: 0.3864\n",
      "Epoch 7/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.9630 - accuracy: 0.4878\n",
      "Epoch 00007: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 801us/sample - loss: 0.9591 - accuracy: 0.4898 - val_loss: 1.0741 - val_accuracy: 0.3627\n",
      "Epoch 8/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.9360 - accuracy: 0.5312\n",
      "Epoch 00008: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 833us/sample - loss: 0.9373 - accuracy: 0.5297 - val_loss: 1.0775 - val_accuracy: 0.3695\n",
      "Epoch 9/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.8967 - accuracy: 0.5686\n",
      "Epoch 00009: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 821us/sample - loss: 0.8937 - accuracy: 0.5705 - val_loss: 1.1051 - val_accuracy: 0.3932\n",
      "Epoch 10/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.8762 - accuracy: 0.5755\n",
      "Epoch 00010: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 789us/sample - loss: 0.8757 - accuracy: 0.5781 - val_loss: 1.1573 - val_accuracy: 0.3322\n",
      "Epoch 11/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.8216 - accuracy: 0.6172\n",
      "Epoch 00011: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 834us/sample - loss: 0.8206 - accuracy: 0.6180 - val_loss: 1.1987 - val_accuracy: 0.3559\n",
      "Epoch 12/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 0.7617 - accuracy: 0.6507\n",
      "Epoch 00012: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 838us/sample - loss: 0.7626 - accuracy: 0.6469 - val_loss: 1.2566 - val_accuracy: 0.3695\n",
      "Epoch 13/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.7241 - accuracy: 0.6641\n",
      "Epoch 00013: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 906us/sample - loss: 0.7230 - accuracy: 0.6647 - val_loss: 1.3545 - val_accuracy: 0.3390\n",
      "Epoch 14/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 0.6669 - accuracy: 0.7022\n",
      "Epoch 00014: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 816us/sample - loss: 0.6723 - accuracy: 0.7003 - val_loss: 1.4492 - val_accuracy: 0.3220\n",
      "Epoch 15/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 0.6344 - accuracy: 0.7109\n",
      "Epoch 00015: val_loss did not improve from 1.04447\n",
      "1178/1178 [==============================] - 1s 1ms/sample - loss: 0.6378 - accuracy: 0.7105 - val_loss: 1.3336 - val_accuracy: 0.3729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15e6797b8>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/classification {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "classification_model.fit(x_train_padded, to_categorical(y_train), validation_split=0.2, epochs=15,verbose=1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.31      0.35       283\n",
      "           1       0.17      0.01      0.03       138\n",
      "           2       0.43      0.69      0.53       305\n",
      "\n",
      "    accuracy                           0.42       726\n",
      "   macro avg       0.33      0.34      0.30       726\n",
      "weighted avg       0.37      0.42      0.36       726\n",
      "\n",
      "[[ 89   4 190]\n",
      " [ 44   2  92]\n",
      " [ 88   6 211]]\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_model_classification()\n",
    "classification_model.load_weights(checkpoint_filepath)\n",
    "classification_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "y_predict = classification_model.predict_classes(x_test_padded)\n",
    "print(metrics.classification_report(y_test, y_predict, labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/LSTM_glove_v2/assets\n"
     ]
    }
   ],
   "source": [
    "classification_model.save('./models/LSTM_glove_v2')\n",
    "# model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(corpus, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sentence_to_indices_padded(sentences):\n",
    "    global word2vec_model\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=30, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded_word2vec = word2vec_sentence_to_indices_padded(x_train)\n",
    "x_test_padded_word2vec = word2vec_sentence_to_indices_padded(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_word2vec():\n",
    "    global model_a_pretrained_weights\n",
    "    vocab_size, embedding_size = model_a_pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=30, dtype='int32'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[model_a_pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(32, return_sequences=False))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.Dense(3, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4821\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_size = model_a_pretrained_weights.shape\n",
    "print(vocab_size)\n",
    "print(embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 30, 100)           482100    \n",
      "_________________________________________________________________\n",
      "lstm_92 (LSTM)               (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "flatten_71 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 499,223\n",
      "Trainable params: 17,123\n",
      "Non-trainable params: 482,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_model_word2vec()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "classification_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1178 samples, validate on 295 samples\n",
      "Epoch 1/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0789 - accuracy: 0.3689\n",
      "Epoch 00001: val_loss improved from inf to 1.03186, saving model to ./model_a_checkpoint/classification 19112020 1659h.h5\n",
      "1178/1178 [==============================] - 3s 3ms/sample - loss: 1.0796 - accuracy: 0.3693 - val_loss: 1.0319 - val_accuracy: 0.4203\n",
      "Epoch 2/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 1.0713 - accuracy: 0.4026\n",
      "Epoch 00002: val_loss improved from 1.03186 to 1.02680, saving model to ./model_a_checkpoint/classification 19112020 1659h.h5\n",
      "1178/1178 [==============================] - 1s 761us/sample - loss: 1.0713 - accuracy: 0.4049 - val_loss: 1.0268 - val_accuracy: 0.4373\n",
      "Epoch 3/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 1.0606 - accuracy: 0.4026\n",
      "Epoch 00003: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 735us/sample - loss: 1.0610 - accuracy: 0.4058 - val_loss: 1.0288 - val_accuracy: 0.4373\n",
      "Epoch 4/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 1.0643 - accuracy: 0.3778\n",
      "Epoch 00004: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 720us/sample - loss: 1.0641 - accuracy: 0.3786 - val_loss: 1.0285 - val_accuracy: 0.4508\n",
      "Epoch 5/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0642 - accuracy: 0.4071\n",
      "Epoch 00005: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 742us/sample - loss: 1.0648 - accuracy: 0.4049 - val_loss: 1.0320 - val_accuracy: 0.4102\n",
      "Epoch 6/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0653 - accuracy: 0.4036\n",
      "Epoch 00006: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 1ms/sample - loss: 1.0654 - accuracy: 0.3998 - val_loss: 1.0284 - val_accuracy: 0.4068\n",
      "Epoch 7/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0659 - accuracy: 0.4259\n",
      "Epoch 00007: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 944us/sample - loss: 1.0647 - accuracy: 0.4270 - val_loss: 1.0319 - val_accuracy: 0.4068\n",
      "Epoch 8/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0670 - accuracy: 0.3880\n",
      "Epoch 00008: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 750us/sample - loss: 1.0667 - accuracy: 0.3871 - val_loss: 1.0288 - val_accuracy: 0.4000\n",
      "Epoch 9/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0658 - accuracy: 0.3866\n",
      "Epoch 00009: val_loss did not improve from 1.02680\n",
      "1178/1178 [==============================] - 1s 760us/sample - loss: 1.0637 - accuracy: 0.3879 - val_loss: 1.0366 - val_accuracy: 0.4271\n",
      "Epoch 10/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0610 - accuracy: 0.3950\n",
      "Epoch 00010: val_loss improved from 1.02680 to 1.02673, saving model to ./model_a_checkpoint/classification 19112020 1659h.h5\n",
      "1178/1178 [==============================] - 1s 945us/sample - loss: 1.0623 - accuracy: 0.3956 - val_loss: 1.0267 - val_accuracy: 0.4203\n",
      "Epoch 11/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 1.0622 - accuracy: 0.4118\n",
      "Epoch 00011: val_loss did not improve from 1.02673\n",
      "1178/1178 [==============================] - 1s 738us/sample - loss: 1.0618 - accuracy: 0.4083 - val_loss: 1.0302 - val_accuracy: 0.4339\n",
      "Epoch 12/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0645 - accuracy: 0.4125\n",
      "Epoch 00012: val_loss did not improve from 1.02673\n",
      "1178/1178 [==============================] - 1s 893us/sample - loss: 1.0627 - accuracy: 0.4134 - val_loss: 1.0294 - val_accuracy: 0.4237\n",
      "Epoch 13/15\n",
      "1152/1178 [============================>.] - ETA: 0s - loss: 1.0666 - accuracy: 0.3993\n",
      "Epoch 00013: val_loss did not improve from 1.02673\n",
      "1178/1178 [==============================] - 1s 854us/sample - loss: 1.0664 - accuracy: 0.3990 - val_loss: 1.0289 - val_accuracy: 0.4169\n",
      "Epoch 14/15\n",
      "1120/1178 [===========================>..] - ETA: 0s - loss: 1.0638 - accuracy: 0.3955\n",
      "Epoch 00014: val_loss did not improve from 1.02673\n",
      "1178/1178 [==============================] - 1s 802us/sample - loss: 1.0639 - accuracy: 0.3964 - val_loss: 1.0319 - val_accuracy: 0.4237\n",
      "Epoch 15/15\n",
      "1088/1178 [==========================>...] - ETA: 0s - loss: 1.0648 - accuracy: 0.4099\n",
      "Epoch 00015: val_loss did not improve from 1.02673\n",
      "1178/1178 [==============================] - 1s 730us/sample - loss: 1.0636 - accuracy: 0.4109 - val_loss: 1.0310 - val_accuracy: 0.4136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x149111208>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/classification {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "classification_model.fit(x_train_padded_word2vec, to_categorical(y_train), validation_split=0.2, epochs=15,verbose=1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.20      0.25       283\n",
      "           1       0.00      0.00      0.00       138\n",
      "           2       0.43      0.77      0.55       305\n",
      "\n",
      "    accuracy                           0.40       726\n",
      "   macro avg       0.25      0.33      0.27       726\n",
      "weighted avg       0.31      0.40      0.33       726\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_model_word2vec()\n",
    "classification_model.load_weights(checkpoint_filepath)\n",
    "classification_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "y_predict = classification_model.predict_classes(x_test_padded_word2vec)\n",
    "print(metrics.classification_report(y_test, y_predict, labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/LSTM_word2vec_v2/assets\n"
     ]
    }
   ],
   "source": [
    "classification_model.save('./models/LSTM_word2vec_v2')\n",
    "# model = keras.models.load_model('path/to/location')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
