{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2669 entries, 0 to 2668\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     2669 non-null   float64\n",
      " 1   cleaned_text           2660 non-null   object \n",
      " 2   favorites              2669 non-null   int64  \n",
      " 3   retweets               2669 non-null   int64  \n",
      " 4   date                   2669 non-null   object \n",
      " 5   tweet_datetime         2669 non-null   object \n",
      " 6   date_part              2669 non-null   object \n",
      " 7   time_part              2669 non-null   object \n",
      " 8   hour                   2669 non-null   int64  \n",
      " 9   year                   2669 non-null   int64  \n",
      " 10  month                  2669 non-null   int64  \n",
      " 11  sentiment_score        2669 non-null   object \n",
      " 12  tweet_length           2669 non-null   int64  \n",
      " 13  compound               2669 non-null   float64\n",
      " 14  entity                 2669 non-null   object \n",
      " 15  entity_type            2669 non-null   object \n",
      " 16  emoji_free_tweets      2661 non-null   object \n",
      " 17  url_free_tweets        2661 non-null   object \n",
      " 18  datetime_5mins_after   2669 non-null   object \n",
      " 19  price_5mins_after      2669 non-null   float64\n",
      " 20  5mins_price_diff_abs   2669 non-null   float64\n",
      " 21  5mins_price_diff_perc  2669 non-null   float64\n",
      " 22  prev_30mins_prices     2533 non-null   object \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 479.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.dropna(subset=['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>compound</th>\n",
       "      <th>entity</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>emoji_free_tweets</th>\n",
       "      <th>url_free_tweets</th>\n",
       "      <th>datetime_5mins_after</th>\n",
       "      <th>price_5mins_after</th>\n",
       "      <th>5mins_price_diff_abs</th>\n",
       "      <th>5mins_price_diff_perc</th>\n",
       "      <th>prev_30mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>['Rand']</td>\n",
       "      <td>['ORG']</td>\n",
       "      <td>thank you rand!</td>\n",
       "      <td>thank you rand!</td>\n",
       "      <td>2017-11-28 10:55:00</td>\n",
       "      <td>261.085000</td>\n",
       "      <td>-0.015000</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>[260.96, 261.03, 261.01, 261.015, 261.04, 261....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>join me live from fort myer in arlington virginia</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>['Fort Myer', 'Arlington', 'Virginia', '➡']</td>\n",
       "      <td>['GPE', 'GPE', 'GPE', 'ORG']</td>\n",
       "      <td>join me live from fort myer in arlington, virg...</td>\n",
       "      <td>join me live from fort myer in arlington, virg...</td>\n",
       "      <td>2017-08-22 09:05:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[243.76, 243.79, 243.85, 243.86, 243.81, 243.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>['Nicole']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>thank you nicole!</td>\n",
       "      <td>thank you nicole!</td>\n",
       "      <td>2017-05-09 07:06:00</td>\n",
       "      <td>239.920000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>[239.73, 239.73, 239.73, 239.73, 239.73, 239.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>thank you to shawn steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>['Shawn Steel']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>thank you to shawn steel for the nice words on...</td>\n",
       "      <td>thank you to shawn steel for the nice words on...</td>\n",
       "      <td>2017-03-08 04:49:00</td>\n",
       "      <td>236.913333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>[236.84, 236.84, 236.84, 236.8533333333333, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.778460e+17</td>\n",
       "      <td>great night in iowa  special people thank you</td>\n",
       "      <td>56446</td>\n",
       "      <td>8039</td>\n",
       "      <td>2017-06-22 11:11:00</td>\n",
       "      <td>2017-06-22 19:11:00</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>19:11:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8622</td>\n",
       "      <td>['night', 'Iowa']</td>\n",
       "      <td>['TIME', 'GPE']</td>\n",
       "      <td>great night in iowa - special people. thank you!</td>\n",
       "      <td>great night in iowa - special people. thank you!</td>\n",
       "      <td>2017-06-22 19:16:00</td>\n",
       "      <td>242.905000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>[242.85, 242.85, 242.85, 242.85, 242.85, 242.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>['Collusion', 'Russia', 'Crooked Hillary', 'Be...</td>\n",
       "      <td>['ORG', 'GPE', 'PERSON', 'PERSON']</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>2018-05-23 09:18:00</td>\n",
       "      <td>271.100000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>[271.18, 271.16, 271.18, 271.15, 271.08, 271.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6689</td>\n",
       "      <td>['Michael Flynn', 'Shadey James Comey', 'Leak'...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'ORDINAL', 'GPE...</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>2018-04-20 18:39:00</td>\n",
       "      <td>266.866667</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>[266.74625000000003, 266.7475, 266.74875, 266....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>['two', '@GCSOFlorida', 'HEROES', 'today', 'Sa...</td>\n",
       "      <td>['CARDINAL', 'ORG', 'ORG', 'DATE', 'PERSON', '...</td>\n",
       "      <td>my thoughts, prayers and condolences are with ...</td>\n",
       "      <td>my thoughts, prayers and condolences are with ...</td>\n",
       "      <td>2018-04-20 06:35:00</td>\n",
       "      <td>268.715000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>[268.77, 268.74333333333334, 268.7166666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9493</td>\n",
       "      <td>['Today', 'Congress', 'Congress', 'House', 'Se...</td>\n",
       "      <td>['DATE', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG']</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>2018-04-18 05:39:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[270.73, 270.7, 270.7275, 270.755, 270.7825, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>['Ronny L. Jackson', 'MD', 'Veterans Affairs',...</td>\n",
       "      <td>['PERSON', 'ORG', 'ORG', 'ORG', 'PERSON', 'ORG...</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>2018-03-29 05:36:00</td>\n",
       "      <td>260.997143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>[260.82, 260.89, 260.9, 260.87666666666667, 26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.353400e+17                                     thank you rand   \n",
       "1     8.997980e+17  join me live from fort myer in arlington virginia   \n",
       "2     8.939700e+17                                   thank you nicole   \n",
       "3     8.819770e+17  thank you to shawn steel for the nice words on...   \n",
       "4     8.778460e+17      great night in iowa  special people thank you   \n",
       "...            ...                                                ...   \n",
       "2664  9.990960e+17  if the person placed very early into my campai...   \n",
       "2665  9.874600e+17  so general michael flynn’s life can be totally...   \n",
       "2666  9.870960e+17  my thoughts prayers and condolences are with t...   \n",
       "2667  9.863570e+17  today’s court decision means that congress mus...   \n",
       "2668  9.791090e+17  i am pleased to announce that i intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4         56446      8039  2017-06-22 11:11:00  2017-06-22 19:11:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2664      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2665      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2666      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2667      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2668      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  ...  compound  \\\n",
       "0     2017-11-28  10:50:00    10  2017  ...    0.4199   \n",
       "1     2017-08-22  09:00:00     9  2017  ...    0.2960   \n",
       "2     2017-05-09  07:01:00     7  2017  ...    0.4199   \n",
       "3     2017-03-08  04:44:00     4  2017  ...    0.6486   \n",
       "4     2017-06-22  19:11:00    19  2017  ...    0.8622   \n",
       "...          ...       ...   ...   ...  ...       ...   \n",
       "2664  2018-05-23  09:13:00     9  2018  ...    0.6792   \n",
       "2665  2018-04-20  18:34:00    18  2018  ...   -0.6689   \n",
       "2666  2018-04-20  06:30:00     6  2018  ...    0.6900   \n",
       "2667  2018-04-18  05:34:00     5  2018  ...   -0.9493   \n",
       "2668  2018-03-29  05:31:00     5  2018  ...    0.9565   \n",
       "\n",
       "                                                 entity  \\\n",
       "0                                              ['Rand']   \n",
       "1           ['Fort Myer', 'Arlington', 'Virginia', '➡']   \n",
       "2                                            ['Nicole']   \n",
       "3                                       ['Shawn Steel']   \n",
       "4                                     ['night', 'Iowa']   \n",
       "...                                                 ...   \n",
       "2664  ['Collusion', 'Russia', 'Crooked Hillary', 'Be...   \n",
       "2665  ['Michael Flynn', 'Shadey James Comey', 'Leak'...   \n",
       "2666  ['two', '@GCSOFlorida', 'HEROES', 'today', 'Sa...   \n",
       "2667  ['Today', 'Congress', 'Congress', 'House', 'Se...   \n",
       "2668  ['Ronny L. Jackson', 'MD', 'Veterans Affairs',...   \n",
       "\n",
       "                                            entity_type  \\\n",
       "0                                               ['ORG']   \n",
       "1                          ['GPE', 'GPE', 'GPE', 'ORG']   \n",
       "2                                            ['PERSON']   \n",
       "3                                            ['PERSON']   \n",
       "4                                       ['TIME', 'GPE']   \n",
       "...                                                 ...   \n",
       "2664                 ['ORG', 'GPE', 'PERSON', 'PERSON']   \n",
       "2665  ['PERSON', 'PERSON', 'PERSON', 'ORDINAL', 'GPE...   \n",
       "2666  ['CARDINAL', 'ORG', 'ORG', 'DATE', 'PERSON', '...   \n",
       "2667        ['DATE', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG']   \n",
       "2668  ['PERSON', 'ORG', 'ORG', 'ORG', 'PERSON', 'ORG...   \n",
       "\n",
       "                                      emoji_free_tweets  \\\n",
       "0                                       thank you rand!   \n",
       "1     join me live from fort myer in arlington, virg...   \n",
       "2                                     thank you nicole!   \n",
       "3     thank you to shawn steel for the nice words on...   \n",
       "4      great night in iowa - special people. thank you!   \n",
       "...                                                 ...   \n",
       "2664  if the person placed very early into my campai...   \n",
       "2665  so general michael flynn’s life can be totally...   \n",
       "2666  my thoughts, prayers and condolences are with ...   \n",
       "2667  today’s court decision means that congress mus...   \n",
       "2668  i am pleased to announce that i intend to nomi...   \n",
       "\n",
       "                                        url_free_tweets datetime_5mins_after  \\\n",
       "0                                       thank you rand!  2017-11-28 10:55:00   \n",
       "1     join me live from fort myer in arlington, virg...  2017-08-22 09:05:00   \n",
       "2                                     thank you nicole!  2017-05-09 07:06:00   \n",
       "3     thank you to shawn steel for the nice words on...  2017-03-08 04:49:00   \n",
       "4      great night in iowa - special people. thank you!  2017-06-22 19:16:00   \n",
       "...                                                 ...                  ...   \n",
       "2664  if the person placed very early into my campai...  2018-05-23 09:18:00   \n",
       "2665  so general michael flynn’s life can be totally...  2018-04-20 18:39:00   \n",
       "2666  my thoughts, prayers and condolences are with ...  2018-04-20 06:35:00   \n",
       "2667  today’s court decision means that congress mus...  2018-04-18 05:39:00   \n",
       "2668  i am pleased to announce that i intend to nomi...  2018-03-29 05:36:00   \n",
       "\n",
       "     price_5mins_after 5mins_price_diff_abs 5mins_price_diff_perc  \\\n",
       "0           261.085000            -0.015000             -0.000057   \n",
       "1           243.670000             0.000000              0.000000   \n",
       "2           239.920000             0.045000              0.000188   \n",
       "3           236.913333             0.033333              0.000141   \n",
       "4           242.905000             0.025000              0.000103   \n",
       "...                ...                  ...                   ...   \n",
       "2664        271.100000             0.060000              0.000221   \n",
       "2665        266.866667             0.046667              0.000175   \n",
       "2666        268.715000             0.095000              0.000354   \n",
       "2667        270.600000             0.000000              0.000000   \n",
       "2668        260.997143             0.014286              0.000055   \n",
       "\n",
       "                                     prev_30mins_prices  \n",
       "0     [260.96, 261.03, 261.01, 261.015, 261.04, 261....  \n",
       "1     [243.76, 243.79, 243.85, 243.86, 243.81, 243.7...  \n",
       "2     [239.73, 239.73, 239.73, 239.73, 239.73, 239.7...  \n",
       "3     [236.84, 236.84, 236.84, 236.8533333333333, 23...  \n",
       "4     [242.85, 242.85, 242.85, 242.85, 242.85, 242.8...  \n",
       "...                                                 ...  \n",
       "2664  [271.18, 271.16, 271.18, 271.15, 271.08, 271.0...  \n",
       "2665  [266.74625000000003, 266.7475, 266.74875, 266....  \n",
       "2666  [268.77, 268.74333333333334, 268.7166666666666...  \n",
       "2667  [270.73, 270.7, 270.7275, 270.755, 270.7825, 2...  \n",
       "2668  [260.82, 260.89, 260.9, 260.87666666666667, 26...  \n",
       "\n",
       "[2660 rows x 23 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X = model_a_df.loc[:, 'cleaned_text']\n",
    "model_a_y = model_a_df.loc[:, '5mins_price_diff_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train, model_a_X_test, model_a_y_train, model_a_y_test = train_test_split(model_a_X, model_a_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_corpus_list = []\n",
    "\n",
    "for i in model_a_X_train:\n",
    "    model_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(model_a_corpus_list, min_count=1, size=100)\n",
    "model_a_pretrained_weights = word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_num_words = [len(i) for i in model_a_corpus_list]\n",
    "model_a_longest_sentence_len = max(model_a_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train_padded = sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "model_a_X_test_padded = sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_count = dict(tokenizer.word_counts)\n",
    "glove_sorted=dict(sorted(glove_word_count.items(), key=lambda x: x[1],reverse=True))\n",
    "print(glove_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in word2vec_model.wv.vocab:\n",
    "    w2c[item]=word2vec_model.wv.vocab[item].count\n",
    "w2c_sorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "print(w2c_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_count = dict(tokenizer.word_counts)\n",
    "glove_sorted=dict(sorted(glove_word_count.items(), key=lambda x: x[1],reverse=True))\n",
    "print(glove_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_w2v = list(w2c.keys())\n",
    "words_gloves = list(glove_word_count.keys())\n",
    "extra = []\\\n",
    "for word in words_w2v:\n",
    "    if word not in words_gloves:\n",
    "        extra.append(word)\n",
    "print(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "print(vocab_size_glove)\n",
    "print(embedding_size)\n",
    "print(count)\n",
    "print(skipped_words)\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a(pretrained_weights, longest_sentence_len):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = create_model_a(model_a_pretrained_weights, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 157, 100)          589300    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 157, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 157, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 591,149\n",
      "Trainable params: 1,849\n",
      "Non-trainable params: 589,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1425 samples, validate on 357 samples\n",
      "Epoch 1/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0349 - mean_absolute_error: 0.1091\n",
      "Epoch 00001: val_loss improved from inf to 0.00001, saving model to ./model_a_checkpoint/09112020 1115h.h5\n",
      "1425/1425 [==============================] - 24s 17ms/sample - loss: 0.0347 - mean_absolute_error: 0.1088 - val_loss: 1.4224e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 2/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0253 - mean_absolute_error: 0.0930\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "1425/1425 [==============================] - 18s 12ms/sample - loss: 0.0252 - mean_absolute_error: 0.0929 - val_loss: 1.5865e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 3/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0196 - mean_absolute_error: 0.0824\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./model_a_checkpoint/09112020 1115h.h5\n",
      "1425/1425 [==============================] - 19s 13ms/sample - loss: 0.0196 - mean_absolute_error: 0.0823 - val_loss: 1.3421e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 4/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0125 - mean_absolute_error: 0.0634\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "1425/1425 [==============================] - 23s 16ms/sample - loss: 0.0126 - mean_absolute_error: 0.0635 - val_loss: 7.8314e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 5/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0607\n",
      "Epoch 00005: val_loss improved from 0.00001 to 0.00000, saving model to ./model_a_checkpoint/09112020 1115h.h5\n",
      "1425/1425 [==============================] - 21s 15ms/sample - loss: 0.0113 - mean_absolute_error: 0.0605 - val_loss: 2.3902e-07 - val_mean_absolute_error: 3.8245e-04\n",
      "Epoch 6/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0532\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 21s 15ms/sample - loss: 0.0087 - mean_absolute_error: 0.0528 - val_loss: 4.8766e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 7/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0072 - mean_absolute_error: 0.0464\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 21s 15ms/sample - loss: 0.0072 - mean_absolute_error: 0.0466 - val_loss: 6.5152e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 8/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0057 - mean_absolute_error: 0.0413\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 21s 15ms/sample - loss: 0.0057 - mean_absolute_error: 0.0415 - val_loss: 5.1884e-06 - val_mean_absolute_error: 0.0022\n",
      "Epoch 9/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0046 - mean_absolute_error: 0.0385\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 26s 19ms/sample - loss: 0.0047 - mean_absolute_error: 0.0386 - val_loss: 4.1759e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 10/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0361\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 20s 14ms/sample - loss: 0.0044 - mean_absolute_error: 0.0361 - val_loss: 1.4643e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 11/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0330\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 22s 16ms/sample - loss: 0.0036 - mean_absolute_error: 0.0330 - val_loss: 2.2805e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 12/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0029 - mean_absolute_error: 0.0300\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 21s 15ms/sample - loss: 0.0029 - mean_absolute_error: 0.0299 - val_loss: 4.0757e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 13/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0031 - mean_absolute_error: 0.0295\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 21s 14ms/sample - loss: 0.0031 - mean_absolute_error: 0.0296 - val_loss: 3.5640e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 14/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0298\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 37s 26ms/sample - loss: 0.0027 - mean_absolute_error: 0.0296 - val_loss: 5.6419e-06 - val_mean_absolute_error: 0.0023\n",
      "Epoch 15/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0266\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 22s 15ms/sample - loss: 0.0020 - mean_absolute_error: 0.0264 - val_loss: 4.9803e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 16/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0253\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 22s 15ms/sample - loss: 0.0021 - mean_absolute_error: 0.0254 - val_loss: 3.0916e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 17/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0271\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 23s 16ms/sample - loss: 0.0022 - mean_absolute_error: 0.0271 - val_loss: 4.0911e-06 - val_mean_absolute_error: 0.0020\n",
      "Epoch 18/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0018 - mean_absolute_error: 0.0247\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 27s 19ms/sample - loss: 0.0018 - mean_absolute_error: 0.0248 - val_loss: 2.8331e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 19/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0018 - mean_absolute_error: 0.0241\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 22s 16ms/sample - loss: 0.0018 - mean_absolute_error: 0.0240 - val_loss: 5.4969e-06 - val_mean_absolute_error: 0.0023\n",
      "Epoch 20/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0221\n",
      "Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to ./model_a_checkpoint/09112020 1115h.h5\n",
      "1425/1425 [==============================] - 30s 21ms/sample - loss: 0.0013 - mean_absolute_error: 0.0220 - val_loss: 1.7924e-07 - val_mean_absolute_error: 2.7590e-04\n",
      "Epoch 21/50\n",
      "1408/1425 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0203\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "1425/1425 [==============================] - 24s 17ms/sample - loss: 0.0012 - mean_absolute_error: 0.0204 - val_loss: 3.8218e-07 - val_mean_absolute_error: 5.1063e-04\n",
      "Epoch 22/50\n",
      "1152/1425 [=======================>......] - ETA: 5s - loss: 0.0011 - mean_absolute_error: 0.0195"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (word vectors + price history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_df = df.dropna(subset=['cleaned_text', 'prev_30mins_prices'])\n",
    "\n",
    "model_b_X = model_b_df.loc[:, ['cleaned_text', 'prev_30mins_prices']]\n",
    "model_b_y = model_b_df.loc[:, '5mins_price_diff_perc']\n",
    "\n",
    "model_b_X_train, model_b_X_test, model_b_y_train, model_b_y_test = train_test_split(model_b_X, model_b_y, test_size=0.33, random_state=42)\n",
    "\n",
    "model_b_X_train_text = model_b_X_train.iloc[:, 0]\n",
    "model_b_X_train_price_history = model_b_X_train.iloc[:, 1]\n",
    "model_b_X_test_text = model_b_X_test.iloc[:, 0]\n",
    "model_b_X_test_price_history = model_b_X_test.iloc[:, 1]\n",
    "                           \n",
    "model_b_corpus_list = []\n",
    "\n",
    "for i in model_b_X_train_text:\n",
    "    model_b_corpus_list.append(i.split())\n",
    "    \n",
    "model_b_word2vec_model = Word2Vec(model_b_corpus_list, min_count=1, size=100)\n",
    "model_b_pretrained_weights = word2vec_model.wv.vectors\n",
    "\n",
    "model_b_num_words = [len(i) for i in model_b_corpus_list]\n",
    "model_b_longest_sentence_len = max(model_b_num_words)\n",
    "\n",
    "model_b_X_train_padded = sentence_to_indices_padded(model_b_X_train_text, model_b_longest_sentence_len)\n",
    "model_b_X_test_padded = sentence_to_indices_padded(model_b_X_test_text, model_b_longest_sentence_len)\n",
    "\n",
    "model_b_X_train_input = [model_b_X_train_padded, model_b_X_train_price_history]\n",
    "model_b_X_test_input = [model_b_X_test_padded, model_b_X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>prev_30mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>happy birthday theleegreenwoodflashbackfriday</td>\n",
       "      <td>[257.65869642857143, 257.658619047619, 257.658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>tomorrow the house votes on kateslaw amp no sa...</td>\n",
       "      <td>[243.66, 243.67, 243.7, 243.65, 243.63, 243.65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>stocks and the economy have a long way to go a...</td>\n",
       "      <td>[267.475, 267.47, 267.49, 267.49, 267.52, 267....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>a year ago the pundits amp talking heads peopl...</td>\n",
       "      <td>[279.12, 279.16, 279.09, 279.08, 279.05, 279.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>looking more amp more like the trump campaign ...</td>\n",
       "      <td>[279.3987916666666, 279.39869047619044, 279.39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>jon kyl will be an extraordinary senator repre...</td>\n",
       "      <td>[264.12, 264.15, 264.23, 264.33, 264.215000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>i am so proud of my daughter ivanka to be abus...</td>\n",
       "      <td>[257.81647058823535, 257.82058823529405, 257.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>nancy pelosi and fake tears chuck schumer held...</td>\n",
       "      <td>[227.6575, 227.66125, 227.665, 227.66875, 227....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>all will stay in mexico if for any reason it b...</td>\n",
       "      <td>[265.26937853107347, 265.27025423728816, 265.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>secshulkins decision is one of the biggest win...</td>\n",
       "      <td>[239.63089285714287, 239.63085714285717, 239.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1691 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text  \\\n",
       "286       happy birthday theleegreenwoodflashbackfriday   \n",
       "824   tomorrow the house votes on kateslaw amp no sa...   \n",
       "75    stocks and the economy have a long way to go a...   \n",
       "2133  a year ago the pundits amp talking heads peopl...   \n",
       "1947  looking more amp more like the trump campaign ...   \n",
       "...                                                 ...   \n",
       "1720  jon kyl will be an extraordinary senator repre...   \n",
       "1151  i am so proud of my daughter ivanka to be abus...   \n",
       "1186  nancy pelosi and fake tears chuck schumer held...   \n",
       "1364  all will stay in mexico if for any reason it b...   \n",
       "900   secshulkins decision is one of the biggest win...   \n",
       "\n",
       "                                     prev_30mins_prices  \n",
       "286   [257.65869642857143, 257.658619047619, 257.658...  \n",
       "824   [243.66, 243.67, 243.7, 243.65, 243.63, 243.65...  \n",
       "75    [267.475, 267.47, 267.49, 267.49, 267.52, 267....  \n",
       "2133  [279.12, 279.16, 279.09, 279.08, 279.05, 279.0...  \n",
       "1947  [279.3987916666666, 279.39869047619044, 279.39...  \n",
       "...                                                 ...  \n",
       "1720  [264.12, 264.15, 264.23, 264.33, 264.215000000...  \n",
       "1151  [257.81647058823535, 257.82058823529405, 257.8...  \n",
       "1186  [227.6575, 227.66125, 227.665, 227.66875, 227....  \n",
       "1364  [265.26937853107347, 265.27025423728816, 265.2...  \n",
       "900   [239.63089285714287, 239.63085714285717, 239.6...  \n",
       "\n",
       "[1691 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='relu')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = create_model_b(model_b_pretrained_weights, model_b_longest_sentence_len, (30,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_index_input (InputLaye [(None, 157)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 157, 100)     589300      sentence_index_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "price_history_input (InputLayer [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM1 (LSTM)             (None, 157, 4)       1680        embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM1 (LSTM)             (None, 30, 4)        96          price_history_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout1 (Dropout)       (None, 157, 4)       0           model1_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout1 (Dropout)       (None, 30, 4)        0           model2_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM2 (LSTM)             (None, 4)            144         model1_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM2 (LSTM)             (None, 4)            144         model2_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout2 (Dropout)       (None, 4)            0           model1_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout2 (Dropout)       (None, 4)            0           model2_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8)            0           model1_dropout2[0][0]            \n",
      "                                                                 model2_dropout2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 4)            36          concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 4)            0           Dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            5           dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 591,405\n",
      "Trainable params: 2,105\n",
      "Non-trainable params: 589,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1352 samples, validate on 339 samples\n",
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "1344/1352 [============================>.] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.1076\n",
      "Epoch 00001: val_loss improved from inf to 0.00004, saving model to ./model_b_checkpoint/09112020 1200h.h5\n",
      "1352/1352 [==============================] - 21s 16ms/sample - loss: 0.0297 - mean_absolute_error: 0.1078 - val_loss: 4.4453e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 2/50\n",
      "1344/1352 [============================>.] - ETA: 0s - loss: 0.0216 - mean_absolute_error: 0.0891\n",
      "Epoch 00002: val_loss improved from 0.00004 to 0.00000, saving model to ./model_b_checkpoint/09112020 1200h.h5\n",
      "1352/1352 [==============================] - 16s 12ms/sample - loss: 0.0216 - mean_absolute_error: 0.0890 - val_loss: 6.6148e-07 - val_mean_absolute_error: 7.2531e-04\n",
      "Epoch 3/50\n",
      "1344/1352 [============================>.] - ETA: 0s - loss: 0.0167 - mean_absolute_error: 0.0822\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "1352/1352 [==============================] - 18s 13ms/sample - loss: 0.0167 - mean_absolute_error: 0.0824 - val_loss: 2.7341e-06 - val_mean_absolute_error: 0.0016\n",
      "Epoch 4/50\n",
      "1344/1352 [============================>.] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0686\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "1352/1352 [==============================] - 21s 15ms/sample - loss: 0.0127 - mean_absolute_error: 0.0684 - val_loss: 4.8632e-06 - val_mean_absolute_error: 0.0022\n",
      "Epoch 5/50\n",
      "1216/1352 [=========================>....] - ETA: 2s - loss: 0.0112 - mean_absolute_error: 0.0650"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_b_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a.fit(model_b_X_train_input, model_b_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
