{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2219 entries, 0 to 2218\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      2219 non-null   float64\n",
      " 1   cleaned_text            2219 non-null   object \n",
      " 2   favorites               2219 non-null   int64  \n",
      " 3   retweets                2219 non-null   int64  \n",
      " 4   date                    2219 non-null   object \n",
      " 5   tweet_datetime          2219 non-null   object \n",
      " 6   date_part               2219 non-null   object \n",
      " 7   time_part               2219 non-null   object \n",
      " 8   hour                    2219 non-null   int64  \n",
      " 9   year                    2219 non-null   int64  \n",
      " 10  month                   2219 non-null   int64  \n",
      " 11  cleaned_text_2          2219 non-null   object \n",
      " 12  datetime_60mins_after   2219 non-null   object \n",
      " 13  price_60mins_after      2219 non-null   float64\n",
      " 14  datetime_now            2219 non-null   object \n",
      " 15  price_now               2219 non-null   float64\n",
      " 16  60mins_price_diff_abs   2219 non-null   float64\n",
      " 17  60mins_price_diff_perc  2219 non-null   float64\n",
      " 18  prev_60mins_prices      2083 non-null   object \n",
      "dtypes: float64(5), int64(5), object(9)\n",
      "memory usage: 329.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "      <th>datetime_60mins_after</th>\n",
       "      <th>price_60mins_after</th>\n",
       "      <th>datetime_now</th>\n",
       "      <th>price_now</th>\n",
       "      <th>60mins_price_diff_abs</th>\n",
       "      <th>60mins_price_diff_perc</th>\n",
       "      <th>prev_60mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand!</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>2017-11-28 11:50:00</td>\n",
       "      <td>261.485000</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>261.100000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>260.96,261.03,261.01,261.015,261.04,261.01,261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>join me live from fort myer in arlington virgi...</td>\n",
       "      <td>2017-08-22 10:00:00</td>\n",
       "      <td>244.260000</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>243.76,243.79,243.85,243.86,243.81,243.78,243....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole!</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>2017-05-09 08:01:00</td>\n",
       "      <td>239.940000</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>239.875000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>239.73,239.73,239.73,239.73,239.73,239.73,239....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "      <td>2017-03-08 05:44:00</td>\n",
       "      <td>237.022857</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>236.880000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>236.84,236.84,236.84,236.85333333333332,236.86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.787250e+17</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>134210</td>\n",
       "      <td>36346</td>\n",
       "      <td>2017-06-24 21:23:00</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>make america great again</td>\n",
       "      <td>2017-06-25 06:23:00</td>\n",
       "      <td>243.326476</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>243.320762</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>243.31790476190474,243.31799999999998,243.3180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>If the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>2018-05-23 10:13:00</td>\n",
       "      <td>271.930000</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>271.040000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>271.18,271.16,271.18,271.15,271.08,271.07,271....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>So General Michael Flynn’s life can be totally...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>2018-04-20 19:34:00</td>\n",
       "      <td>267.025000</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>266.820000</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>266.74625000000003,266.7475,266.74875,266.75,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>My thoughts, prayers and condolences are with ...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>2018-04-20 07:30:00</td>\n",
       "      <td>269.070000</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>268.620000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>268.77,268.74333333333334,268.71666666666664,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>Today’s Court decision means that Congress mus...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>2018-04-18 06:34:00</td>\n",
       "      <td>270.695000</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>270.73,270.7,270.72749999999996,270.755,270.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>I am pleased to announce that I intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>2018-03-29 06:31:00</td>\n",
       "      <td>261.170000</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>260.982857</td>\n",
       "      <td>0.187143</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>260.82,260.89,260.9,260.87666666666667,260.853...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2219 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.353400e+17                                   Thank you Rand!    \n",
       "1     8.997980e+17  Join me live from Fort Myer in Arlington, Virg...   \n",
       "2     8.939700e+17                                 Thank you Nicole!    \n",
       "3     8.819770e+17  Thank you to Shawn Steel for the nice words on...   \n",
       "4     8.787250e+17                          MAKE AMERICA GREAT AGAIN!   \n",
       "...            ...                                                ...   \n",
       "2214  9.990960e+17  If the person placed very early into my campai...   \n",
       "2215  9.874600e+17  So General Michael Flynn’s life can be totally...   \n",
       "2216  9.870960e+17  My thoughts, prayers and condolences are with ...   \n",
       "2217  9.863570e+17  Today’s Court decision means that Congress mus...   \n",
       "2218  9.791090e+17  I am pleased to announce that I intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4        134210     36346  2017-06-24 21:23:00  2017-06-25 05:23:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2214      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2215      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2216      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2217      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2218      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   \n",
       "1     2017-08-22  09:00:00     9  2017      8   \n",
       "2     2017-05-09  07:01:00     7  2017      5   \n",
       "3     2017-03-08  04:44:00     4  2017      3   \n",
       "4     2017-06-25  05:23:00     5  2017      6   \n",
       "...          ...       ...   ...   ...    ...   \n",
       "2214  2018-05-23  09:13:00     9  2018      5   \n",
       "2215  2018-04-20  18:34:00    18  2018      4   \n",
       "2216  2018-04-20  06:30:00     6  2018      4   \n",
       "2217  2018-04-18  05:34:00     5  2018      4   \n",
       "2218  2018-03-29  05:31:00     5  2018      3   \n",
       "\n",
       "                                         cleaned_text_2 datetime_60mins_after  \\\n",
       "0                                       thank you rand    2017-11-28 11:50:00   \n",
       "1     join me live from fort myer in arlington virgi...   2017-08-22 10:00:00   \n",
       "2                                     thank you nicole    2017-05-09 08:01:00   \n",
       "3       thank you to shawn steel for the nice words on    2017-03-08 05:44:00   \n",
       "4                              make america great again   2017-06-25 06:23:00   \n",
       "...                                                 ...                   ...   \n",
       "2214  if the person placed very early into my campai...   2018-05-23 10:13:00   \n",
       "2215  so general michael flynn’s life can be totally...   2018-04-20 19:34:00   \n",
       "2216  my thoughts prayers and condolences are with t...   2018-04-20 07:30:00   \n",
       "2217  today’s court decision means that congress mus...   2018-04-18 06:34:00   \n",
       "2218  i am pleased to announce that i intend to nomi...   2018-03-29 06:31:00   \n",
       "\n",
       "      price_60mins_after         datetime_now   price_now  \\\n",
       "0             261.485000  2017-11-28 10:50:00  261.100000   \n",
       "1             244.260000  2017-08-22 09:00:00  243.670000   \n",
       "2             239.940000  2017-05-09 07:01:00  239.875000   \n",
       "3             237.022857  2017-03-08 04:44:00  236.880000   \n",
       "4             243.326476  2017-06-25 05:23:00  243.320762   \n",
       "...                  ...                  ...         ...   \n",
       "2214          271.930000  2018-05-23 09:13:00  271.040000   \n",
       "2215          267.025000  2018-04-20 18:34:00  266.820000   \n",
       "2216          269.070000  2018-04-20 06:30:00  268.620000   \n",
       "2217          270.695000  2018-04-18 05:34:00  270.600000   \n",
       "2218          261.170000  2018-03-29 05:31:00  260.982857   \n",
       "\n",
       "      60mins_price_diff_abs  60mins_price_diff_perc  \\\n",
       "0                  0.385000                0.001475   \n",
       "1                  0.590000                0.002421   \n",
       "2                  0.065000                0.000271   \n",
       "3                  0.142857                0.000603   \n",
       "4                  0.005714                0.000023   \n",
       "...                     ...                     ...   \n",
       "2214               0.890000                0.003284   \n",
       "2215               0.205000                0.000768   \n",
       "2216               0.450000                0.001675   \n",
       "2217               0.095000                0.000351   \n",
       "2218               0.187143                0.000717   \n",
       "\n",
       "                                     prev_60mins_prices  \n",
       "0     260.96,261.03,261.01,261.015,261.04,261.01,261...  \n",
       "1     243.76,243.79,243.85,243.86,243.81,243.78,243....  \n",
       "2     239.73,239.73,239.73,239.73,239.73,239.73,239....  \n",
       "3     236.84,236.84,236.84,236.85333333333332,236.86...  \n",
       "4     243.31790476190474,243.31799999999998,243.3180...  \n",
       "...                                                 ...  \n",
       "2214  271.18,271.16,271.18,271.15,271.08,271.07,271....  \n",
       "2215  266.74625000000003,266.7475,266.74875,266.75,2...  \n",
       "2216  268.77,268.74333333333334,268.71666666666664,2...  \n",
       "2217  270.73,270.7,270.72749999999996,270.755,270.78...  \n",
       "2218  260.82,260.89,260.9,260.87666666666667,260.853...  \n",
       "\n",
       "[2219 rows x 19 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X = model_a_df.loc[:, 'cleaned_text_2']\n",
    "model_a_y = model_a_df.loc[:, '60mins_price_diff_abs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train, model_a_X_test, model_a_y_train, model_a_y_test = train_test_split(model_a_X, model_a_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_corpus_list = []\n",
    "\n",
    "for i in model_a_X_train:\n",
    "    model_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(model_a_corpus_list, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_num_words = [len(i) for i in model_a_corpus_list]\n",
    "model_a_longest_sentence_len = max(model_a_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in model_a_word2vec_model.wv.vocab:\n",
    "                indices.append(model_a_word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train_padded = sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "model_a_X_test_padded = sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.100d.txt', encoding='utf8')\n",
    "glove_vocab = []\n",
    "glove_vocab_index = {}\n",
    "count = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    glove_vocab.append(word)\n",
    "    glove_vocab_index[word] = count\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    count += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1359, 62, 21, 1121, 62, 13, 1, 2550, 25, 1, 1360, 4, 28, 375, 120, 7, 75, 75, 75, 37, 19]\n",
      "Found 5169 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(model_a_X_train)\n",
    "sequences = tokenizer.texts_to_sequences(model_a_X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(sequences[1])\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    global glove_vocab\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in glove_vocab:\n",
    "                indices.append(glove_vocab_index[word])\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_glove = glove_sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "x_test_glove = glove_sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5170\n",
      "100\n",
      "652\n",
      "['don’t', 'didn’t', '“the', 'it’s', 'doesn’t', 'can’t', 'trump”', 'america’s', 'he’s', 'strzok', 'kavanaugh', 'wasn’t', '“trump', 'that’s', 'couldn’t', '“president', 'americafirst', 'states”', 'aren’t', 'today’s', 'jongun', 'i’m', '“it', 'isn’t', 'shouldn’t', 'i’ve', '“this', 'trump’s', 'rosendale', 'balderson', 'magarally', 'denuclearization', 'we’re', 'jobsnotmobs', 'prstrong', 'wouldn’t', '“you', 'lawabiding', 'they’re', 'we’ve', 'maralago', '“a', 'taxreform', '“collusion”', 'you’re', 'cryin’', 'weren’t', 'councel', 'they’ve', '“i', 'uswomensopen', 'times”', 'happyindependenceday', 'workers”', '“sources”', '“fbi', 'stop”', 'there’s', 'flynn’s', 'he’ll', '“it’s', '“when', 'country’s', 'haven’t', 'hurricanemichael', 'potusabroad', '“these', 'clinton’s', '“they', '“consumer', 'singaporesummit', 'here’s', '“presidential', 'hillary’s', 'sampp', '“o”', 'won’t', 'r’s', 'sheriff’s', '“thank', 'blakeman', 'america”', 'votekarenhandel', '“there', 'obama’s', 'party’s', 'covfefe', 'usmca', '“justice”', 'gillum', 'brennan’s', 'people”', '“special”', 'something’s', 'antitrump', 'world’s', 'indopacific', 'scalise', '“bruce', 'ohr’s', 'i’ll', '“what', 'correspondents’', 'expropriations', '“south', 'farmers”', 'nation’s', 'daines', 'counterintelligence', '“anonymous', 'sources”', 'news”', 'cajunnavy', 'wilsond', 'coleading', 'secretarygeneral', '“make', 'again”', 'militaryveterans', 'upfake', 'antius', 'magajobsnotmobs', 'buildthewall', 'worldautismawarenessday', 'liub', 'woolsey', '“pledge', '“during', 'foundation”', 'priding', 'raffensperger', '“press”', 'puertoricowith', 'badmouths', 'harm’s', 'east”', 'securitya', 'democrats’', 'authorizescont', 'unverifiable”', 'strassel', 'there”', 'carafano', 'blagojevich', 'comeys', 'worker”', 'coauthor', '“trumponomics”', '“best', 'gains”', 'nonwalled', 'greenvillespartanburg', 'campaign”', '“please', 'war”', 'meet”', 'maria”', 'killed”', 'onceinageneration', 'grandstander', 'storyopinion', 'hurricaneflorence', 'books”', '“judge', 'court”', '“private', 'adp”', 'extinguishers”', 'wstate', 'agentlover', 'scjustice', 'reassigned”', 'ocarenightmare', 'youlesm', '“greenbrier', 'dinner”', 'yesterday’s', '“big', 'one”', 'hardearned', 'career”', 'shamrockbowl', 'varadkar', 'house”', 'roomtoday', 'changedcomplete', 'kate’s', 'againpotusabroad', 'repealandreplace', 'jobcreating', 'storm’s', 'tillerson', 'exposeda', 'days”', 'economy”', '“bet', 'force”', 'gentiloni', '”this', 'criminals”', 'stopthebias', 'progrowth', 'riyadhsummit', 'atampt', 'vamissionact', 'government’s', 'loveragent', '“overseas”', 'shuette', '“his', 'cubanamerican', 'flagstand', '“senator”', 'sobbingly', 'warfighters', 'tradecraft', '“mentally', 'retarded”', 'southerner”', 'opportunity”', 'rothfus', 'republicansgot', 'teamusaon', 'exfbi', '“purge”', 'firstrespondersday', 'suv’s', 'presidency”', 'citiesboth', 'recordhigh', 'billionyear', 'happenalso', 'houstonstrong', 'poroshenko', 'before”', 'bordersthey', 'makeamericagreatagain', 'liyuan', '“mr', 'candidates”', 'fandos', 'highand', 'amvets', 'prohibitions', 'lawenforcementappreciationday', 'senate’s', 'nationsusaatungaunga', '“resist”', '“obstruct”', '“forever”', 'voteralphnorman', 'vfwconvention', 'thingsbut', 'cubavideo', 'tarkanian', 'mexico’s', 'issawith', 'vetshas', 'northamwho', 'virginiais', 'wiedefeld', 'todayhappythanksgiving', '“had', 'securitycharles', 'mcculloughfmr', 'livescharlottesville', 'outand', 'cubamemorandum', 'opioidepidemicfull', 'violationall', 'crimesbecause', 'sessions”', 'parentteacher', 'else”', 'country”', '“trump’s', 'speech”', 'trumptime', '“no', 'steele”', 'bordersthe', 'ourselvesnot', 'jurists', 'bastilleday', 'hunt”', 'august”', '“paper', 'trail”', '“private”', 'winour', '“media”', 'commanderinchiefs', 'writein', 'kustoff', 'lowerpriced', 'hispanicamericans', 'hispanicamerican', '“we’re', 'great”', 'you’ve', 'abcwashington', '“ship', 'fools”', 'dodgersred', 'shellacked', 'congressionalbaseballgame', 'cutreform', 'otherwiseand', 'ussfitzgerald', 'mindtruly', 'ndsen', 'unredacted', 'yearmost', 'we’d', 'rescissions', '“infestation”', 'elelments', 'leakin’', 'wife’s', 'georgiabad', 'jobkilling', 'partnersduring', 'prince’s', 'jobsborder', 'securityisis', '“manufacturing', 'survey”', 'profitsthere', 'watch”', 'morici', 'happyhanukkah', 'specificbut', 'ukraineanother', '“unprecedented', 'rise”', '“more', 'gop”', 'distroys', 'crimes”', '“hillary', 'digenova', 'kateslaw', 'me”', 'bookbring', 'justin’s', 'countrystandforouranthem', '“acid', 'washed”', '“pastor', 'problack', 'event”', 'state”', 'lavar’s', 'man’s', 'trishregan', '“did', 'weaponizing', 'gain”', '“china', 'server”', '“director', 'oppositedisgraceful', 'trumpstyle', 'ussjohnsmccain', 'aideit', 'campaignpeople', 'spyingthis', 'scandalous”', 'peaceofficersmemorialday', 'andpoliceweek', 'is”', '“ohr', '“media', '“fake', 'commanderinchief', '“obama', 'judges”', '“independent', 'judiciary”', 'magarallyin', 'countries’', 'racefake', 'heavytruck', 'nationfull', 'usss', 'ossoff', 'jeanclaude', 'trumppence', '“boom', 'high”', 'trouncingthe', 'magatickets', '“john', 'this”', '“spygate', 'china’s', 'tarrifs', 'barriersalso', '“liars', 'conspiracy”', 'russiatrump', 'pelosi’s', 'hearby', 'rv’s', 'president”', 'shepherding', 'generalsecretary', 'wins”', 'code”', 'credit”', 'staminai', '“department', 'considerations”', '“other', 'side”', 'vasecretary', 'shulkin', 'europe’s', 'unverifiable', 'trumprussia', 'convictions”', 'tigersfull', 'hurricaneirma', 'hillarythey', 'cutsmakeamericagreatagain', 'ingloriously', 'relationshipbut', 'importantbut', '“out', 'mind”', 'antisecond', 'yearfour', 'thatstory', '“martin', 'act”', 'redesignates', '“why', 'offensesconstitution', 'thisstormtrooper', 'almost”', '“heart”', 'goodlatte', 'infrastructureweek', 'wdiverse', 'playbookcall', 'excoriated', 'lawthe', 'investigators”', 'astonishingthat', '“how', 'place”', '“richard', 'hero”', 'americaexecutive', 'hillaryclinton', 'justicedepartment', 'terroristalbaghdaditheir', 'jobsand', 'commanderinchief’s', '“if', 'scam”', 'whope', 'exonerating', '“there’s', 'weissman', 'terrorismrelated', 'foreignborn', 'taxcutsandjobsact', 'placesspent', 'differentthat’s', 'elected”', 'colmery', 'greatgrandchildren', 'satisfiedtruly', 'lifecont', 'texasstrong', '‘caravan’', 'placethe', 'lifemerry', 'extortionist', 'hacking”', 'centuryofservice', 'ncaachampionsphotos', 'jfkfiles', 'shulkin’s', 'happened”', 'hydesmith', 'wowthe', 'possiblefar', '“a”and', 'disgracedraintheswamp', 'destinybut', 'armynavygame', '“unsolved', 'mystery”', 'autismawarenessday', 'lightitupblue', 'wyou', 'year’s', 'presidentelect', 'comey’s', 'collusion”', 'trumponomics', 'longheld', 'wellwishers', 'anthemrespect', 'murl', 'workforceweek', '“tariffed”', 'yougod', 'year”', 'showdavid', 'congratspeggy', 'alabamathat', 'countrybipartisan', 'godblesstheusa', 'american’s', 'gratefullawenforcementappreciationdaypresident', 'happened”“how', 'russia”', 'annie’s', 'congratulationshave', 'officer”', 'fbi’s', 'strzokpage', '“nuclear', 'ohio’s', 'opioidepidemic', 'againamericafirst', 'poll”', 'giaritelli', 'ussarizona', 'honorthemremarks', 'goal”rob', 'goldmanvice', 'ripevelyn', 'incrediblethanks', 'frankenstien', 'presidential”', 'autoworkers', 'admonished', '“peanuts”', 'ribbonno', 'thisobviously', 'rexnord', 'boomingbut', 'mulvaney', 'couched', 'journalism”', 'she’s', '“barrack', 'what’s', 'dream”', 'memembers', 'governorsi', 'gps”', '“fakers”', 'ssteel', '“leadership”', 'u”', 'melania’s', 'zinke', 'alike“remember', 'harbor”', 'circuit”', 'haspel', '“papers”', 'unelectable', '“mike”', '“toast”', 'teachers”', 'opioidcrisis', '“national', 'soon”', 'lineitem', 'staes', 'republicansremember', 'membersnot', 'magarallytonight', 'gobbler’s', 'hcare', 'marineone', 'shinzō', '“us', 'insulting”', '“will', 'around”', 'koreanationalassembly', 'potusinasia', 'cantbut', 'canamp', 'teamscalise', 'aftermaththe', 'dossierthe', 'breach”', 'medalofhonor', 'slabinski', 'overcomeeven', '“secretary', 'allegations”', 'viewingreporting', 'republicanconservative', 'canley', '“we’ll', 'who’s', '“i’d', 'gaetz', 'uskorea', 'interdictact', '“flood', 'gates”', 'let’s', 'democratled', '“comedian”', 'linesmuch', 'builthas', 'mansioncompound', 'rightthe', 'mistake”', 'countrytell', '“border', 'wall”', 'democrat’s', 'stateno', 'weeklyaddress', 'commmitment', 'nationsmet', 'discgrace', '“an', 'outrageous”', 'draintheswamp', '“former', 'dossier”', 'wilkesbarre', '“releasing”', 'situationwill', 'xinping', 'lesm', '“russians', 'disgraceand', 'magarallyreplay', '“capitalist', 'comeback”', 'nonmonetary', '“some', 'known”', 'andeavor', 'mattarella', 'russiarussia', '“mainstream', 'depression”', 'cutsreform']\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "        \n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "\n",
    "# print(vocab_size_glove)\n",
    "# print(embedding_size_glove)\n",
    "# print(count)\n",
    "# print(skipped_words)\n",
    "\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=model_a_longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = create_model_a(model_a_pretrained_weights, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "model_a.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 98, 100)           516900    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 98, 4)             1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 98, 4)             0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 518,749\n",
      "Trainable params: 1,849\n",
      "Non-trainable params: 516,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1188 samples, validate on 298 samples\n",
      "Epoch 1/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 1.0518 - mae: 0.9247\n",
      "Epoch 00001: val_loss improved from inf to 0.82128, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 4s 3ms/sample - loss: 1.0504 - mae: 0.9243 - val_loss: 0.8213 - val_mae: 0.8292\n",
      "Epoch 2/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.7666\n",
      "Epoch 00002: val_loss improved from 0.82128 to 0.63401, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 609us/sample - loss: 0.7625 - mae: 0.7665 - val_loss: 0.6340 - val_mae: 0.7103\n",
      "Epoch 3/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.6199 - mae: 0.6730\n",
      "Epoch 00003: val_loss improved from 0.63401 to 0.50482, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 575us/sample - loss: 0.6201 - mae: 0.6734 - val_loss: 0.5048 - val_mae: 0.6144\n",
      "Epoch 4/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.5129 - mae: 0.5875\n",
      "Epoch 00004: val_loss improved from 0.50482 to 0.41384, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 577us/sample - loss: 0.5123 - mae: 0.5875 - val_loss: 0.4138 - val_mae: 0.5366\n",
      "Epoch 5/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.4296 - mae: 0.5235\n",
      "Epoch 00005: val_loss improved from 0.41384 to 0.34803, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 582us/sample - loss: 0.4288 - mae: 0.5232 - val_loss: 0.3480 - val_mae: 0.4726\n",
      "Epoch 6/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.3755 - mae: 0.4710\n",
      "Epoch 00006: val_loss improved from 0.34803 to 0.30071, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 582us/sample - loss: 0.3769 - mae: 0.4709 - val_loss: 0.3007 - val_mae: 0.4207\n",
      "Epoch 7/50\n",
      "1120/1188 [===========================>..] - ETA: 0s - loss: 0.3353 - mae: 0.4261\n",
      "Epoch 00007: val_loss improved from 0.30071 to 0.26501, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 582us/sample - loss: 0.3335 - mae: 0.4265 - val_loss: 0.2650 - val_mae: 0.3764\n",
      "Epoch 8/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.3007 - mae: 0.3897\n",
      "Epoch 00008: val_loss improved from 0.26501 to 0.23883, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 595us/sample - loss: 0.3053 - mae: 0.3907 - val_loss: 0.2388 - val_mae: 0.3396\n",
      "Epoch 9/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2792 - mae: 0.3618\n",
      "Epoch 00009: val_loss improved from 0.23883 to 0.21936, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 577us/sample - loss: 0.2789 - mae: 0.3619 - val_loss: 0.2194 - val_mae: 0.3090\n",
      "Epoch 10/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2656 - mae: 0.3487\n",
      "Epoch 00010: val_loss improved from 0.21936 to 0.20462, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 590us/sample - loss: 0.2693 - mae: 0.3508 - val_loss: 0.2046 - val_mae: 0.2845\n",
      "Epoch 11/50\n",
      "1120/1188 [===========================>..] - ETA: 0s - loss: 0.2616 - mae: 0.3351\n",
      "Epoch 00011: val_loss improved from 0.20462 to 0.19415, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 572us/sample - loss: 0.2561 - mae: 0.3334 - val_loss: 0.1941 - val_mae: 0.2657\n",
      "Epoch 12/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2432 - mae: 0.3121\n",
      "Epoch 00012: val_loss improved from 0.19415 to 0.18590, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 578us/sample - loss: 0.2435 - mae: 0.3120 - val_loss: 0.1859 - val_mae: 0.2497\n",
      "Epoch 13/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2403 - mae: 0.3091\n",
      "Epoch 00013: val_loss improved from 0.18590 to 0.17996, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 568us/sample - loss: 0.2421 - mae: 0.3091 - val_loss: 0.1800 - val_mae: 0.2372\n",
      "Epoch 14/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2247 - mae: 0.2953\n",
      "Epoch 00014: val_loss improved from 0.17996 to 0.17542, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 581us/sample - loss: 0.2267 - mae: 0.2958 - val_loss: 0.1754 - val_mae: 0.2269\n",
      "Epoch 15/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2332 - mae: 0.2932\n",
      "Epoch 00015: val_loss improved from 0.17542 to 0.17186, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 561us/sample - loss: 0.2326 - mae: 0.2928 - val_loss: 0.1719 - val_mae: 0.2179\n",
      "Epoch 16/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2253 - mae: 0.2800\n",
      "Epoch 00016: val_loss improved from 0.17186 to 0.16927, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 567us/sample - loss: 0.2246 - mae: 0.2796 - val_loss: 0.1693 - val_mae: 0.2109\n",
      "Epoch 17/50\n",
      "1088/1188 [==========================>...] - ETA: 0s - loss: 0.2211 - mae: 0.2767\n",
      "Epoch 00017: val_loss improved from 0.16927 to 0.16703, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 583us/sample - loss: 0.2150 - mae: 0.2759 - val_loss: 0.1670 - val_mae: 0.2047\n",
      "Epoch 18/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2240 - mae: 0.2770\n",
      "Epoch 00018: val_loss improved from 0.16703 to 0.16580, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 564us/sample - loss: 0.2235 - mae: 0.2769 - val_loss: 0.1658 - val_mae: 0.2010\n",
      "Epoch 19/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2151 - mae: 0.2718\n",
      "Epoch 00019: val_loss improved from 0.16580 to 0.16474, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 615us/sample - loss: 0.2144 - mae: 0.2721 - val_loss: 0.1647 - val_mae: 0.1975\n",
      "Epoch 20/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2192 - mae: 0.2750\n",
      "Epoch 00020: val_loss improved from 0.16474 to 0.16388, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 573us/sample - loss: 0.2204 - mae: 0.2760 - val_loss: 0.1639 - val_mae: 0.1946\n",
      "Epoch 21/50\n",
      "1120/1188 [===========================>..] - ETA: 0s - loss: 0.2196 - mae: 0.2624\n",
      "Epoch 00021: val_loss improved from 0.16388 to 0.16317, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 573us/sample - loss: 0.2149 - mae: 0.2630 - val_loss: 0.1632 - val_mae: 0.1921\n",
      "Epoch 22/50\n",
      "1120/1188 [===========================>..] - ETA: 0s - loss: 0.2156 - mae: 0.2634\n",
      "Epoch 00022: val_loss improved from 0.16317 to 0.16261, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 583us/sample - loss: 0.2160 - mae: 0.2661 - val_loss: 0.1626 - val_mae: 0.1900\n",
      "Epoch 23/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2162 - mae: 0.2605\n",
      "Epoch 00023: val_loss improved from 0.16261 to 0.16210, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 554us/sample - loss: 0.2160 - mae: 0.2614 - val_loss: 0.1621 - val_mae: 0.1879\n",
      "Epoch 24/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2206 - mae: 0.2649\n",
      "Epoch 00024: val_loss improved from 0.16210 to 0.16190, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 573us/sample - loss: 0.2201 - mae: 0.2645 - val_loss: 0.1619 - val_mae: 0.1870\n",
      "Epoch 25/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2144 - mae: 0.2624\n",
      "Epoch 00025: val_loss improved from 0.16190 to 0.16172, saving model to ./model_a_checkpoint/11112020 1140h.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188/1188 [==============================] - 1s 568us/sample - loss: 0.2142 - mae: 0.2626 - val_loss: 0.1617 - val_mae: 0.1863\n",
      "Epoch 26/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2144 - mae: 0.2661\n",
      "Epoch 00026: val_loss improved from 0.16172 to 0.16153, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 569us/sample - loss: 0.2148 - mae: 0.2668 - val_loss: 0.1615 - val_mae: 0.1854\n",
      "Epoch 27/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2203 - mae: 0.2664\n",
      "Epoch 00027: val_loss improved from 0.16153 to 0.16146, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 569us/sample - loss: 0.2197 - mae: 0.2659 - val_loss: 0.1615 - val_mae: 0.1850\n",
      "Epoch 28/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2167 - mae: 0.2631\n",
      "Epoch 00028: val_loss improved from 0.16146 to 0.16137, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 580us/sample - loss: 0.2136 - mae: 0.2626 - val_loss: 0.1614 - val_mae: 0.1846\n",
      "Epoch 29/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2089 - mae: 0.2600\n",
      "Epoch 00029: val_loss improved from 0.16137 to 0.16131, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 562us/sample - loss: 0.2108 - mae: 0.2594 - val_loss: 0.1613 - val_mae: 0.1844\n",
      "Epoch 30/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2179 - mae: 0.2613\n",
      "Epoch 00030: val_loss improved from 0.16131 to 0.16125, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 584us/sample - loss: 0.2203 - mae: 0.2618 - val_loss: 0.1612 - val_mae: 0.1841\n",
      "Epoch 31/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2055 - mae: 0.2576\n",
      "Epoch 00031: val_loss improved from 0.16125 to 0.16116, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 583us/sample - loss: 0.2104 - mae: 0.2569 - val_loss: 0.1612 - val_mae: 0.1837\n",
      "Epoch 32/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2037 - mae: 0.255 - ETA: 0s - loss: 0.2134 - mae: 0.2590\n",
      "Epoch 00032: val_loss improved from 0.16116 to 0.16112, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 584us/sample - loss: 0.2150 - mae: 0.2600 - val_loss: 0.1611 - val_mae: 0.1835\n",
      "Epoch 33/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2182 - mae: 0.2602\n",
      "Epoch 00033: val_loss improved from 0.16112 to 0.16105, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 562us/sample - loss: 0.2178 - mae: 0.2600 - val_loss: 0.1611 - val_mae: 0.1832\n",
      "Epoch 34/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2151 - mae: 0.2555\n",
      "Epoch 00034: val_loss improved from 0.16105 to 0.16101, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 568us/sample - loss: 0.2145 - mae: 0.2551 - val_loss: 0.1610 - val_mae: 0.1831\n",
      "Epoch 35/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2161 - mae: 0.2624\n",
      "Epoch 00035: val_loss improved from 0.16101 to 0.16100, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 592us/sample - loss: 0.2154 - mae: 0.2618 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 36/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2184 - mae: 0.2567\n",
      "Epoch 00036: val_loss improved from 0.16100 to 0.16099, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 557us/sample - loss: 0.2144 - mae: 0.2555 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 37/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2169 - mae: 0.2596\n",
      "Epoch 00037: val_loss improved from 0.16099 to 0.16098, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 562us/sample - loss: 0.2163 - mae: 0.2591 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 38/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2181 - mae: 0.2609\n",
      "Epoch 00038: val_loss improved from 0.16098 to 0.16098, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 579us/sample - loss: 0.2172 - mae: 0.2615 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 39/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2109 - mae: 0.2572\n",
      "Epoch 00039: val_loss improved from 0.16098 to 0.16097, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 561us/sample - loss: 0.2103 - mae: 0.2568 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 40/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2109 - mae: 0.2550\n",
      "Epoch 00040: val_loss improved from 0.16097 to 0.16096, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 582us/sample - loss: 0.2102 - mae: 0.2544 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 41/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2157 - mae: 0.2651\n",
      "Epoch 00041: val_loss improved from 0.16096 to 0.16096, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 594us/sample - loss: 0.2150 - mae: 0.2645 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 42/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2152 - mae: 0.2614\n",
      "Epoch 00042: val_loss improved from 0.16096 to 0.16095, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 588us/sample - loss: 0.2138 - mae: 0.2627 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 43/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2173 - mae: 0.2592\n",
      "Epoch 00043: val_loss did not improve from 0.16095\n",
      "1188/1188 [==============================] - 1s 559us/sample - loss: 0.2171 - mae: 0.2593 - val_loss: 0.1610 - val_mae: 0.1830\n",
      "Epoch 44/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2114 - mae: 0.2548\n",
      "Epoch 00044: val_loss improved from 0.16095 to 0.16093, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 563us/sample - loss: 0.2171 - mae: 0.2574 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 45/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2175 - mae: 0.2615\n",
      "Epoch 00045: val_loss improved from 0.16093 to 0.16092, saving model to ./model_a_checkpoint/11112020 1140h.h5\n",
      "1188/1188 [==============================] - 1s 570us/sample - loss: 0.2139 - mae: 0.2597 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 46/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2264 - mae: 0.2648\n",
      "Epoch 00046: val_loss did not improve from 0.16092\n",
      "1188/1188 [==============================] - 1s 558us/sample - loss: 0.2267 - mae: 0.2653 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 47/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2205 - mae: 0.2642\n",
      "Epoch 00047: val_loss did not improve from 0.16092\n",
      "1188/1188 [==============================] - 1s 524us/sample - loss: 0.2197 - mae: 0.2635 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 48/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2169 - mae: 0.2603\n",
      "Epoch 00048: val_loss did not improve from 0.16092\n",
      "1188/1188 [==============================] - 1s 537us/sample - loss: 0.2165 - mae: 0.2602 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 49/50\n",
      "1152/1188 [============================>.] - ETA: 0s - loss: 0.2169 - mae: 0.2584\n",
      "Epoch 00049: val_loss did not improve from 0.16092\n",
      "1188/1188 [==============================] - 1s 547us/sample - loss: 0.2126 - mae: 0.2561 - val_loss: 0.1609 - val_mae: 0.1830\n",
      "Epoch 50/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.2133 - mae: 0.2558\n",
      "Epoch 00050: val_loss did not improve from 0.16092\n",
      "1188/1188 [==============================] - 1s 532us/sample - loss: 0.2127 - mae: 0.2555 - val_loss: 0.1609 - val_mae: 0.1830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19ce46df828>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_a.predict(model_a_X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00430753]\n",
      " [0.00430682]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430703]\n",
      " [0.00430688]\n",
      " [0.00430682]\n",
      " [0.0043068 ]\n",
      " [0.00430676]\n",
      " [0.00430684]\n",
      " [0.00430749]\n",
      " [0.00430711]\n",
      " [0.00430698]\n",
      " [0.00430806]\n",
      " [0.00430738]\n",
      " [0.00430707]\n",
      " [0.0043068 ]\n",
      " [0.00430689]\n",
      " [0.0043078 ]\n",
      " [0.00430712]\n",
      " [0.00430689]\n",
      " [0.00430676]\n",
      " [0.00430685]\n",
      " [0.00430692]\n",
      " [0.0043074 ]\n",
      " [0.0043071 ]\n",
      " [0.00430682]\n",
      " [0.00430679]\n",
      " [0.00430686]\n",
      " [0.00430685]\n",
      " [0.00430676]\n",
      " [0.00431033]\n",
      " [0.00430703]\n",
      " [0.00430686]\n",
      " [0.00430876]\n",
      " [0.00430732]\n",
      " [0.00430677]\n",
      " [0.00430846]\n",
      " [0.00430772]\n",
      " [0.00430689]\n",
      " [0.00430686]\n",
      " [0.00430788]\n",
      " [0.0043069 ]\n",
      " [0.00430665]\n",
      " [0.00430679]\n",
      " [0.00430771]\n",
      " [0.0043069 ]\n",
      " [0.00430724]\n",
      " [0.0043068 ]\n",
      " [0.0043077 ]\n",
      " [0.00430685]\n",
      " [0.00430682]\n",
      " [0.00430686]\n",
      " [0.00430727]\n",
      " [0.00430772]\n",
      " [0.00430677]\n",
      " [0.00430697]\n",
      " [0.00430789]\n",
      " [0.0043068 ]\n",
      " [0.00430671]\n",
      " [0.0043068 ]\n",
      " [0.00430686]\n",
      " [0.00430742]\n",
      " [0.00430718]\n",
      " [0.00430679]\n",
      " [0.00431788]\n",
      " [0.00430686]\n",
      " [0.0043077 ]\n",
      " [0.00430712]\n",
      " [0.00430688]\n",
      " [0.00430684]\n",
      " [0.0043081 ]\n",
      " [0.00430698]\n",
      " [0.00430681]\n",
      " [0.00430747]\n",
      " [0.00430698]\n",
      " [0.00430688]\n",
      " [0.00430791]\n",
      " [0.00430685]\n",
      " [0.00430688]\n",
      " [0.00430684]\n",
      " [0.00430683]\n",
      " [0.0043068 ]\n",
      " [0.0043068 ]\n",
      " [0.00430679]\n",
      " [0.00430683]\n",
      " [0.00430683]\n",
      " [0.00430685]\n",
      " [0.00430683]\n",
      " [0.00430674]\n",
      " [0.0043068 ]\n",
      " [0.0043068 ]\n",
      " [0.00430741]\n",
      " [0.00430677]\n",
      " [0.00430674]\n",
      " [0.00430673]\n",
      " [0.00430686]\n",
      " [0.00430679]\n",
      " [0.00430772]\n",
      " [0.00430695]\n",
      " [0.00430683]\n",
      " [0.00430683]\n",
      " [0.00430743]\n",
      " [0.00430683]\n",
      " [0.00430728]\n",
      " [0.00430719]\n",
      " [0.00430687]\n",
      " [0.00430683]\n",
      " [0.00430685]\n",
      " [0.00430684]\n",
      " [0.00430688]\n",
      " [0.00430837]\n",
      " [0.00430677]\n",
      " [0.00430684]\n",
      " [0.00430689]\n",
      " [0.00430688]\n",
      " [0.0043069 ]\n",
      " [0.00430687]\n",
      " [0.00430745]\n",
      " [0.0043073 ]\n",
      " [0.00430688]\n",
      " [0.00430679]\n",
      " [0.00430732]\n",
      " [0.00430701]\n",
      " [0.00431635]\n",
      " [0.00430686]\n",
      " [0.00430681]\n",
      " [0.00430699]\n",
      " [0.00430685]\n",
      " [0.00430684]\n",
      " [0.0043068 ]\n",
      " [0.00430714]\n",
      " [0.00430669]\n",
      " [0.00430698]\n",
      " [0.0043068 ]\n",
      " [0.00430768]\n",
      " [0.00430673]\n",
      " [0.00430681]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430679]\n",
      " [0.00430677]\n",
      " [0.00430781]\n",
      " [0.00430706]\n",
      " [0.00430676]\n",
      " [0.00430758]\n",
      " [0.00432811]\n",
      " [0.00430673]\n",
      " [0.00430679]\n",
      " [0.00430677]\n",
      " [0.00430782]\n",
      " [0.00430702]\n",
      " [0.00430688]\n",
      " [0.0043072 ]\n",
      " [0.00430708]\n",
      " [0.00430684]\n",
      " [0.00430677]\n",
      " [0.00430684]\n",
      " [0.00430683]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.00430685]\n",
      " [0.00430756]\n",
      " [0.00430689]\n",
      " [0.00430742]\n",
      " [0.00430697]\n",
      " [0.00430683]\n",
      " [0.00430683]\n",
      " [0.00431033]\n",
      " [0.00430683]\n",
      " [0.00430684]\n",
      " [0.00430689]\n",
      " [0.0043068 ]\n",
      " [0.00430688]\n",
      " [0.00430688]\n",
      " [0.00430724]\n",
      " [0.004307  ]\n",
      " [0.00430683]\n",
      " [0.00430698]\n",
      " [0.00430684]\n",
      " [0.00430687]\n",
      " [0.00430679]\n",
      " [0.00430682]\n",
      " [0.00430688]\n",
      " [0.0043072 ]\n",
      " [0.00430687]\n",
      " [0.00430691]\n",
      " [0.00430677]\n",
      " [0.00430677]\n",
      " [0.00430736]\n",
      " [0.00430708]\n",
      " [0.00430729]\n",
      " [0.00430678]\n",
      " [0.00430681]\n",
      " [0.00430679]\n",
      " [0.00430717]\n",
      " [0.00430682]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430715]\n",
      " [0.00430685]\n",
      " [0.00430684]\n",
      " [0.0043068 ]\n",
      " [0.00430668]\n",
      " [0.00430679]\n",
      " [0.00430709]\n",
      " [0.00430677]\n",
      " [0.00430698]\n",
      " [0.00430684]\n",
      " [0.00430683]\n",
      " [0.0043068 ]\n",
      " [0.0043068 ]\n",
      " [0.00430685]\n",
      " [0.00430758]\n",
      " [0.00430675]\n",
      " [0.00430686]\n",
      " [0.00430677]\n",
      " [0.00430788]\n",
      " [0.00430681]\n",
      " [0.00430702]\n",
      " [0.00430679]\n",
      " [0.00430723]\n",
      " [0.00430669]\n",
      " [0.00430711]\n",
      " [0.00430688]\n",
      " [0.00430681]\n",
      " [0.00430705]\n",
      " [0.00430677]\n",
      " [0.00430702]\n",
      " [0.00430682]\n",
      " [0.00430677]\n",
      " [0.00430681]\n",
      " [0.00430726]\n",
      " [0.00430747]\n",
      " [0.00430781]\n",
      " [0.00430679]\n",
      " [0.00430766]\n",
      " [0.00430682]\n",
      " [0.00430685]\n",
      " [0.00430685]\n",
      " [0.00430728]\n",
      " [0.00430674]\n",
      " [0.00430739]\n",
      " [0.00430736]\n",
      " [0.00430689]\n",
      " [0.00430689]\n",
      " [0.004307  ]\n",
      " [0.00430685]\n",
      " [0.00430727]\n",
      " [0.00430761]\n",
      " [0.0043071 ]\n",
      " [0.00430687]\n",
      " [0.0043071 ]\n",
      " [0.00430682]\n",
      " [0.00430796]\n",
      " [0.00430686]\n",
      " [0.00430677]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430682]\n",
      " [0.00430676]\n",
      " [0.00430705]\n",
      " [0.00430677]\n",
      " [0.00430782]\n",
      " [0.00430687]\n",
      " [0.0043068 ]\n",
      " [0.00430683]\n",
      " [0.00430686]\n",
      " [0.00430745]\n",
      " [0.0043068 ]\n",
      " [0.00430684]\n",
      " [0.00430687]\n",
      " [0.00430689]\n",
      " [0.00430681]\n",
      " [0.00430685]\n",
      " [0.00430679]\n",
      " [0.00430683]\n",
      " [0.00430682]\n",
      " [0.00430738]\n",
      " [0.00430737]\n",
      " [0.00430683]\n",
      " [0.00430679]\n",
      " [0.00430682]\n",
      " [0.00430677]\n",
      " [0.00430685]\n",
      " [0.00430695]\n",
      " [0.00430683]\n",
      " [0.0043073 ]\n",
      " [0.00430683]\n",
      " [0.00430685]\n",
      " [0.00430683]\n",
      " [0.00430684]\n",
      " [0.0043069 ]\n",
      " [0.00430683]\n",
      " [0.00430689]\n",
      " [0.00430682]\n",
      " [0.0043068 ]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.0043068 ]\n",
      " [0.0043069 ]\n",
      " [0.00430725]\n",
      " [0.0043068 ]\n",
      " [0.00430701]\n",
      " [0.00430682]\n",
      " [0.0043068 ]\n",
      " [0.00430701]\n",
      " [0.00430686]\n",
      " [0.00430685]\n",
      " [0.00430679]\n",
      " [0.00430773]\n",
      " [0.00430684]\n",
      " [0.00430679]\n",
      " [0.00430682]\n",
      " [0.00430676]\n",
      " [0.00430682]\n",
      " [0.00430695]\n",
      " [0.0043068 ]\n",
      " [0.00430693]\n",
      " [0.00430684]\n",
      " [0.00430752]\n",
      " [0.00430665]\n",
      " [0.00430681]\n",
      " [0.00430679]\n",
      " [0.00430676]\n",
      " [0.0043072 ]\n",
      " [0.00430683]\n",
      " [0.00430682]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.00430731]\n",
      " [0.00430684]\n",
      " [0.00430683]\n",
      " [0.00430704]\n",
      " [0.00430713]\n",
      " [0.00430685]\n",
      " [0.00431099]\n",
      " [0.00430677]\n",
      " [0.00430697]\n",
      " [0.00430673]\n",
      " [0.00430677]\n",
      " [0.00430684]\n",
      " [0.00430697]\n",
      " [0.00430714]\n",
      " [0.00430687]\n",
      " [0.00430682]\n",
      " [0.00430687]\n",
      " [0.00430702]\n",
      " [0.00430771]\n",
      " [0.00430683]\n",
      " [0.00430759]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.00430686]\n",
      " [0.00432004]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430749]\n",
      " [0.00430712]\n",
      " [0.004307  ]\n",
      " [0.00430709]\n",
      " [0.00430683]\n",
      " [0.00430711]\n",
      " [0.00430683]\n",
      " [0.00430688]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.00430684]\n",
      " [0.00430731]\n",
      " [0.00430718]\n",
      " [0.00430701]\n",
      " [0.00430674]\n",
      " [0.00430683]\n",
      " [0.0043069 ]\n",
      " [0.00430708]\n",
      " [0.00430682]\n",
      " [0.00430685]\n",
      " [0.00430687]\n",
      " [0.00430684]\n",
      " [0.00430683]\n",
      " [0.00430684]\n",
      " [0.00430711]\n",
      " [0.00430679]\n",
      " [0.00430689]\n",
      " [0.00430743]\n",
      " [0.00430682]\n",
      " [0.00430688]\n",
      " [0.00430718]\n",
      " [0.00430767]\n",
      " [0.0043068 ]\n",
      " [0.00430676]\n",
      " [0.00430734]\n",
      " [0.0043074 ]\n",
      " [0.00430743]\n",
      " [0.00430679]\n",
      " [0.00430749]\n",
      " [0.00430674]\n",
      " [0.00430677]\n",
      " [0.00430686]\n",
      " [0.00430686]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430708]\n",
      " [0.00430684]\n",
      " [0.00430685]\n",
      " [0.00430707]\n",
      " [0.00430691]\n",
      " [0.00430679]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430685]\n",
      " [0.00430694]\n",
      " [0.00430687]\n",
      " [0.0043068 ]\n",
      " [0.00430704]\n",
      " [0.00430686]\n",
      " [0.00430679]\n",
      " [0.00430683]\n",
      " [0.0043068 ]\n",
      " [0.00430679]\n",
      " [0.00430746]\n",
      " [0.00430686]\n",
      " [0.00430701]\n",
      " [0.0043069 ]\n",
      " [0.00430691]\n",
      " [0.00430735]\n",
      " [0.00430848]\n",
      " [0.00430723]\n",
      " [0.00430677]\n",
      " [0.00430717]\n",
      " [0.00430757]\n",
      " [0.00430682]\n",
      " [0.00430708]\n",
      " [0.00430704]\n",
      " [0.00430681]\n",
      " [0.00430684]\n",
      " [0.00430714]\n",
      " [0.00430728]\n",
      " [0.00430682]\n",
      " [0.00430693]\n",
      " [0.00430701]\n",
      " [0.00430685]\n",
      " [0.00430683]\n",
      " [0.00430685]\n",
      " [0.00430792]\n",
      " [0.00430676]\n",
      " [0.00430687]\n",
      " [0.00430683]\n",
      " [0.00430765]\n",
      " [0.0043068 ]\n",
      " [0.00430683]\n",
      " [0.00430686]\n",
      " [0.00430712]\n",
      " [0.00430742]\n",
      " [0.0043068 ]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430677]\n",
      " [0.00430684]\n",
      " [0.00430684]\n",
      " [0.00430681]\n",
      " [0.00430698]\n",
      " [0.00430734]\n",
      " [0.00430788]\n",
      " [0.00430729]\n",
      " [0.00430773]\n",
      " [0.00430687]\n",
      " [0.00430679]\n",
      " [0.00430701]\n",
      " [0.0043078 ]\n",
      " [0.00430682]\n",
      " [0.00430714]\n",
      " [0.00430677]\n",
      " [0.0043068 ]\n",
      " [0.0043071 ]\n",
      " [0.00430778]\n",
      " [0.004307  ]\n",
      " [0.00430683]\n",
      " [0.0043076 ]\n",
      " [0.00430813]\n",
      " [0.00430692]\n",
      " [0.00430693]\n",
      " [0.0043068 ]\n",
      " [0.00430689]\n",
      " [0.00430704]\n",
      " [0.00430677]\n",
      " [0.00430683]\n",
      " [0.004308  ]\n",
      " [0.00430681]\n",
      " [0.00430682]\n",
      " [0.00430724]\n",
      " [0.00430684]\n",
      " [0.00430689]\n",
      " [0.00430686]\n",
      " [0.00430681]\n",
      " [0.00430678]\n",
      " [0.00430682]\n",
      " [0.00430681]\n",
      " [0.00430679]\n",
      " [0.00430707]\n",
      " [0.00430688]\n",
      " [0.00430716]\n",
      " [0.00430745]\n",
      " [0.00430777]\n",
      " [0.0043075 ]\n",
      " [0.00430684]\n",
      " [0.00430681]\n",
      " [0.00430681]\n",
      " [0.00430691]\n",
      " [0.00430682]\n",
      " [0.00430704]\n",
      " [0.00430682]\n",
      " [0.00430694]\n",
      " [0.00430682]\n",
      " [0.00430684]\n",
      " [0.00430689]\n",
      " [0.00430689]\n",
      " [0.00430679]\n",
      " [0.00430677]\n",
      " [0.00430713]\n",
      " [0.00430863]\n",
      " [0.00430767]\n",
      " [0.00430701]\n",
      " [0.00430684]\n",
      " [0.00430674]\n",
      " [0.00430679]\n",
      " [0.00430679]\n",
      " [0.00430679]\n",
      " [0.00430681]\n",
      " [0.00430721]\n",
      " [0.00430679]\n",
      " [0.00369498]\n",
      " [0.00430682]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430679]\n",
      " [0.00430679]\n",
      " [0.00430793]\n",
      " [0.00430687]\n",
      " [0.00430683]\n",
      " [0.00430833]\n",
      " [0.00430732]\n",
      " [0.00430688]\n",
      " [0.00430712]\n",
      " [0.00430676]\n",
      " [0.00430679]\n",
      " [0.00430706]\n",
      " [0.00430681]\n",
      " [0.00430778]\n",
      " [0.0043068 ]\n",
      " [0.0043068 ]\n",
      " [0.00430684]\n",
      " [0.00430682]\n",
      " [0.00430679]\n",
      " [0.0043068 ]\n",
      " [0.0043068 ]\n",
      " [0.00430701]\n",
      " [0.00430694]\n",
      " [0.00430713]\n",
      " [0.00430805]\n",
      " [0.00430681]\n",
      " [0.00430683]\n",
      " [0.00430679]\n",
      " [0.004307  ]\n",
      " [0.00430685]\n",
      " [0.00430682]\n",
      " [0.00430721]\n",
      " [0.00430682]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.0043068 ]\n",
      " [0.00430677]\n",
      " [0.00430723]\n",
      " [0.00430682]\n",
      " [0.00430693]\n",
      " [0.00430702]\n",
      " [0.00430721]\n",
      " [0.00430677]\n",
      " [0.00430688]\n",
      " [0.00430684]\n",
      " [0.00430684]\n",
      " [0.00430687]\n",
      " [0.00430685]\n",
      " [0.00430727]\n",
      " [0.00430702]\n",
      " [0.00430682]\n",
      " [0.00430748]\n",
      " [0.00430679]\n",
      " [0.00430714]\n",
      " [0.00430794]\n",
      " [0.00430685]\n",
      " [0.00430686]\n",
      " [0.00430683]\n",
      " [0.00430694]\n",
      " [0.00430682]\n",
      " [0.00430686]\n",
      " [0.00430679]\n",
      " [0.00430682]\n",
      " [0.00430709]\n",
      " [0.00430682]\n",
      " [0.00430678]\n",
      " [0.00430684]\n",
      " [0.00430685]\n",
      " [0.00430694]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430732]\n",
      " [0.00430684]\n",
      " [0.00430679]\n",
      " [0.00430684]\n",
      " [0.00430682]\n",
      " [0.00430674]\n",
      " [0.00430767]\n",
      " [0.00430686]\n",
      " [0.00430688]\n",
      " [0.00430683]\n",
      " [0.00430675]\n",
      " [0.00430677]\n",
      " [0.00430723]\n",
      " [0.00430691]\n",
      " [0.00430693]\n",
      " [0.00430677]\n",
      " [0.00430681]\n",
      " [0.0043068 ]\n",
      " [0.00430683]\n",
      " [0.0043069 ]\n",
      " [0.00430716]\n",
      " [0.00430682]\n",
      " [0.00430688]\n",
      " [0.00430679]\n",
      " [0.00430677]\n",
      " [0.00430709]\n",
      " [0.00430688]\n",
      " [0.00430706]\n",
      " [0.00430683]\n",
      " [0.00430684]\n",
      " [0.00430721]\n",
      " [0.00430704]\n",
      " [0.00430679]\n",
      " [0.00430687]\n",
      " [0.00430707]\n",
      " [0.00430686]\n",
      " [0.00430859]\n",
      " [0.00430836]\n",
      " [0.00430682]\n",
      " [0.00430679]\n",
      " [0.00430675]\n",
      " [0.00431853]\n",
      " [0.00430766]\n",
      " [0.00430679]\n",
      " [0.00430679]\n",
      " [0.00430683]\n",
      " [0.00430758]\n",
      " [0.00430681]\n",
      " [0.00430681]\n",
      " [0.00430771]\n",
      " [0.00430688]\n",
      " [0.00430689]\n",
      " [0.0043068 ]\n",
      " [0.00430769]\n",
      " [0.00430686]\n",
      " [0.00430676]\n",
      " [0.00430683]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430734]\n",
      " [0.00430666]\n",
      " [0.0043068 ]\n",
      " [0.0043079 ]\n",
      " [0.00430685]\n",
      " [0.00430677]\n",
      " [0.00430677]\n",
      " [0.00430679]\n",
      " [0.00430692]\n",
      " [0.00430682]\n",
      " [0.0043068 ]\n",
      " [0.00430683]\n",
      " [0.00430685]\n",
      " [0.00430698]\n",
      " [0.00430793]\n",
      " [0.00430677]\n",
      " [0.00430713]\n",
      " [0.00430746]\n",
      " [0.00430709]\n",
      " [0.00430683]\n",
      " [0.00430707]\n",
      " [0.00430677]\n",
      " [0.00430676]\n",
      " [0.00430686]\n",
      " [0.00430687]\n",
      " [0.00430684]\n",
      " [0.00430681]\n",
      " [0.00430678]\n",
      " [0.00430679]\n",
      " [0.00430686]\n",
      " [0.00430678]\n",
      " [0.00430677]\n",
      " [0.00430681]\n",
      " [0.00430676]\n",
      " [0.00430678]\n",
      " [0.0043073 ]\n",
      " [0.00430682]\n",
      " [0.00430682]\n",
      " [0.00430743]\n",
      " [0.00430679]\n",
      " [0.00430676]\n",
      " [0.0043068 ]\n",
      " [0.00430678]\n",
      " [0.00430677]\n",
      " [0.00430682]\n",
      " [0.00430721]\n",
      " [0.00430679]\n",
      " [0.00430689]\n",
      " [0.00430682]\n",
      " [0.00430682]\n",
      " [0.00430683]\n",
      " [0.00430683]\n",
      " [0.00430677]\n",
      " [0.00430847]\n",
      " [0.00430724]\n",
      " [0.00430708]\n",
      " [0.0043068 ]\n",
      " [0.00430717]\n",
      " [0.00430689]\n",
      " [0.00430682]\n",
      " [0.00430696]\n",
      " [0.00430776]\n",
      " [0.00430683]\n",
      " [0.00430736]\n",
      " [0.00430682]\n",
      " [0.00430706]\n",
      " [0.00430677]\n",
      " [0.00430678]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_loaded = create_model_a(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "model_a_loaded.load_weights('model_a_checkpoint/11112020 1140h.h5')\n",
    "model_a_loaded.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00238514]\n",
      " [0.00235754]\n",
      " [0.00235534]\n",
      " [0.0023582 ]\n",
      " [0.00237024]\n",
      " [0.00235906]\n",
      " [0.0023573 ]\n",
      " [0.00235647]\n",
      " [0.00235504]\n",
      " [0.00235772]\n",
      " [0.00240183]\n",
      " [0.00237271]\n",
      " [0.00236416]\n",
      " [0.00240976]\n",
      " [0.00238383]\n",
      " [0.00236809]\n",
      " [0.00235581]\n",
      " [0.00236115]\n",
      " [0.00240907]\n",
      " [0.00236887]\n",
      " [0.00235909]\n",
      " [0.0023551 ]\n",
      " [0.00235808]\n",
      " [0.00236249]\n",
      " [0.00238639]\n",
      " [0.00237387]\n",
      " [0.00235665]\n",
      " [0.00235575]\n",
      " [0.00235915]\n",
      " [0.00235868]\n",
      " [0.00235504]\n",
      " [0.00250107]\n",
      " [0.00236648]\n",
      " [0.00235772]\n",
      " [0.00243449]\n",
      " [0.00237662]\n",
      " [0.00235575]\n",
      " [0.00245643]\n",
      " [0.00241041]\n",
      " [0.00235873]\n",
      " [0.00235885]\n",
      " [0.00244528]\n",
      " [0.00236535]\n",
      " [0.00235504]\n",
      " [0.00235546]\n",
      " [0.00240242]\n",
      " [0.00235993]\n",
      " [0.00238812]\n",
      " [0.00235653]\n",
      " [0.00239646]\n",
      " [0.00235909]\n",
      " [0.00235623]\n",
      " [0.00235865]\n",
      " [0.00237709]\n",
      " [0.00244355]\n",
      " [0.00235537]\n",
      " [0.0023616 ]\n",
      " [0.00239721]\n",
      " [0.00235665]\n",
      " [0.00235498]\n",
      " [0.00235584]\n",
      " [0.00235865]\n",
      " [0.00239766]\n",
      " [0.00237283]\n",
      " [0.00235543]\n",
      " [0.00700068]\n",
      " [0.00235841]\n",
      " [0.00239605]\n",
      " [0.00237286]\n",
      " [0.00235865]\n",
      " [0.00235814]\n",
      " [0.00241321]\n",
      " [0.00236273]\n",
      " [0.00235605]\n",
      " [0.00238729]\n",
      " [0.0023655 ]\n",
      " [0.00236008]\n",
      " [0.00241393]\n",
      " [0.00235748]\n",
      " [0.0023585 ]\n",
      " [0.00235689]\n",
      " [0.00235665]\n",
      " [0.00235748]\n",
      " [0.00235617]\n",
      " [0.00235596]\n",
      " [0.00235689]\n",
      " [0.0023579 ]\n",
      " [0.00235862]\n",
      " [0.00235805]\n",
      " [0.00235513]\n",
      " [0.00235617]\n",
      " [0.0023559 ]\n",
      " [0.00238544]\n",
      " [0.00235534]\n",
      " [0.00235513]\n",
      " [0.00235504]\n",
      " [0.0023593 ]\n",
      " [0.00235602]\n",
      " [0.00241357]\n",
      " [0.00236386]\n",
      " [0.00235891]\n",
      " [0.00235784]\n",
      " [0.00237885]\n",
      " [0.00235814]\n",
      " [0.00237989]\n",
      " [0.00237185]\n",
      " [0.00235856]\n",
      " [0.00235838]\n",
      " [0.00235718]\n",
      " [0.00235718]\n",
      " [0.00235885]\n",
      " [0.00242978]\n",
      " [0.00235534]\n",
      " [0.00235951]\n",
      " [0.00236064]\n",
      " [0.00235909]\n",
      " [0.00235918]\n",
      " [0.00235885]\n",
      " [0.0023883 ]\n",
      " [0.00238031]\n",
      " [0.0023585 ]\n",
      " [0.00235534]\n",
      " [0.00238967]\n",
      " [0.00236386]\n",
      " [0.0054464 ]\n",
      " [0.00235838]\n",
      " [0.00235629]\n",
      " [0.00236359]\n",
      " [0.00235885]\n",
      " [0.00235742]\n",
      " [0.00235558]\n",
      " [0.00236925]\n",
      " [0.00235495]\n",
      " [0.0023638 ]\n",
      " [0.00235665]\n",
      " [0.00239551]\n",
      " [0.00235534]\n",
      " [0.00235742]\n",
      " [0.00235543]\n",
      " [0.0023576 ]\n",
      " [0.00235522]\n",
      " [0.00235546]\n",
      " [0.00239831]\n",
      " [0.00237045]\n",
      " [0.00235522]\n",
      " [0.00238702]\n",
      " [0.00687653]\n",
      " [0.0023551 ]\n",
      " [0.00235599]\n",
      " [0.00235528]\n",
      " [0.00242224]\n",
      " [0.00237051]\n",
      " [0.00235951]\n",
      " [0.00237679]\n",
      " [0.00236696]\n",
      " [0.00235879]\n",
      " [0.00235558]\n",
      " [0.00235695]\n",
      " [0.00235802]\n",
      " [0.00235516]\n",
      " [0.00235951]\n",
      " [0.00235784]\n",
      " [0.00239757]\n",
      " [0.00236267]\n",
      " [0.00239176]\n",
      " [0.00236586]\n",
      " [0.00235754]\n",
      " [0.00235713]\n",
      " [0.00250107]\n",
      " [0.00235748]\n",
      " [0.00235814]\n",
      " [0.00236195]\n",
      " [0.00235629]\n",
      " [0.00235906]\n",
      " [0.00235891]\n",
      " [0.00238386]\n",
      " [0.0023666 ]\n",
      " [0.00235739]\n",
      " [0.00236183]\n",
      " [0.00235692]\n",
      " [0.00235885]\n",
      " [0.00235543]\n",
      " [0.00235641]\n",
      " [0.00236034]\n",
      " [0.00237745]\n",
      " [0.00235873]\n",
      " [0.00236112]\n",
      " [0.00235558]\n",
      " [0.00235537]\n",
      " [0.00238371]\n",
      " [0.00237274]\n",
      " [0.00237858]\n",
      " [0.00235513]\n",
      " [0.00235784]\n",
      " [0.00235546]\n",
      " [0.00237268]\n",
      " [0.00235677]\n",
      " [0.00235704]\n",
      " [0.00235873]\n",
      " [0.00237945]\n",
      " [0.00235891]\n",
      " [0.00235742]\n",
      " [0.00235629]\n",
      " [0.00235504]\n",
      " [0.00235558]\n",
      " [0.00237131]\n",
      " [0.00235513]\n",
      " [0.00236607]\n",
      " [0.00235745]\n",
      " [0.00235739]\n",
      " [0.00235602]\n",
      " [0.00235718]\n",
      " [0.00235796]\n",
      " [0.00239444]\n",
      " [0.00235507]\n",
      " [0.0023582 ]\n",
      " [0.00235537]\n",
      " [0.00240669]\n",
      " [0.00235692]\n",
      " [0.00237042]\n",
      " [0.00235635]\n",
      " [0.00237781]\n",
      " [0.00235504]\n",
      " [0.00238639]\n",
      " [0.00235903]\n",
      " [0.00235596]\n",
      " [0.00236747]\n",
      " [0.00235516]\n",
      " [0.00236961]\n",
      " [0.00235784]\n",
      " [0.00235534]\n",
      " [0.00235713]\n",
      " [0.00237352]\n",
      " [0.00239739]\n",
      " [0.00240159]\n",
      " [0.00235569]\n",
      " [0.00240409]\n",
      " [0.00235635]\n",
      " [0.0023576 ]\n",
      " [0.00235772]\n",
      " [0.00237811]\n",
      " [0.00235513]\n",
      " [0.00238076]\n",
      " [0.00238332]\n",
      " [0.00236148]\n",
      " [0.00236052]\n",
      " [0.00236517]\n",
      " [0.00235969]\n",
      " [0.00238228]\n",
      " [0.00238568]\n",
      " [0.00237286]\n",
      " [0.00236112]\n",
      " [0.00237465]\n",
      " [0.00235683]\n",
      " [0.00242007]\n",
      " [0.00235838]\n",
      " [0.00235531]\n",
      " [0.00235543]\n",
      " [0.00235826]\n",
      " [0.00235802]\n",
      " [0.00235498]\n",
      " [0.00236696]\n",
      " [0.00235534]\n",
      " [0.00240409]\n",
      " [0.00235897]\n",
      " [0.00235617]\n",
      " [0.00235739]\n",
      " [0.00235823]\n",
      " [0.0024001 ]\n",
      " [0.00235575]\n",
      " [0.00235695]\n",
      " [0.00235891]\n",
      " [0.00235921]\n",
      " [0.00235635]\n",
      " [0.00235778]\n",
      " [0.00235558]\n",
      " [0.00235778]\n",
      " [0.00235701]\n",
      " [0.00238633]\n",
      " [0.00238925]\n",
      " [0.0023579 ]\n",
      " [0.0023554 ]\n",
      " [0.00235605]\n",
      " [0.00235528]\n",
      " [0.00235802]\n",
      " [0.00236416]\n",
      " [0.00235906]\n",
      " [0.00237897]\n",
      " [0.00235692]\n",
      " [0.00235778]\n",
      " [0.00235647]\n",
      " [0.00235832]\n",
      " [0.00235975]\n",
      " [0.00235707]\n",
      " [0.00235873]\n",
      " [0.00235844]\n",
      " [0.00235602]\n",
      " [0.00235534]\n",
      " [0.00235897]\n",
      " [0.00235713]\n",
      " [0.00236118]\n",
      " [0.00238261]\n",
      " [0.00235629]\n",
      " [0.0023675 ]\n",
      " [0.00235647]\n",
      " [0.00235617]\n",
      " [0.00236762]\n",
      " [0.00235873]\n",
      " [0.00235757]\n",
      " [0.00235534]\n",
      " [0.00240433]\n",
      " [0.00235707]\n",
      " [0.00235569]\n",
      " [0.00235814]\n",
      " [0.00235513]\n",
      " [0.00235641]\n",
      " [0.00236344]\n",
      " [0.00235558]\n",
      " [0.00236058]\n",
      " [0.00235766]\n",
      " [0.00238371]\n",
      " [0.00235504]\n",
      " [0.00235552]\n",
      " [0.00235546]\n",
      " [0.00235558]\n",
      " [0.00238091]\n",
      " [0.00235713]\n",
      " [0.00235653]\n",
      " [0.00235519]\n",
      " [0.00235987]\n",
      " [0.00238425]\n",
      " [0.00235742]\n",
      " [0.00235778]\n",
      " [0.00236601]\n",
      " [0.00236812]\n",
      " [0.00235796]\n",
      " [0.00251395]\n",
      " [0.00235516]\n",
      " [0.00236252]\n",
      " [0.00235504]\n",
      " [0.00235516]\n",
      " [0.00235739]\n",
      " [0.00236326]\n",
      " [0.00237298]\n",
      " [0.00235915]\n",
      " [0.00235745]\n",
      " [0.00235951]\n",
      " [0.00236583]\n",
      " [0.00238678]\n",
      " [0.00235772]\n",
      " [0.00239736]\n",
      " [0.00235516]\n",
      " [0.00235903]\n",
      " [0.00235778]\n",
      " [0.00342301]\n",
      " [0.00235641]\n",
      " [0.00235772]\n",
      " [0.00239894]\n",
      " [0.00237292]\n",
      " [0.00236589]\n",
      " [0.00236785]\n",
      " [0.00235742]\n",
      " [0.00238016]\n",
      " [0.00235739]\n",
      " [0.00235951]\n",
      " [0.00235558]\n",
      " [0.00235778]\n",
      " [0.00235742]\n",
      " [0.00237912]\n",
      " [0.00237682]\n",
      " [0.00236285]\n",
      " [0.00235513]\n",
      " [0.00235915]\n",
      " [0.00236136]\n",
      " [0.00236636]\n",
      " [0.00235635]\n",
      " [0.00235897]\n",
      " [0.0023607 ]\n",
      " [0.00235766]\n",
      " [0.00235981]\n",
      " [0.00235841]\n",
      " [0.00237644]\n",
      " [0.00235564]\n",
      " [0.00236005]\n",
      " [0.00238159]\n",
      " [0.00235695]\n",
      " [0.00235993]\n",
      " [0.0023827 ]\n",
      " [0.00240022]\n",
      " [0.00235555]\n",
      " [0.00235555]\n",
      " [0.00237679]\n",
      " [0.00238353]\n",
      " [0.00238445]\n",
      " [0.00235617]\n",
      " [0.00239205]\n",
      " [0.0023551 ]\n",
      " [0.00235528]\n",
      " [0.00235903]\n",
      " [0.00235838]\n",
      " [0.00235668]\n",
      " [0.00235808]\n",
      " [0.00236559]\n",
      " [0.00235718]\n",
      " [0.0023576 ]\n",
      " [0.00236857]\n",
      " [0.002361  ]\n",
      " [0.00235543]\n",
      " [0.00235534]\n",
      " [0.00235796]\n",
      " [0.00235841]\n",
      " [0.00235993]\n",
      " [0.00235906]\n",
      " [0.00235534]\n",
      " [0.00236893]\n",
      " [0.00235909]\n",
      " [0.00235522]\n",
      " [0.00235608]\n",
      " [0.00235617]\n",
      " [0.00235575]\n",
      " [0.00238639]\n",
      " [0.00235873]\n",
      " [0.00237107]\n",
      " [0.00236183]\n",
      " [0.00236052]\n",
      " [0.00238574]\n",
      " [0.00241446]\n",
      " [0.00237748]\n",
      " [0.00235513]\n",
      " [0.00238812]\n",
      " [0.00239801]\n",
      " [0.00235659]\n",
      " [0.0023759 ]\n",
      " [0.00236553]\n",
      " [0.00235695]\n",
      " [0.00235742]\n",
      " [0.00236973]\n",
      " [0.00237775]\n",
      " [0.00235647]\n",
      " [0.00236216]\n",
      " [0.0023669 ]\n",
      " [0.0023576 ]\n",
      " [0.0023568 ]\n",
      " [0.00235754]\n",
      " [0.00240403]\n",
      " [0.00235507]\n",
      " [0.00236005]\n",
      " [0.0023576 ]\n",
      " [0.00239065]\n",
      " [0.00235635]\n",
      " [0.00235677]\n",
      " [0.00235915]\n",
      " [0.00237221]\n",
      " [0.00237733]\n",
      " [0.00235707]\n",
      " [0.00235623]\n",
      " [0.00235742]\n",
      " [0.00235513]\n",
      " [0.00235707]\n",
      " [0.00235879]\n",
      " [0.00235671]\n",
      " [0.00236186]\n",
      " [0.00237867]\n",
      " [0.00239933]\n",
      " [0.00240028]\n",
      " [0.0023962 ]\n",
      " [0.00235844]\n",
      " [0.00235546]\n",
      " [0.00236517]\n",
      " [0.00241151]\n",
      " [0.00235748]\n",
      " [0.00237012]\n",
      " [0.00235528]\n",
      " [0.00235617]\n",
      " [0.00236896]\n",
      " [0.00240055]\n",
      " [0.00236714]\n",
      " [0.00235698]\n",
      " [0.00243217]\n",
      " [0.0024465 ]\n",
      " [0.00235993]\n",
      " [0.00236106]\n",
      " [0.00235647]\n",
      " [0.00236207]\n",
      " [0.00237185]\n",
      " [0.0023554 ]\n",
      " [0.00235772]\n",
      " [0.00247291]\n",
      " [0.00235569]\n",
      " [0.00235617]\n",
      " [0.00237864]\n",
      " [0.00235742]\n",
      " [0.00235975]\n",
      " [0.00235796]\n",
      " [0.00235707]\n",
      " [0.00235581]\n",
      " [0.00235617]\n",
      " [0.00235638]\n",
      " [0.00235605]\n",
      " [0.00238749]\n",
      " [0.00235885]\n",
      " [0.00237742]\n",
      " [0.00238481]\n",
      " [0.00239581]\n",
      " [0.00239137]\n",
      " [0.00235885]\n",
      " [0.00235659]\n",
      " [0.00235677]\n",
      " [0.00235957]\n",
      " [0.00235704]\n",
      " [0.00236952]\n",
      " [0.00235713]\n",
      " [0.00236034]\n",
      " [0.00235614]\n",
      " [0.00235885]\n",
      " [0.00236017]\n",
      " [0.00235987]\n",
      " [0.00235516]\n",
      " [0.00235564]\n",
      " [0.00237119]\n",
      " [0.00243366]\n",
      " [0.00240654]\n",
      " [0.00236583]\n",
      " [0.00235739]\n",
      " [0.00235507]\n",
      " [0.00235522]\n",
      " [0.00235552]\n",
      " [0.00235668]\n",
      " [0.00235546]\n",
      " [0.00237513]\n",
      " [0.00235569]\n",
      " [0.02983713]\n",
      " [0.00235629]\n",
      " [0.0023554 ]\n",
      " [0.00235862]\n",
      " [0.00235569]\n",
      " [0.00235519]\n",
      " [0.00241339]\n",
      " [0.00235784]\n",
      " [0.00235754]\n",
      " [0.00241357]\n",
      " [0.00238419]\n",
      " [0.00236014]\n",
      " [0.00237757]\n",
      " [0.00235513]\n",
      " [0.00235558]\n",
      " [0.00236708]\n",
      " [0.00235653]\n",
      " [0.00239581]\n",
      " [0.00235608]\n",
      " [0.00235569]\n",
      " [0.00235739]\n",
      " [0.00235635]\n",
      " [0.0023554 ]\n",
      " [0.00235558]\n",
      " [0.0023559 ]\n",
      " [0.00236619]\n",
      " [0.00236106]\n",
      " [0.00237185]\n",
      " [0.00241411]\n",
      " [0.00235581]\n",
      " [0.00235826]\n",
      " [0.00235647]\n",
      " [0.00236541]\n",
      " [0.0023576 ]\n",
      " [0.00235656]\n",
      " [0.0023717 ]\n",
      " [0.00235707]\n",
      " [0.00235778]\n",
      " [0.00235748]\n",
      " [0.00235602]\n",
      " [0.00235558]\n",
      " [0.00237763]\n",
      " [0.00235659]\n",
      " [0.0023613 ]\n",
      " [0.00236359]\n",
      " [0.00238585]\n",
      " [0.0023554 ]\n",
      " [0.00235885]\n",
      " [0.00235832]\n",
      " [0.00235727]\n",
      " [0.00235832]\n",
      " [0.00235799]\n",
      " [0.00237924]\n",
      " [0.00236741]\n",
      " [0.00235683]\n",
      " [0.00238809]\n",
      " [0.00235543]\n",
      " [0.00237375]\n",
      " [0.00242859]\n",
      " [0.00235957]\n",
      " [0.00235823]\n",
      " [0.00235704]\n",
      " [0.0023613 ]\n",
      " [0.00235674]\n",
      " [0.00235856]\n",
      " [0.00235558]\n",
      " [0.00235704]\n",
      " [0.00237411]\n",
      " [0.00235695]\n",
      " [0.0023554 ]\n",
      " [0.00235718]\n",
      " [0.0023582 ]\n",
      " [0.00236225]\n",
      " [0.00235522]\n",
      " [0.00235862]\n",
      " [0.00237566]\n",
      " [0.00235796]\n",
      " [0.00235552]\n",
      " [0.00235701]\n",
      " [0.00235796]\n",
      " [0.00235537]\n",
      " [0.00239325]\n",
      " [0.00236019]\n",
      " [0.00235832]\n",
      " [0.00235686]\n",
      " [0.00235519]\n",
      " [0.00235537]\n",
      " [0.002372  ]\n",
      " [0.00235969]\n",
      " [0.00236017]\n",
      " [0.00235528]\n",
      " [0.00235656]\n",
      " [0.00235766]\n",
      " [0.00235805]\n",
      " [0.00235987]\n",
      " [0.00237244]\n",
      " [0.00235659]\n",
      " [0.00235885]\n",
      " [0.00235564]\n",
      " [0.00235528]\n",
      " [0.0023694 ]\n",
      " [0.0023593 ]\n",
      " [0.00236645]\n",
      " [0.00235745]\n",
      " [0.00235766]\n",
      " [0.00237986]\n",
      " [0.00236759]\n",
      " [0.00235534]\n",
      " [0.00235921]\n",
      " [0.0023928 ]\n",
      " [0.00235909]\n",
      " [0.00242284]\n",
      " [0.00243405]\n",
      " [0.00235623]\n",
      " [0.00235522]\n",
      " [0.00235513]\n",
      " [0.00304657]\n",
      " [0.0023945 ]\n",
      " [0.00235575]\n",
      " [0.00235558]\n",
      " [0.00235742]\n",
      " [0.00239882]\n",
      " [0.00235593]\n",
      " [0.00235659]\n",
      " [0.00238991]\n",
      " [0.00235873]\n",
      " [0.00236017]\n",
      " [0.00235584]\n",
      " [0.00239921]\n",
      " [0.00235796]\n",
      " [0.00235498]\n",
      " [0.00235659]\n",
      " [0.00235668]\n",
      " [0.00235897]\n",
      " [0.00239384]\n",
      " [0.00235492]\n",
      " [0.00235629]\n",
      " [0.00241166]\n",
      " [0.00235862]\n",
      " [0.00235519]\n",
      " [0.00235546]\n",
      " [0.00235534]\n",
      " [0.00236082]\n",
      " [0.00235656]\n",
      " [0.00235543]\n",
      " [0.00235927]\n",
      " [0.00236019]\n",
      " [0.00236282]\n",
      " [0.00240439]\n",
      " [0.00235569]\n",
      " [0.00237438]\n",
      " [0.00239632]\n",
      " [0.00237024]\n",
      " [0.00235927]\n",
      " [0.00236678]\n",
      " [0.00235546]\n",
      " [0.00235498]\n",
      " [0.00235909]\n",
      " [0.00235826]\n",
      " [0.00235668]\n",
      " [0.00235656]\n",
      " [0.00235564]\n",
      " [0.00235614]\n",
      " [0.00235957]\n",
      " [0.00235546]\n",
      " [0.00235513]\n",
      " [0.00235742]\n",
      " [0.00235513]\n",
      " [0.0023554 ]\n",
      " [0.00239599]\n",
      " [0.00235647]\n",
      " [0.00235635]\n",
      " [0.00238445]\n",
      " [0.00235635]\n",
      " [0.00235498]\n",
      " [0.00235605]\n",
      " [0.00235534]\n",
      " [0.00235528]\n",
      " [0.00235766]\n",
      " [0.00238192]\n",
      " [0.00235599]\n",
      " [0.00235891]\n",
      " [0.00235623]\n",
      " [0.00235608]\n",
      " [0.00235805]\n",
      " [0.00235727]\n",
      " [0.00235513]\n",
      " [0.00241297]\n",
      " [0.00237426]\n",
      " [0.00236633]\n",
      " [0.00235581]\n",
      " [0.00237852]\n",
      " [0.00235945]\n",
      " [0.00235626]\n",
      " [0.00236455]\n",
      " [0.00241554]\n",
      " [0.00235805]\n",
      " [0.00237674]\n",
      " [0.00235599]\n",
      " [0.00236732]\n",
      " [0.00235513]\n",
      " [0.00235564]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_a_loaded.predict(model_a_X_test_padded)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 0s 376us/sample - loss: 2.9841e-04 - mae: 0.0173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.000298409072583083, 0.017268343]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model_a.evaluate(model_a_X_test_padded, model_a_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (word vectors + price history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_df = df.dropna(subset=['cleaned_text', 'prev_60mins_prices'])\n",
    "\n",
    "model_b_X = model_b_df.loc[:, ['cleaned_text_2', 'prev_60mins_prices']]\n",
    "model_b_y = model_b_df.loc[:, '60mins_price_diff_perc']\n",
    "\n",
    "model_b_X_train, model_b_X_test, model_b_y_train, model_b_y_test = train_test_split(model_b_X, model_b_y, test_size=0.33, random_state=42)\n",
    "\n",
    "model_b_X_train_text = model_b_X_train.iloc[:, 0]\n",
    "model_b_X_train_price_history = model_b_X_train.iloc[:, 1].apply(lambda x: split(strip()))\n",
    "model_b_X_test_text = model_b_X_test.iloc[:, 0]\n",
    "model_b_X_test_price_history = model_b_X_test.iloc[:, 1]\n",
    "                           \n",
    "model_b_corpus_list = []\n",
    "\n",
    "for i in model_b_X_train_text:\n",
    "    model_b_corpus_list.append(i.split())\n",
    "    \n",
    "model_b_word2vec_model = Word2Vec(model_b_corpus_list, min_count=1, size=100)\n",
    "model_b_pretrained_weights = model_b_word2vec_model.wv.vectors\n",
    "\n",
    "model_b_num_words = [len(i) for i in model_b_corpus_list]\n",
    "model_b_longest_sentence_len = max(model_b_num_words)\n",
    "\n",
    "model_b_X_train_padded = sentence_to_indices_padded(model_b_X_train_text, model_b_longest_sentence_len)\n",
    "model_b_X_test_padded = sentence_to_indices_padded(model_b_X_test_text, model_b_longest_sentence_len)\n",
    "\n",
    "model_b_X_train_input = [model_b_X_train_padded, np.array(model_b_X_train_price_history)]\n",
    "model_b_X_test_input = [model_b_X_test_padded, model_b_X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(model_b_X_train_input[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='relu')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model_b = create_model_b(model_b_pretrained_weights, model_b_longest_sentence_len, (30,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_index_input (InputLaye [(None, 157)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 157, 100)     568600      sentence_index_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "price_history_input (InputLayer [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM1 (LSTM)             (None, 157, 4)       1680        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM1 (LSTM)             (None, 30, 4)        96          price_history_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout1 (Dropout)       (None, 157, 4)       0           model1_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout1 (Dropout)       (None, 30, 4)        0           model2_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM2 (LSTM)             (None, 4)            144         model1_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM2 (LSTM)             (None, 4)            144         model2_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout2 (Dropout)       (None, 4)            0           model1_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout2 (Dropout)       (None, 4)            0           model2_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           model1_dropout2[0][0]            \n",
      "                                                                 model2_dropout2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 4)            36          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           Dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 570,705\n",
      "Trainable params: 2,105\n",
      "Non-trainable params: 568,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected price_history_input to have 3 dimensions, but got array with shape (1697, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-51ac47a17a6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     save_best_only=True) \n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_b_X_train_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_b_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    374\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected price_history_input to have 3 dimensions, but got array with shape (1697, 1)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_b_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_b.fit(model_b_X_train_input, model_b_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[277.26, 277.27, 277.28, 277.28999999999996, 277.3, 277.305, 277.31, 277.31, 277.34000000000003, 277.37, 277.34, 277.3, 277.31, 277.31, 277.31, 277.31, 277.31, 277.35, 277.35, 277.35, 277.35, 277.35, 277.35, 277.35, 277.346, 277.342, 277.338, 277.334, 277.33, 277.33]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b_X_train_input[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
