{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2219 entries, 0 to 2218\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      2219 non-null   float64\n",
      " 1   cleaned_text            2219 non-null   object \n",
      " 2   favorites               2219 non-null   int64  \n",
      " 3   retweets                2219 non-null   int64  \n",
      " 4   date                    2219 non-null   object \n",
      " 5   tweet_datetime          2219 non-null   object \n",
      " 6   date_part               2219 non-null   object \n",
      " 7   time_part               2219 non-null   object \n",
      " 8   hour                    2219 non-null   int64  \n",
      " 9   year                    2219 non-null   int64  \n",
      " 10  month                   2219 non-null   int64  \n",
      " 11  cleaned_text_2          2219 non-null   object \n",
      " 12  datetime_60mins_after   2219 non-null   object \n",
      " 13  price_60mins_after      2219 non-null   float64\n",
      " 14  datetime_now            2219 non-null   object \n",
      " 15  price_now               2219 non-null   float64\n",
      " 16  60mins_price_diff_abs   2219 non-null   float64\n",
      " 17  60mins_price_diff_perc  2219 non-null   float64\n",
      " 18  prev_60mins_prices      2083 non-null   object \n",
      "dtypes: float64(5), int64(5), object(9)\n",
      "memory usage: 329.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "      <th>datetime_60mins_after</th>\n",
       "      <th>price_60mins_after</th>\n",
       "      <th>datetime_now</th>\n",
       "      <th>price_now</th>\n",
       "      <th>60mins_price_diff_abs</th>\n",
       "      <th>60mins_price_diff_perc</th>\n",
       "      <th>prev_60mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand!</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>2017-11-28 11:50:00</td>\n",
       "      <td>261.485000</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>261.100000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>260.96,261.03,261.01,261.015,261.04,261.01,261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>join me live from fort myer in arlington virgi...</td>\n",
       "      <td>2017-08-22 10:00:00</td>\n",
       "      <td>244.260000</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>243.76,243.79,243.85,243.86,243.81,243.78,243....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole!</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>2017-05-09 08:01:00</td>\n",
       "      <td>239.940000</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>239.875000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>239.73,239.73,239.73,239.73,239.73,239.73,239....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "      <td>2017-03-08 05:44:00</td>\n",
       "      <td>237.022857</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>236.880000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>236.84,236.84,236.84,236.85333333333332,236.86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.787250e+17</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>134210</td>\n",
       "      <td>36346</td>\n",
       "      <td>2017-06-24 21:23:00</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>make america great again</td>\n",
       "      <td>2017-06-25 06:23:00</td>\n",
       "      <td>243.326476</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>243.320762</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>243.31790476190474,243.31799999999998,243.3180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>If the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>2018-05-23 10:13:00</td>\n",
       "      <td>271.930000</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>271.040000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>271.18,271.16,271.18,271.15,271.08,271.07,271....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>So General Michael Flynn’s life can be totally...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>2018-04-20 19:34:00</td>\n",
       "      <td>267.025000</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>266.820000</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>266.74625000000003,266.7475,266.74875,266.75,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>My thoughts, prayers and condolences are with ...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>2018-04-20 07:30:00</td>\n",
       "      <td>269.070000</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>268.620000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>268.77,268.74333333333334,268.71666666666664,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>Today’s Court decision means that Congress mus...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>2018-04-18 06:34:00</td>\n",
       "      <td>270.695000</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>270.73,270.7,270.72749999999996,270.755,270.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>I am pleased to announce that I intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>2018-03-29 06:31:00</td>\n",
       "      <td>261.170000</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>260.982857</td>\n",
       "      <td>0.187143</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>260.82,260.89,260.9,260.87666666666667,260.853...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2219 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.353400e+17                                   Thank you Rand!    \n",
       "1     8.997980e+17  Join me live from Fort Myer in Arlington, Virg...   \n",
       "2     8.939700e+17                                 Thank you Nicole!    \n",
       "3     8.819770e+17  Thank you to Shawn Steel for the nice words on...   \n",
       "4     8.787250e+17                          MAKE AMERICA GREAT AGAIN!   \n",
       "...            ...                                                ...   \n",
       "2214  9.990960e+17  If the person placed very early into my campai...   \n",
       "2215  9.874600e+17  So General Michael Flynn’s life can be totally...   \n",
       "2216  9.870960e+17  My thoughts, prayers and condolences are with ...   \n",
       "2217  9.863570e+17  Today’s Court decision means that Congress mus...   \n",
       "2218  9.791090e+17  I am pleased to announce that I intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4        134210     36346  2017-06-24 21:23:00  2017-06-25 05:23:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2214      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2215      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2216      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2217      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2218      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   \n",
       "1     2017-08-22  09:00:00     9  2017      8   \n",
       "2     2017-05-09  07:01:00     7  2017      5   \n",
       "3     2017-03-08  04:44:00     4  2017      3   \n",
       "4     2017-06-25  05:23:00     5  2017      6   \n",
       "...          ...       ...   ...   ...    ...   \n",
       "2214  2018-05-23  09:13:00     9  2018      5   \n",
       "2215  2018-04-20  18:34:00    18  2018      4   \n",
       "2216  2018-04-20  06:30:00     6  2018      4   \n",
       "2217  2018-04-18  05:34:00     5  2018      4   \n",
       "2218  2018-03-29  05:31:00     5  2018      3   \n",
       "\n",
       "                                         cleaned_text_2 datetime_60mins_after  \\\n",
       "0                                       thank you rand    2017-11-28 11:50:00   \n",
       "1     join me live from fort myer in arlington virgi...   2017-08-22 10:00:00   \n",
       "2                                     thank you nicole    2017-05-09 08:01:00   \n",
       "3       thank you to shawn steel for the nice words on    2017-03-08 05:44:00   \n",
       "4                              make america great again   2017-06-25 06:23:00   \n",
       "...                                                 ...                   ...   \n",
       "2214  if the person placed very early into my campai...   2018-05-23 10:13:00   \n",
       "2215  so general michael flynn’s life can be totally...   2018-04-20 19:34:00   \n",
       "2216  my thoughts prayers and condolences are with t...   2018-04-20 07:30:00   \n",
       "2217  today’s court decision means that congress mus...   2018-04-18 06:34:00   \n",
       "2218  i am pleased to announce that i intend to nomi...   2018-03-29 06:31:00   \n",
       "\n",
       "      price_60mins_after         datetime_now   price_now  \\\n",
       "0             261.485000  2017-11-28 10:50:00  261.100000   \n",
       "1             244.260000  2017-08-22 09:00:00  243.670000   \n",
       "2             239.940000  2017-05-09 07:01:00  239.875000   \n",
       "3             237.022857  2017-03-08 04:44:00  236.880000   \n",
       "4             243.326476  2017-06-25 05:23:00  243.320762   \n",
       "...                  ...                  ...         ...   \n",
       "2214          271.930000  2018-05-23 09:13:00  271.040000   \n",
       "2215          267.025000  2018-04-20 18:34:00  266.820000   \n",
       "2216          269.070000  2018-04-20 06:30:00  268.620000   \n",
       "2217          270.695000  2018-04-18 05:34:00  270.600000   \n",
       "2218          261.170000  2018-03-29 05:31:00  260.982857   \n",
       "\n",
       "      60mins_price_diff_abs  60mins_price_diff_perc  \\\n",
       "0                  0.385000                0.001475   \n",
       "1                  0.590000                0.002421   \n",
       "2                  0.065000                0.000271   \n",
       "3                  0.142857                0.000603   \n",
       "4                  0.005714                0.000023   \n",
       "...                     ...                     ...   \n",
       "2214               0.890000                0.003284   \n",
       "2215               0.205000                0.000768   \n",
       "2216               0.450000                0.001675   \n",
       "2217               0.095000                0.000351   \n",
       "2218               0.187143                0.000717   \n",
       "\n",
       "                                     prev_60mins_prices  \n",
       "0     260.96,261.03,261.01,261.015,261.04,261.01,261...  \n",
       "1     243.76,243.79,243.85,243.86,243.81,243.78,243....  \n",
       "2     239.73,239.73,239.73,239.73,239.73,239.73,239....  \n",
       "3     236.84,236.84,236.84,236.85333333333332,236.86...  \n",
       "4     243.31790476190474,243.31799999999998,243.3180...  \n",
       "...                                                 ...  \n",
       "2214  271.18,271.16,271.18,271.15,271.08,271.07,271....  \n",
       "2215  266.74625000000003,266.7475,266.74875,266.75,2...  \n",
       "2216  268.77,268.74333333333334,268.71666666666664,2...  \n",
       "2217  270.73,270.7,270.72749999999996,270.755,270.78...  \n",
       "2218  260.82,260.89,260.9,260.87666666666667,260.853...  \n",
       "\n",
       "[2219 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X = model_a_df.loc[:, 'cleaned_text_2']\n",
    "model_a_y = model_a_df.loc[:, '60mins_price_diff_perc']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train, model_a_X_test, model_a_y_train, model_a_y_test = train_test_split(model_a_X, model_a_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_corpus_list = []\n",
    "\n",
    "for i in model_a_X_train:\n",
    "    model_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(model_a_corpus_list, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_num_words = [len(i) for i in model_a_corpus_list]\n",
    "model_a_longest_sentence_len = max(model_a_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sentence_to_indices_padded(sentences, longest_sentence_len, word2vec_model):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len, glove_model):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                indices.append(word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train_padded = word2vec_sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len, model_a_word2vec_model)\n",
    "model_a_X_test_padded = word2vec_sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len, model_a_word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(model_a_X_train)\n",
    "sequences = tokenizer.texts_to_sequences(model_a_X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_count = dict(tokenizer.word_counts)\n",
    "glove_sorted=dict(sorted(glove_word_count.items(), key=lambda x: x[1],reverse=True))\n",
    "print(glove_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in model_a_word2vec_model.wv.vocab:\n",
    "    w2c[item]=model_a_word2vec_model.wv.vocab[item].count\n",
    "w2c_sorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "print(w2c_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_count = dict(tokenizer.word_counts)\n",
    "glove_sorted=dict(sorted(glove_word_count.items(), key=lambda x: x[1],reverse=True))\n",
    "print(glove_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_w2v = list(w2c.keys())\n",
    "words_gloves = list(glove_word_count.keys())\n",
    "extra = []\n",
    "for word in words_w2v:\n",
    "    if word not in words_gloves:\n",
    "        extra.append(word)\n",
    "print(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "print(vocab_size_glove)\n",
    "print(embedding_size_glove)\n",
    "print(count)\n",
    "print(skipped_words)\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=model_a_longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_2seq_tanh(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_1seq_tanh(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='tanh'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_1seq_sigmoid(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a_2seq_sigmoid(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "#     model.add(embedding_layer_glove)\n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTMs, tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model_a_2seq_tanh = create_model_a_2seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_2seq_tanh.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1188 samples, validate on 298 samples\n",
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0927\n",
      "Epoch 00001: val_loss improved from inf to 0.02292, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 14s 12ms/sample - loss: 0.0305 - mean_absolute_error: 0.0933 - val_loss: 0.0229 - val_mean_absolute_error: 0.0700\n",
      "Epoch 2/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0888\n",
      "Epoch 00002: val_loss improved from 0.02292 to 0.02283, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0291 - mean_absolute_error: 0.0885 - val_loss: 0.0228 - val_mean_absolute_error: 0.0699\n",
      "Epoch 3/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0851\n",
      "Epoch 00003: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 12s 10ms/sample - loss: 0.0287 - mean_absolute_error: 0.0852 - val_loss: 0.0231 - val_mean_absolute_error: 0.0725\n",
      "Epoch 4/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0283 - mean_absolute_error: 0.0870\n",
      "Epoch 00004: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0291 - mean_absolute_error: 0.0877 - val_loss: 0.0233 - val_mean_absolute_error: 0.0737\n",
      "Epoch 5/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0866\n",
      "Epoch 00005: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0291 - mean_absolute_error: 0.0866 - val_loss: 0.0232 - val_mean_absolute_error: 0.0734\n",
      "Epoch 6/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0857\n",
      "Epoch 00006: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0289 - mean_absolute_error: 0.0856 - val_loss: 0.0229 - val_mean_absolute_error: 0.0700\n",
      "Epoch 7/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0841\n",
      "Epoch 00007: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 10ms/sample - loss: 0.0287 - mean_absolute_error: 0.0840 - val_loss: 0.0229 - val_mean_absolute_error: 0.0697\n",
      "Epoch 8/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0857\n",
      "Epoch 00008: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0288 - mean_absolute_error: 0.0856 - val_loss: 0.0228 - val_mean_absolute_error: 0.0696\n",
      "Epoch 9/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0854\n",
      "Epoch 00009: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 13s 11ms/sample - loss: 0.0288 - mean_absolute_error: 0.0853 - val_loss: 0.0229 - val_mean_absolute_error: 0.0699\n",
      "Epoch 10/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0863\n",
      "Epoch 00010: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0288 - mean_absolute_error: 0.0861 - val_loss: 0.0230 - val_mean_absolute_error: 0.0736\n",
      "Epoch 11/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0852\n",
      "Epoch 00011: val_loss improved from 0.02283 to 0.02283, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 14s 12ms/sample - loss: 0.0288 - mean_absolute_error: 0.0850 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 12/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0286 - mean_absolute_error: 0.0840\n",
      "Epoch 00012: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0842 - val_loss: 0.0229 - val_mean_absolute_error: 0.0705\n",
      "Epoch 13/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0277 - mean_absolute_error: 0.0859\n",
      "Epoch 00013: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0290 - mean_absolute_error: 0.0868 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 14/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0896\n",
      "Epoch 00014: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0293 - mean_absolute_error: 0.0893 - val_loss: 0.0229 - val_mean_absolute_error: 0.0697\n",
      "Epoch 15/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0845\n",
      "Epoch 00015: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0844 - val_loss: 0.0229 - val_mean_absolute_error: 0.0702\n",
      "Epoch 16/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0836\n",
      "Epoch 00016: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0230 - val_mean_absolute_error: 0.0707\n",
      "Epoch 17/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0844\n",
      "Epoch 00017: val_loss improved from 0.02283 to 0.02283, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0842 - val_loss: 0.0228 - val_mean_absolute_error: 0.0693\n",
      "Epoch 18/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0845\n",
      "Epoch 00018: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0288 - mean_absolute_error: 0.0844 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 19/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0836\n",
      "Epoch 00019: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 12s 10ms/sample - loss: 0.0286 - mean_absolute_error: 0.0834 - val_loss: 0.0229 - val_mean_absolute_error: 0.0695\n",
      "Epoch 20/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0836\n",
      "Epoch 00020: val_loss did not improve from 0.02283\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0835 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 21/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0835\n",
      "Epoch 00021: val_loss improved from 0.02283 to 0.02282, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0835 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 22/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0842\n",
      "Epoch 00022: val_loss improved from 0.02282 to 0.02282, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0839 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 23/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0835\n",
      "Epoch 00023: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0835 - val_loss: 0.0229 - val_mean_absolute_error: 0.0696\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0845\n",
      "Epoch 00024: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0288 - mean_absolute_error: 0.0844 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 25/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0838\n",
      "Epoch 00025: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0836 - val_loss: 0.0229 - val_mean_absolute_error: 0.0701\n",
      "Epoch 26/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0835\n",
      "Epoch 00026: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 9s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0835 - val_loss: 0.0228 - val_mean_absolute_error: 0.0690\n",
      "Epoch 27/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0836\n",
      "Epoch 00027: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0835 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 28/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0834\n",
      "Epoch 00028: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 9s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0833 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 29/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0840\n",
      "Epoch 00029: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 9s 8ms/sample - loss: 0.0288 - mean_absolute_error: 0.0840 - val_loss: 0.0230 - val_mean_absolute_error: 0.0706\n",
      "Epoch 30/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0837\n",
      "Epoch 00030: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 13s 11ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0231 - val_mean_absolute_error: 0.0717\n",
      "Epoch 31/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0850\n",
      "Epoch 00031: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0286 - mean_absolute_error: 0.0851 - val_loss: 0.0228 - val_mean_absolute_error: 0.0695\n",
      "Epoch 32/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0840\n",
      "Epoch 00032: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0286 - mean_absolute_error: 0.0838 - val_loss: 0.0229 - val_mean_absolute_error: 0.0698\n",
      "Epoch 33/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0838\n",
      "Epoch 00033: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0288 - mean_absolute_error: 0.0837 - val_loss: 0.0229 - val_mean_absolute_error: 0.0702\n",
      "Epoch 34/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0839\n",
      "Epoch 00034: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 35/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0844\n",
      "Epoch 00035: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0843 - val_loss: 0.0228 - val_mean_absolute_error: 0.0693\n",
      "Epoch 36/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0837\n",
      "Epoch 00036: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 37/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0833\n",
      "Epoch 00037: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0836 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 38/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0844\n",
      "Epoch 00038: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0843 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 39/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0836\n",
      "Epoch 00039: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0286 - mean_absolute_error: 0.0834 - val_loss: 0.0229 - val_mean_absolute_error: 0.0697\n",
      "Epoch 40/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0837\n",
      "Epoch 00040: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 19s 16ms/sample - loss: 0.0287 - mean_absolute_error: 0.0837 - val_loss: 0.0228 - val_mean_absolute_error: 0.0690\n",
      "Epoch 41/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0844\n",
      "Epoch 00041: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0844 - val_loss: 0.0230 - val_mean_absolute_error: 0.0712\n",
      "Epoch 42/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0278 - mean_absolute_error: 0.0831\n",
      "Epoch 00042: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 13s 11ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0228 - val_mean_absolute_error: 0.0691\n",
      "Epoch 43/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0853\n",
      "Epoch 00043: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 11s 9ms/sample - loss: 0.0287 - mean_absolute_error: 0.0854 - val_loss: 0.0229 - val_mean_absolute_error: 0.0694\n",
      "Epoch 44/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0834\n",
      "Epoch 00044: val_loss did not improve from 0.02282\n",
      "1188/1188 [==============================] - 12s 10ms/sample - loss: 0.0287 - mean_absolute_error: 0.0834 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 45/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0846\n",
      "Epoch 00045: val_loss improved from 0.02282 to 0.02281, saving model to ./model_a_checkpoint/model_a_2seq_tanh 11112020 1302h.h5\n",
      "1188/1188 [==============================] - 16s 13ms/sample - loss: 0.0287 - mean_absolute_error: 0.0844 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 46/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0843\n",
      "Epoch 00046: val_loss did not improve from 0.02281\n",
      "1188/1188 [==============================] - 9s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0844 - val_loss: 0.0228 - val_mean_absolute_error: 0.0690\n",
      "Epoch 47/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0841\n",
      "Epoch 00047: val_loss did not improve from 0.02281\n",
      "1188/1188 [==============================] - 10s 9ms/sample - loss: 0.0288 - mean_absolute_error: 0.0842 - val_loss: 0.0228 - val_mean_absolute_error: 0.0692\n",
      "Epoch 48/50\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0840\n",
      "Epoch 00048: val_loss did not improve from 0.02281\n",
      "1188/1188 [==============================] - 10s 8ms/sample - loss: 0.0287 - mean_absolute_error: 0.0838 - val_loss: 0.0229 - val_mean_absolute_error: 0.0694\n",
      "Epoch 49/50\n",
      " 128/1188 [==>...........................] - ETA: 9s - loss: 0.0441 - mean_absolute_error: 0.0900"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_2seq_tanh {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_2seq_tanh.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LSTM, tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_1seq_tanh = create_model_a_1seq_tanh(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_1seq_tanh.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_1seq_tanh {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_1seq_tanh.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTMs, sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_2seq_sigmoid = create_model_a_2seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_2seq_sigmoid.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_2seq_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/model_a_2seq_sigmoid {dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a_2seq_sigmoid.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LSTM, sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_1seq_sigmoid = create_model_a_1seq_sigmoid(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_a_1seq_sigmoid.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "model_a_1seq_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = create_model_a(model_a_pretrained_weights, model_a_longest_sentence_len)\n",
    "loaded_model.load_weights('./model_a_checkpoint/11112020 1242h (tanh).h5')\n",
    "loaded_model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "y_pred = loaded_model.predict(model_a_X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model_a.evaluate(model_a_X_test_padded, model_a_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh\n",
    "\n",
    "from sklearn import metrics\n",
    "dev_loss, dev_acc = loaded_model.evaluate(model_a_X_test_padded,  model_a_y_test, verbose=1)\n",
    "\n",
    "\n",
    "print( np.sqrt( metrics.mean_squared_error(model_a_y_train, loaded_model.predict(model_a_X_train_padded)) ) )\n",
    "print( np.sqrt( metrics.mean_squared_error(model_a_y_test, loaded_model.predict(model_a_X_test_padded)) ) )\n",
    "print( np.sqrt( metrics.mean_squared_error(model_a_y_test, 0*model_a_y_test ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(model_a_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (word vectors + price history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_df = df.dropna(subset=['cleaned_text', 'prev_60mins_prices'])\n",
    "\n",
    "model_b_X = model_b_df.loc[:, ['cleaned_text_2', 'prev_60mins_prices']]\n",
    "model_b_y = model_b_df.loc[:, '60mins_price_diff_perc']\n",
    "\n",
    "model_b_X_train, model_b_X_test, model_b_y_train, model_b_y_test = train_test_split(model_b_X, model_b_y, test_size=0.33, random_state=42)\n",
    "\n",
    "model_b_X_train_text = model_b_X_train.iloc[:, 0]\n",
    "model_b_X_train_price_history = model_b_X_train.iloc[:, 1].apply(lambda x: split(strip()))\n",
    "model_b_X_test_text = model_b_X_test.iloc[:, 0]\n",
    "model_b_X_test_price_history = model_b_X_test.iloc[:, 1]\n",
    "                           \n",
    "model_b_corpus_list = []\n",
    "\n",
    "for i in model_b_X_train_text:\n",
    "    model_b_corpus_list.append(i.split())\n",
    "    \n",
    "model_b_word2vec_model = Word2Vec(model_b_corpus_list, min_count=1, size=100)\n",
    "model_b_pretrained_weights = model_b_word2vec_model.wv.vectors\n",
    "\n",
    "model_b_num_words = [len(i) for i in model_b_corpus_list]\n",
    "model_b_longest_sentence_len = max(model_b_num_words)\n",
    "\n",
    "model_b_X_train_padded = sentence_to_indices_padded(model_b_X_train_text, model_b_longest_sentence_len)\n",
    "model_b_X_test_padded = sentence_to_indices_padded(model_b_X_test_text, model_b_longest_sentence_len)\n",
    "\n",
    "model_b_X_train_input = [model_b_X_train_padded, np.array(model_b_X_train_price_history)]\n",
    "model_b_X_test_input = [model_b_X_test_padded, model_b_X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.array(model_b_X_train_input[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='relu')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = create_model_b(model_b_pretrained_weights, model_b_longest_sentence_len, (30,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_b_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_b.fit(model_b_X_train_input, model_b_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_X_train_input[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
