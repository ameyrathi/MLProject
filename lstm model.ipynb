{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_5mins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, 'cleaned_text']\n",
    "y = df.loc[:, '5mins_price_diff_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "\n",
    "for i in X:\n",
    "    corpus_list.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10608, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2Vec(corpus_list, min_count=1, size=100)\n",
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices_padded(X, longest_sentence_len):\n",
    "    result = []\n",
    "    for sentence in X:\n",
    "        indices = [word2vec_model.wv.vocab[i].index for i in sentence.split()]\n",
    "        result.append(indices)\n",
    "    return pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_words = [len(i) for i in corpus_list]\n",
    "longest_sentence_len = max(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702    Kim Jong Un of North Korea proclaims “unwaveri...\n",
       "2164    Prime Minister Trudeau is being so indignant, ...\n",
       "1281    “Trump gets no credit for what he’s done in th...\n",
       "1866    ....China, which is for the first time doing p...\n",
       "306     Two dozen NFL players continue to kneel during...\n",
       "                              ...                        \n",
       "1638    Best economic numbers in decades. If the Democ...\n",
       "1095    \"Is it legal for a sitting President to be \"\"w...\n",
       "1130    The FAKE NEWS media (failing @nytimes, @CNN, @...\n",
       "1294    As I predicted all along, Obamacare has been s...\n",
       "860     KAREN HANDEL FOR CONGRESS. She will fight for ...\n",
       "Name: cleaned_text, Length: 1788, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10608"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        if word_to_vec_map[word].shape[0] == emb_dim:\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = keras.layers.Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(pretrained_weights, longest_sentence_len):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 158, 100)          1060800   \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 158, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 158, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,062,649\n",
      "Trainable params: 1,062,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_LSTM_model(pretrained_weights, longest_sentence_len)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  339   469   470 ...     0     0     0]\n",
      " [  269   286  3898 ...     0     0     0]\n",
      " [  823   634    65 ...     0     0     0]\n",
      " ...\n",
      " [   22   309   442 ...     0     0     0]\n",
      " [  258    14  3291 ...     0     0     0]\n",
      " [10601 10602  1126 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_to_indices_padded(X_train, longest_sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4320 10473  5364  9016  8178  5010  1475 10107  2947  1371 10527  7902\n",
      "  3667  8546  9452 10024   441  4320  5783  5534  4592  8519  8258  6776\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "max_length = vocab_size\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=longest_sentence_len, padding='post')\n",
    "print(padded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you Rand! '"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
