{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2219 entries, 0 to 2218\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      2219 non-null   float64\n",
      " 1   cleaned_text            2219 non-null   object \n",
      " 2   favorites               2219 non-null   int64  \n",
      " 3   retweets                2219 non-null   int64  \n",
      " 4   date                    2219 non-null   object \n",
      " 5   tweet_datetime          2219 non-null   object \n",
      " 6   date_part               2219 non-null   object \n",
      " 7   time_part               2219 non-null   object \n",
      " 8   hour                    2219 non-null   int64  \n",
      " 9   year                    2219 non-null   int64  \n",
      " 10  month                   2219 non-null   int64  \n",
      " 11  cleaned_text_2          2219 non-null   object \n",
      " 12  datetime_60mins_after   2219 non-null   object \n",
      " 13  price_60mins_after      2219 non-null   float64\n",
      " 14  datetime_now            2219 non-null   object \n",
      " 15  price_now               2219 non-null   float64\n",
      " 16  60mins_price_diff_abs   2219 non-null   float64\n",
      " 17  60mins_price_diff_perc  2219 non-null   float64\n",
      " 18  prev_60mins_prices      2083 non-null   object \n",
      "dtypes: float64(5), int64(5), object(9)\n",
      "memory usage: 329.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_stocks_combined_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>date_part</th>\n",
       "      <th>time_part</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "      <th>datetime_60mins_after</th>\n",
       "      <th>price_60mins_after</th>\n",
       "      <th>datetime_now</th>\n",
       "      <th>price_now</th>\n",
       "      <th>60mins_price_diff_abs</th>\n",
       "      <th>60mins_price_diff_perc</th>\n",
       "      <th>prev_60mins_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.353400e+17</td>\n",
       "      <td>Thank you Rand!</td>\n",
       "      <td>42793</td>\n",
       "      <td>9125</td>\n",
       "      <td>2017-11-28 02:50:00</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>10:50:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>thank you rand</td>\n",
       "      <td>2017-11-28 11:50:00</td>\n",
       "      <td>261.485000</td>\n",
       "      <td>2017-11-28 10:50:00</td>\n",
       "      <td>261.100000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>260.96,261.03,261.01,261.015,261.04,261.01,261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.997980e+17</td>\n",
       "      <td>Join me live from Fort Myer in Arlington, Virg...</td>\n",
       "      <td>36009</td>\n",
       "      <td>4891</td>\n",
       "      <td>2017-08-22 01:00:00</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>join me live from fort myer in arlington virgi...</td>\n",
       "      <td>2017-08-22 10:00:00</td>\n",
       "      <td>244.260000</td>\n",
       "      <td>2017-08-22 09:00:00</td>\n",
       "      <td>243.670000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>243.76,243.79,243.85,243.86,243.81,243.78,243....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.939700e+17</td>\n",
       "      <td>Thank you Nicole!</td>\n",
       "      <td>43367</td>\n",
       "      <td>8275</td>\n",
       "      <td>2017-05-08 23:01:00</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>07:01:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>thank you nicole</td>\n",
       "      <td>2017-05-09 08:01:00</td>\n",
       "      <td>239.940000</td>\n",
       "      <td>2017-05-09 07:01:00</td>\n",
       "      <td>239.875000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>239.73,239.73,239.73,239.73,239.73,239.73,239....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.819770e+17</td>\n",
       "      <td>Thank you to Shawn Steel for the nice words on...</td>\n",
       "      <td>50956</td>\n",
       "      <td>7465</td>\n",
       "      <td>2017-03-07 20:44:00</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>04:44:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>thank you to shawn steel for the nice words on</td>\n",
       "      <td>2017-03-08 05:44:00</td>\n",
       "      <td>237.022857</td>\n",
       "      <td>2017-03-08 04:44:00</td>\n",
       "      <td>236.880000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>236.84,236.84,236.84,236.85333333333332,236.86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.787250e+17</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>134210</td>\n",
       "      <td>36346</td>\n",
       "      <td>2017-06-24 21:23:00</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>05:23:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>make america great again</td>\n",
       "      <td>2017-06-25 06:23:00</td>\n",
       "      <td>243.326476</td>\n",
       "      <td>2017-06-25 05:23:00</td>\n",
       "      <td>243.320762</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>243.31790476190474,243.31799999999998,243.3180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>9.990960e+17</td>\n",
       "      <td>If the person placed very early into my campai...</td>\n",
       "      <td>78529</td>\n",
       "      <td>20098</td>\n",
       "      <td>2018-05-23 01:13:00</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>09:13:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>if the person placed very early into my campai...</td>\n",
       "      <td>2018-05-23 10:13:00</td>\n",
       "      <td>271.930000</td>\n",
       "      <td>2018-05-23 09:13:00</td>\n",
       "      <td>271.040000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>271.18,271.16,271.18,271.15,271.08,271.07,271....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>9.874600e+17</td>\n",
       "      <td>So General Michael Flynn’s life can be totally...</td>\n",
       "      <td>93569</td>\n",
       "      <td>25259</td>\n",
       "      <td>2018-04-20 10:34:00</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>18:34:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>so general michael flynn’s life can be totally...</td>\n",
       "      <td>2018-04-20 19:34:00</td>\n",
       "      <td>267.025000</td>\n",
       "      <td>2018-04-20 18:34:00</td>\n",
       "      <td>266.820000</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>266.74625000000003,266.7475,266.74875,266.75,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>9.870960e+17</td>\n",
       "      <td>My thoughts, prayers and condolences are with ...</td>\n",
       "      <td>62645</td>\n",
       "      <td>16081</td>\n",
       "      <td>2018-04-19 22:30:00</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>my thoughts prayers and condolences are with t...</td>\n",
       "      <td>2018-04-20 07:30:00</td>\n",
       "      <td>269.070000</td>\n",
       "      <td>2018-04-20 06:30:00</td>\n",
       "      <td>268.620000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>268.77,268.74333333333334,268.71666666666664,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>9.863570e+17</td>\n",
       "      <td>Today’s Court decision means that Congress mus...</td>\n",
       "      <td>56749</td>\n",
       "      <td>12426</td>\n",
       "      <td>2018-04-17 21:34:00</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>05:34:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>today’s court decision means that congress mus...</td>\n",
       "      <td>2018-04-18 06:34:00</td>\n",
       "      <td>270.695000</td>\n",
       "      <td>2018-04-18 05:34:00</td>\n",
       "      <td>270.600000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>270.73,270.7,270.72749999999996,270.755,270.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>9.791090e+17</td>\n",
       "      <td>I am pleased to announce that I intend to nomi...</td>\n",
       "      <td>66173</td>\n",
       "      <td>13399</td>\n",
       "      <td>2018-03-28 21:31:00</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>05:31:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>i am pleased to announce that i intend to nomi...</td>\n",
       "      <td>2018-03-29 06:31:00</td>\n",
       "      <td>261.170000</td>\n",
       "      <td>2018-03-29 05:31:00</td>\n",
       "      <td>260.982857</td>\n",
       "      <td>0.187143</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>260.82,260.89,260.9,260.87666666666667,260.853...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2219 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                       cleaned_text  \\\n",
       "0     9.353400e+17                                   Thank you Rand!    \n",
       "1     8.997980e+17  Join me live from Fort Myer in Arlington, Virg...   \n",
       "2     8.939700e+17                                 Thank you Nicole!    \n",
       "3     8.819770e+17  Thank you to Shawn Steel for the nice words on...   \n",
       "4     8.787250e+17                          MAKE AMERICA GREAT AGAIN!   \n",
       "...            ...                                                ...   \n",
       "2214  9.990960e+17  If the person placed very early into my campai...   \n",
       "2215  9.874600e+17  So General Michael Flynn’s life can be totally...   \n",
       "2216  9.870960e+17  My thoughts, prayers and condolences are with ...   \n",
       "2217  9.863570e+17  Today’s Court decision means that Congress mus...   \n",
       "2218  9.791090e+17  I am pleased to announce that I intend to nomi...   \n",
       "\n",
       "      favorites  retweets                 date       tweet_datetime  \\\n",
       "0         42793      9125  2017-11-28 02:50:00  2017-11-28 10:50:00   \n",
       "1         36009      4891  2017-08-22 01:00:00  2017-08-22 09:00:00   \n",
       "2         43367      8275  2017-05-08 23:01:00  2017-05-09 07:01:00   \n",
       "3         50956      7465  2017-03-07 20:44:00  2017-03-08 04:44:00   \n",
       "4        134210     36346  2017-06-24 21:23:00  2017-06-25 05:23:00   \n",
       "...         ...       ...                  ...                  ...   \n",
       "2214      78529     20098  2018-05-23 01:13:00  2018-05-23 09:13:00   \n",
       "2215      93569     25259  2018-04-20 10:34:00  2018-04-20 18:34:00   \n",
       "2216      62645     16081  2018-04-19 22:30:00  2018-04-20 06:30:00   \n",
       "2217      56749     12426  2018-04-17 21:34:00  2018-04-18 05:34:00   \n",
       "2218      66173     13399  2018-03-28 21:31:00  2018-03-29 05:31:00   \n",
       "\n",
       "       date_part time_part  hour  year  month  \\\n",
       "0     2017-11-28  10:50:00    10  2017     11   \n",
       "1     2017-08-22  09:00:00     9  2017      8   \n",
       "2     2017-05-09  07:01:00     7  2017      5   \n",
       "3     2017-03-08  04:44:00     4  2017      3   \n",
       "4     2017-06-25  05:23:00     5  2017      6   \n",
       "...          ...       ...   ...   ...    ...   \n",
       "2214  2018-05-23  09:13:00     9  2018      5   \n",
       "2215  2018-04-20  18:34:00    18  2018      4   \n",
       "2216  2018-04-20  06:30:00     6  2018      4   \n",
       "2217  2018-04-18  05:34:00     5  2018      4   \n",
       "2218  2018-03-29  05:31:00     5  2018      3   \n",
       "\n",
       "                                         cleaned_text_2 datetime_60mins_after  \\\n",
       "0                                       thank you rand    2017-11-28 11:50:00   \n",
       "1     join me live from fort myer in arlington virgi...   2017-08-22 10:00:00   \n",
       "2                                     thank you nicole    2017-05-09 08:01:00   \n",
       "3       thank you to shawn steel for the nice words on    2017-03-08 05:44:00   \n",
       "4                              make america great again   2017-06-25 06:23:00   \n",
       "...                                                 ...                   ...   \n",
       "2214  if the person placed very early into my campai...   2018-05-23 10:13:00   \n",
       "2215  so general michael flynn’s life can be totally...   2018-04-20 19:34:00   \n",
       "2216  my thoughts prayers and condolences are with t...   2018-04-20 07:30:00   \n",
       "2217  today’s court decision means that congress mus...   2018-04-18 06:34:00   \n",
       "2218  i am pleased to announce that i intend to nomi...   2018-03-29 06:31:00   \n",
       "\n",
       "      price_60mins_after         datetime_now   price_now  \\\n",
       "0             261.485000  2017-11-28 10:50:00  261.100000   \n",
       "1             244.260000  2017-08-22 09:00:00  243.670000   \n",
       "2             239.940000  2017-05-09 07:01:00  239.875000   \n",
       "3             237.022857  2017-03-08 04:44:00  236.880000   \n",
       "4             243.326476  2017-06-25 05:23:00  243.320762   \n",
       "...                  ...                  ...         ...   \n",
       "2214          271.930000  2018-05-23 09:13:00  271.040000   \n",
       "2215          267.025000  2018-04-20 18:34:00  266.820000   \n",
       "2216          269.070000  2018-04-20 06:30:00  268.620000   \n",
       "2217          270.695000  2018-04-18 05:34:00  270.600000   \n",
       "2218          261.170000  2018-03-29 05:31:00  260.982857   \n",
       "\n",
       "      60mins_price_diff_abs  60mins_price_diff_perc  \\\n",
       "0                  0.385000                0.001475   \n",
       "1                  0.590000                0.002421   \n",
       "2                  0.065000                0.000271   \n",
       "3                  0.142857                0.000603   \n",
       "4                  0.005714                0.000023   \n",
       "...                     ...                     ...   \n",
       "2214               0.890000                0.003284   \n",
       "2215               0.205000                0.000768   \n",
       "2216               0.450000                0.001675   \n",
       "2217               0.095000                0.000351   \n",
       "2218               0.187143                0.000717   \n",
       "\n",
       "                                     prev_60mins_prices  \n",
       "0     260.96,261.03,261.01,261.015,261.04,261.01,261...  \n",
       "1     243.76,243.79,243.85,243.86,243.81,243.78,243....  \n",
       "2     239.73,239.73,239.73,239.73,239.73,239.73,239....  \n",
       "3     236.84,236.84,236.84,236.85333333333332,236.86...  \n",
       "4     243.31790476190474,243.31799999999998,243.3180...  \n",
       "...                                                 ...  \n",
       "2214  271.18,271.16,271.18,271.15,271.08,271.07,271....  \n",
       "2215  266.74625000000003,266.7475,266.74875,266.75,2...  \n",
       "2216  268.77,268.74333333333334,268.71666666666664,2...  \n",
       "2217  270.73,270.7,270.72749999999996,270.755,270.78...  \n",
       "2218  260.82,260.89,260.9,260.87666666666667,260.853...  \n",
       "\n",
       "[2219 rows x 19 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (only word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X = model_a_df.loc[:, 'cleaned_text_2']\n",
    "model_a_y = model_a_df.loc[:, '60mins_price_diff_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train, model_a_X_test, model_a_y_train, model_a_y_test = train_test_split(model_a_X, model_a_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_corpus_list = []\n",
    "\n",
    "for i in model_a_X_train:\n",
    "    model_a_corpus_list.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_word2vec_model = Word2Vec(model_a_corpus_list, min_count=1, size=100)\n",
    "model_a_pretrained_weights = model_a_word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_num_words = [len(i) for i in model_a_corpus_list]\n",
    "model_a_longest_sentence_len = max(model_a_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in model_a_word2vec_model.wv.vocab:\n",
    "                indices.append(model_a_word2vec_model.wv.vocab[word].index)\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_X_train_padded = sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "model_a_X_test_padded = sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.twitter.27B.100d.txt', encoding='utf8')\n",
    "glove_vocab = []\n",
    "glove_vocab_index = {}\n",
    "count = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    glove_vocab.append(word)\n",
    "    glove_vocab_index[word] = count\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    count += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1359, 62, 21, 1121, 62, 13, 1, 2550, 25, 1, 1360, 4, 28, 375, 120, 7, 75, 75, 75, 37, 19]\n",
      "Found 5169 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amey/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(model_a_X_train)\n",
    "sequences = tokenizer.texts_to_sequences(model_a_X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(sequences[1])\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_sentence_to_indices_padded(sentences, longest_sentence_len):\n",
    "    global glove_vocab\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        indices = []\n",
    "        sentence_splitted = sentence.split()\n",
    "        for word in sentence_splitted:\n",
    "            if word in glove_vocab:\n",
    "                indices.append(glove_vocab_index[word])\n",
    "        result.append(indices)\n",
    "    return keras.preprocessing.sequence.pad_sequences(result, maxlen=longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_glove = glove_sentence_to_indices_padded(model_a_X_train, model_a_longest_sentence_len)\n",
    "x_test_glove = glove_sentence_to_indices_padded(model_a_X_test, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5894\n",
      "100\n",
      "916\n",
      "['don’t', 'foxandfriends', 'didn’t', 'it’s', 'can’t', '“the', 'strzok', 'doesn’t', 'isn’t', 'loudobbs', 'seanhannity', 'councel', 'that’s', 'aren’t', 'couldn’t', 'trump”', 'kavanaugh', 'wasn’t', 'he’s', 'won’t', 'magarally', 'foxbusiness', '“trump', 'america’s', 'we’ve', 'dhsgov', 'petestauber', 'tuckercarlson', 'i’ve', 'country’s', 'emmanuelmacron', 'mariabartiromo', '“i', 'today’s', 'senatemajldr', 'usmca', 'antitrump', 'states”', 'deptvetaffairs', 'haspel', 'oreillyfactor', 'abeshinzo', 'maralago', 'taxreform', 'people’s', '“we', 'we’re', 'speakerryan', 'wouldn’t', 'i’m', 'fitton', 'ohr’s', 'you’re', 'they’re', 'they’ve', 'wyou', 'ivankatrump', 'i’ll', '“it', '“mr', 'ossoff', 'icegov', 'americafirst', 'rosendale', 'world’s', 'jobsnotmobs', 'breitbartnews', '“why', 'gopleader', 'hillary’s', '“you', 'tillerson', 'mcgahn', '“president', '“justice”', 'denuclearization', '“this', 'ingrahamangle', 'jimhagedornmn', 'brennan’s', 'sheriff’s', 'nonmonetary', 'votekarenhandel', 'hawleymo', 'markburnetttv', 'jpnpmo', '“how', 'hurricaneharvey', 'blakeman', 'america”', 'shopfloornam', '“peanuts”', 'scottforflorida', 'president”', '“no', 'shinzō', 'strassel', 'jongun', 'we’ll', 'mueller’s', 'troybalderson', 'dougducey', 'covfefe', '“trump’s', 'davidmuir', 'stevescalise', 'system”', 'housegop', '“when', 'femabrock', '“what', 'daveweigel', 'what’s', 'balderson', 'dannytarkanian', '“these', 'worker”', 'judicialwatch', '“it’s', 'trump’s', '“there’s', 'schuette', 'uswomensopen', '“bruce', 'nikkihaley', 'thebigeasy', '“there', 'jfkfiles', 'scalise', '“presidential', 'draintheswamp', 'cutsreform', 'kateslaw', 'lawabiding', 'nation’s', 'correspondents’', 'judgejeanine', 'good”', '“liars', 'conspiracy”', 'jobsborder', 'god’s', 'ricardorossello', 'prstrong', '“director', 'erikpaulsen', 'europe’s', 'hurricanemichael', '“animals”', '“special”', 'china’s', 'expropriations', '“south', 'farmers”', 'here’s', '“north', 'something’s', 'sampp', 'obama’s', '“fbi', 'r’s', 'fbi’s', '“media', 'people”', 'is”', 'judgeships', 'again”', 'repdandonovan', 'times”', 'stop”', 'exposed”', '“o”', 'general’s', 'murl', 'nycemergencymgt', 'workforceweek', 'wctc', 'govwalker', 'oppositedisgraceful', 'bordersthey', 'jessebwatters', 'magoo”', 'rosenstein', 'peepers”', '“according', 'spoken”', 'charactersjust', 'excoriated', 'cathymcmorris', 'yearmost', 'usedgov', 'betsydevosed', 'indopacific', 'wdiverse', 'jeffress', 'commanderinchief’s', '“papers”', 'senatordole', 'customsborder', 'rightthe', 'mistake”', 'senatorwicker', 'else”', 'edrollins', 'country”', 'againpotusabroad', 'plantplants', 'miamidade', 'breach”', 'hurricaneirma', 'wrightbrothers', 'junckereu', '“donald', 'seen”', 'mauriciomacri', 'bigwe', 'secazar', 'davidasmanfox', 'improvesall', 'career”', 'justice”', 'govabbott', 'fastthe', 'cindyhydesmith', 'she’s', 'prisonreform', 'pesident', 'you’ve', 'andeavor', 'mediathe', 'place”', '“richard', 'hero”', 'we’d', 'recordhigh', 'aseansummit', 'stateskorea', 'unredacted', 'commanderinchiefs', 'uncheckedcrime', '“slow', 'walking”', 'hurricaneflorence', 'winour', 'overcomeeven', 'wilkesbarre', '“bet', 'force”', 'pagewas', 'notax', 'infrastructureweek', 'steele”', '“obama', 'judges”', '“independent', 'judiciary”', 'marineone', 'countryand', 'yougod', '“absolutely', 'nothing”', '“after', 'him”', 'weissman’s', 'steyer', 'confirmgina', 'mikepence', 'floridaemergency', 'dougburgum', 'brentsanfordnd', 'senjohnhoeven', 'repkevincramer', 'senatorheitkamp', 'fortunedems', 'mnuchin', 'orrinhatch', 'warmbier', 'later”', '‘greatest', 'president’', '“fakers”', 'tariffstrade', '“on', 'miles”', 'jamiejmcintyre', 'glfop', '“senator”', 'sobbingly', '“i’m', 'syria”', '“press”', '“sources”', '“national', 'soon”', 'standing”', 'romoabcnews', 'abcworldnews', 'gcsoflorida', 'are”', 'attorney–client', 'jobcreating', 'fight”', 'anymorewe', 'turnbullmalcolm', 'underpinnings', 'prosecutors”', 'upfake', 'rossello', 'littlewe', 'nationfull', 'federalreserve', 'garyplayer', 'stauber', 'abcwashington', 'presidentscup', '“pastor', 'problack', 'event”', 'senategop', 'johncornyn', 'repealandreplace', 'senschumer', 'counterintelligence', 'ourselvesnot', 'there’s', 'there”', 'democrats’', 'taxcutsandjobsact', 'maria”', 'killed”', 'congratspeggy', '“rat”', 'bad”', 'yesterday’s', '“infestation”', 'elelments', 'placethe', 'himselfi', 'ruinedand', 'nbcwsj', 'hotlinejosh', 'exonerating', 'honor“we', 'howiecarrshow', 'happened”', 'perkinscte', 'denverriggleman', 'harm’s', '“barrack', 'dream”', 'mbwdc', 'nvgop', 'decades”', 'mulvaney', 'deanheller', 'voteralphnorman', 'apprenticebut', 'incredible”', 'saracarterdc', 'hispanicamericans', 'hispanicamerican', 'bastilleday', 'farmagricultural', 'crimes”', 'tarkanian', 'repbost', '“ship', 'fools”', 'navalacademy', 'prezydentpl', '“president’s', '“tariffed”', 'kate’s', 'exfbi', 'tomfitton', 'presidential”', 'dianeharkey', 'issawith', 'vetshas', 'leakin’', 'lyin’', '“legal”', '“his', 'rohrabacher', 'doexcept', 'where’s', 'hcare', 'mindtruly', 'oneamericaappeal', 'mexico’s', 'secretaryperry', 'secretaryzinke', 'secpricemd', 'postthis', 'sessionsthese', 'comeys', '“arrests', 'satisfiedtruly', 'peaceofficersmemorialday', 'andpoliceweek', 'rterdogan', '“next', 'door”', '“at', 'information”', '“china', 'clinton’s', 'server”', 'obstruction”', 'gorsuch', 'coleading', 'u”', 'melania’s', 'houstonstrong', 'russiarussia', 'danaperino', 'bradthorthank', 'astonishingthat', 'terroristalbaghdaditheir', 'lindseygrahamsc', 'nonwalled', 'scottwalker', 'leahvukmir', 'kevincramer', '“flood', 'gates”', 'violationall', 'crimesbecause', 'sessions”', 'fact”', '“i”', 'mikedunleavygov', 'cosumer', 'detractors”', 'agothis', 'progrowth', 'jobs”', 'ussteel', 'admonished', 'happyhanukkah', 'woolsey', 'morriseywv', 'carolmillerwv', 'wstate', '“leadership”', '“man', 'year”', 'gps”', 'asahutchinson', 'walltowall', 'ribbonno', '“smooth', 'machine”', 'bernieandsid', 'alandersh', 'stephenmoore', 'trumponomics', 'longheld', 'flagstand', 'uscgacademy', 'bpolitics', 'misstated', 'georgiabad', 'senorrinhatch', 'shepherding', 'administration’s', 'atampt', 'counceljustice', '“foreign', 'workers”', 'vfwhq', 'vfwconvention', '“journalists”', 'cubavideo', 'days”', 'commanderinchief', 'partnersduring', 'prince’s', 'marineband', 'wilsond', '“insurance', 'policy”', 'entrapping', 'misstatements', 'haven’t', '“department', 'considerations”', '“other', 'side”', 'ussfitzgerald', 'marthamcsally', '“more', 'gop”', 'elonmusk', 'falconheavy', 'nasa’s', 'cortessteve', 'storm’s', 'mattarella', '“i’d', 'do”', 'wpost', 'secnielsen', 'rothfus', 'taxesvery', 'meritbased', 'who’s', 'happyindependenceday', 'opioidepidemic', '“had', 'securitycharles', 'mcculloughfmr', 'credit”', '“kim', 'invade”', '”this', 'criminals”', 'trishregan', '“anonymous', 'sources”', 'he’ll', 'tomreedcongress', '“comedian”', 'linesmuch', 'wead', 'stateno', '“punchdrunk”', 'safefull', '“excellent', 'secshulkins', 'lavar’s', 'man’s', 'teamscalise', '“unsolved', 'mystery”', 'terroristthese', 'screg', 'cuttingstock', 'nationalprayerbreakfast', 'rondesantisfl', 'presidents”', 'youtube’s', 'healthcarebill', 'erikpaulson', 'jasonlewis', 'disgracedraintheswamp', 'trumptime', 'sharkgregnorman', 'us”', '“new', 'campaign”', 'chided', 'jerrybrowngov', 'gavinnewsom', 'hurricanelane', 'staes', 'raffensperger', 'briankempga', 'votebradraff', 'everybody’s', '“piggy', 'bank”', '“trumper”', 'poll”', 'annagiaritelli', 'dcexaminer', 'latterday', 'builthas', 'mansioncompound', 'toyotacenter', 'placesspent', 'differentthat’s', 'elected”', 'offensesconstitution', 'thisstormtrooper', 'almost”', 'lawthe', 'investigators”', 'tarrifs', 'barriersalso', 'antisecond', 'edwgillespie', 'hillaryclinton', 'priding', 'ripevelyn', 'worldautismawarenessday', 'liub', 'republicansgot', 'lifecont', 'storyopinion', 'regime’s', 'iranprotests', 'carafano', 'missiles”also', '“shut', 'tests”', 'uskorea', 'playef', 'naftabut', 'autoworkers', 'news”', 'situationwill', '“thank', 'greenvillespartanburg', 'placelocked', 'loadedshould', 'timewell', 'floridajust', 'bustthe', '“bombed”', 'greggutfeld', 'petehegseth', 'congressmenwomen', 'highand', 'weissman', 'senjoniernst', 'chuckgrassley', 'terrybranstad', 'farreaching', 'necessary”', 'buddforcongress', 'reassigned”', 'jeanclaude', 'votemarsha', 'hardearned', 'historywhy', '“collusion”', 'ukraineanother', 'businessvery', 'presidentsthen', 'securitya', 'ustreasury', 'evergrowing', 'ussarizona', 'exposeda', 'incrediblethanks', '“still', 'percent”', 'crazy”', 'nextrevfnc', '“big', 'one”', 'flynn’s', '“manufacturing', 'survey”', 'officer”', 'misrepresentations', 'guy’s', 'opioidepidemicfull', 'marianorajoy', '“fake', 'wvgovernor', 'kustoff', 'somolia', 'obamaand', 'sidepodesta', 'bordersthe', '“find', 'historyand', 'they’ll', '“bonkers”', 'recommits', 'way”', 'hillaryrussia', 'erictrump', 'seasonno', 'anthemrespect', '“during', 'foundation”', 'repmarkmeadows', 'jimjordan', 'raullabrador', 'housepresident', 'ussouth', 'us–china', 'me”', 'bookbring', 'renegotiation', 'distroys', 'justicedepartment', 'lesleyrstahl', 'lawenforcementappreciationday', 'jim’s', '“boom', 'high”', 'gobbler’s', '“trumps', 'dcthank', 'forwardthis', 'investigated”', 'denuclearize', 'grahamcassidy', 'lineitem', 'reportedfake', 'russiatrump', 'usfake', 'singaporesummit', 'thingsbut', 'endakennytd', 'reploubarletta', 'recusal', 'digenova', '“emails', 'security”', 'expensing', 'northamwho', 'virginiais', 'commencment', 'comey”', 'presidency”', 'secretarygeneral', '“make', '“us', 'insulting”', '“will', 'around”', 'agreeyet', 'alike“remember', 'harbor”', 'suv’s', '“heart”', 'goodlatte', 'greatshould', 'happened”“how', '“russians', 'disgraceand', 'healthcaretax', 'cutssecurity', 'republicansremember', 'membersnot', 'secretservice', 'party’s', 'rexnord', 'secpompeo', 'ainsleyearhardt', 'playbookcall', 'strzokpage', 'strategy”', '”carter', 'trumpstyle', '“out', 'mind”', 'halfstaff', '“catch', 'release”', '“stop', 'pelosischumer', 'heavytruck', '“martin', 'act”', 'redesignates', 'americaexecutive', 'marthamaccallum', 'trumka', 'aflcio', 'novemberand', 'senatorsessions', 'texasstrong', 'gaetz', '“we’re', 'great”', 'usairforce', 'trumppence', 'hhsgov', 'hydesmith', '“very', 'comey’s', 'doj’s', '“hair', 'show”', '“humanized”', '“monster', 'ratings”', 'jurists', 'commercegov', 'secretaryross', 'zinke', 'realromadowney', 'countrybipartisan', '“border', 'wall”', 'democrat’s', 'stopthebias', 'ohio’s', '“weak', 'laws”', 'injusticeand', '‘till', 'unrevealed', 'declassification', 'democratled', '“caravans”', 'congress’', 'congressionalgoldmedal', 'misleadingly', 'team”', 'todayhappythanksgiving', 'pgajohndaly', 'clintonrussiafbijusticeobamacomeylynch', 'goal”rob', 'goldmanvice', 'militaryveterans', '“they', 'couched', 'journalism”', 'geraldorivera', '“we’ll', 'candidates”', 'fandos', 'henrymcmaster', 'colmery', 'secshulkin', 'parisdennard', 'jobkilling', 'mattformontana', 'gregformontana', 'republicanconservative', 'antius', '“a', 'kilmeade', 'loveragent', 'interdictact', '“acid', 'washed”', 'extortionist', '“senior', 'official”', 'himher', '“special', 'livescharlottesville', 'let’s', '“congressman', 'facts”', 'vasecretary', 'shulkin', '“a”and', 'usarmy’s', 'centuryofservice', 'securityisis', 'ingloriously', 'shouldn’t', 'relationshipbut', 'importantbut', 'onceinageneration', 'peoplethe', 'lowerpriced', '“eric', 'thebrodyfile', 'cbnnews', 'hillarythey', 'mayor’s', '“mainstream', 'depression”', 'washtimes', 'roomtoday', 'chanded', 'firstever', 'daythankaveteran', 'cubamemorandum', 'giuseppeconteit', 'cantbut', 'canamp', 'crimethere', 'o’rourke', 'againschumer', 'cavedgambled', 'lost”', 'mukasey', 'messjust', 'repcummings', 'thisobviously', 'unverifiable', 'trumprussia', 'east”', 'madewe', 'buildthewall', 'lisamurkowski', 'dakota’s', 'chagsameach', '“consumer', 'august”', 'repkevinyoder', 'wellwishers', 'boomingbut', 'motheras', 'alainberset', 'circuit”', 'ndsen', 'phoenixrally', '“former', 'dossier”', 'billionyear', 'women’s', '“resist”', '“obstruct”', 'cryin’', '“forever”', '“fox', 'facethenation', 'pelosi’s', 'squawkcnbc', 'joesquawk', 'now—were', '“hercules”', '“nuclear', 'quicklybut', 'lloydsmuckerpa', 'yearfour', 'coauthor', '“trumponomics”', '“best', 'gains”', 'rescissions', 'hearby', 'liliantintori', 'marcorubio']\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "count = 0\n",
    "skipped_words = []\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        skipped_words.append(word)\n",
    "        \n",
    "vocab_size_glove, embedding_size_glove = embedding_matrix.shape\n",
    "\n",
    "# print(vocab_size_glove)\n",
    "# print(embedding_size_glove)\n",
    "# print(count)\n",
    "# print(skipped_words)\n",
    "\n",
    "embedding_layer_glove = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=model_a_longest_sentence_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a(pretrained_weights, longest_sentence_len):\n",
    "    global embedding_layer_glove\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=longest_sentence_len, dtype='int32'))\n",
    "    model.add(embedding_layer_glove)\n",
    "    #model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False))  \n",
    "    model.add(layers.LSTM(4, return_sequences=True, name='LSTM1'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout1'))\n",
    "    model.add(layers.LSTM(4, return_sequences=False, name='LSTM2'))\n",
    "    model.add(layers.Dropout(0.25,name='Dropout2'))\n",
    "    model.add(layers.Dense(4,name='Dense',activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = create_model_a(model_a_pretrained_weights, model_a_longest_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 157, 100)          589400    \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 157, 4)            1680      \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 157, 4)            0         \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 591,249\n",
      "Trainable params: 1,849\n",
      "Non-trainable params: 589,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1425 samples, validate on 357 samples\n",
      "Epoch 1/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.7520 - mae: 0.8478\n",
      "Epoch 00001: val_loss improved from inf to 0.43913, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 5s 3ms/sample - loss: 0.7425 - mae: 0.8419 - val_loss: 0.4391 - val_mae: 0.6627\n",
      "Epoch 2/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.3342 - mae: 0.5624\n",
      "Epoch 00002: val_loss improved from 0.43913 to 0.21717, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 743us/sample - loss: 0.3317 - mae: 0.5601 - val_loss: 0.2172 - val_mae: 0.4660\n",
      "Epoch 3/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.1900 - mae: 0.4197\n",
      "Epoch 00003: val_loss improved from 0.21717 to 0.11213, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 740us/sample - loss: 0.1881 - mae: 0.4174 - val_loss: 0.1121 - val_mae: 0.3349\n",
      "Epoch 4/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.1049 - mae: 0.3069\n",
      "Epoch 00004: val_loss improved from 0.11213 to 0.05813, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 741us/sample - loss: 0.1040 - mae: 0.3053 - val_loss: 0.0581 - val_mae: 0.2411\n",
      "Epoch 5/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0597 - mae: 0.2248\n",
      "Epoch 00005: val_loss improved from 0.05813 to 0.02964, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 741us/sample - loss: 0.0590 - mae: 0.2234 - val_loss: 0.0296 - val_mae: 0.1722\n",
      "Epoch 6/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0363 - mae: 0.1688\n",
      "Epoch 00006: val_loss improved from 0.02964 to 0.01463, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 757us/sample - loss: 0.0359 - mae: 0.1679 - val_loss: 0.0146 - val_mae: 0.1209\n",
      "Epoch 7/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1279\n",
      "Epoch 00007: val_loss improved from 0.01463 to 0.00675, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 745us/sample - loss: 0.0229 - mae: 0.1279 - val_loss: 0.0067 - val_mae: 0.0821\n",
      "Epoch 8/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0146 - mae: 0.0964\n",
      "Epoch 00008: val_loss improved from 0.00675 to 0.00286, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 746us/sample - loss: 0.0144 - mae: 0.0958 - val_loss: 0.0029 - val_mae: 0.0535\n",
      "Epoch 9/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0103 - mae: 0.0762\n",
      "Epoch 00009: val_loss improved from 0.00286 to 0.00102, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 783us/sample - loss: 0.0102 - mae: 0.0755 - val_loss: 0.0010 - val_mae: 0.0319\n",
      "Epoch 10/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0081 - mae: 0.0615\n",
      "Epoch 00010: val_loss improved from 0.00102 to 0.00027, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 753us/sample - loss: 0.0081 - mae: 0.0612 - val_loss: 2.6717e-04 - val_mae: 0.0163\n",
      "Epoch 11/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0078 - mae: 0.0610\n",
      "Epoch 00011: val_loss improved from 0.00027 to 0.00002, saving model to ./model_a_checkpoint/10112020 1819h.h5\n",
      "1425/1425 [==============================] - 1s 747us/sample - loss: 0.0077 - mae: 0.0608 - val_loss: 1.9264e-05 - val_mae: 0.0044\n",
      "Epoch 12/50\n",
      "1344/1425 [===========================>..] - ETA: 0s - loss: 0.0069 - mae: 0.0570\n",
      "Epoch 00012: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 743us/sample - loss: 0.0069 - mae: 0.0568 - val_loss: 2.2061e-05 - val_mae: 0.0047\n",
      "Epoch 13/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0060 - mae: 0.0546\n",
      "Epoch 00013: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 735us/sample - loss: 0.0059 - mae: 0.0545 - val_loss: 1.1358e-04 - val_mae: 0.0106\n",
      "Epoch 14/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0053 - mae: 0.0534\n",
      "Epoch 00014: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 728us/sample - loss: 0.0053 - mae: 0.0534 - val_loss: 2.3416e-04 - val_mae: 0.0153\n",
      "Epoch 15/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0050 - mae: 0.0524\n",
      "Epoch 00015: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 728us/sample - loss: 0.0051 - mae: 0.0527 - val_loss: 3.0124e-04 - val_mae: 0.0174\n",
      "Epoch 16/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0049 - mae: 0.0511\n",
      "Epoch 00016: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 738us/sample - loss: 0.0050 - mae: 0.0513 - val_loss: 3.5837e-04 - val_mae: 0.0189\n",
      "Epoch 17/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0055 - mae: 0.0527\n",
      "Epoch 00017: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 716us/sample - loss: 0.0054 - mae: 0.0526 - val_loss: 4.3767e-04 - val_mae: 0.0209\n",
      "Epoch 18/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0054 - mae: 0.0523\n",
      "Epoch 00018: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 719us/sample - loss: 0.0055 - mae: 0.0525 - val_loss: 5.0093e-04 - val_mae: 0.0224\n",
      "Epoch 19/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0049 - mae: 0.0508\n",
      "Epoch 00019: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 715us/sample - loss: 0.0049 - mae: 0.0506 - val_loss: 5.0611e-04 - val_mae: 0.0225\n",
      "Epoch 20/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0058 - mae: 0.0542\n",
      "Epoch 00020: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 739us/sample - loss: 0.0057 - mae: 0.0538 - val_loss: 5.2522e-04 - val_mae: 0.0229\n",
      "Epoch 21/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0054 - mae: 0.0534\n",
      "Epoch 00021: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 725us/sample - loss: 0.0055 - mae: 0.0537 - val_loss: 5.0058e-04 - val_mae: 0.0224\n",
      "Epoch 22/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0056 - mae: 0.0527\n",
      "Epoch 00022: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 723us/sample - loss: 0.0056 - mae: 0.0525 - val_loss: 5.2217e-04 - val_mae: 0.0228\n",
      "Epoch 23/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0043 - mae: 0.0473\n",
      "Epoch 00023: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 726us/sample - loss: 0.0043 - mae: 0.0472 - val_loss: 4.9022e-04 - val_mae: 0.0221\n",
      "Epoch 24/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0049 - mae: 0.0503\n",
      "Epoch 00024: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 727us/sample - loss: 0.0048 - mae: 0.0500 - val_loss: 4.7509e-04 - val_mae: 0.0218\n",
      "Epoch 25/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0045 - mae: 0.0491\n",
      "Epoch 00025: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 731us/sample - loss: 0.0045 - mae: 0.0490 - val_loss: 4.8963e-04 - val_mae: 0.0221\n",
      "Epoch 26/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0047 - mae: 0.0486\n",
      "Epoch 00026: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 727us/sample - loss: 0.0047 - mae: 0.0485 - val_loss: 4.7861e-04 - val_mae: 0.0219\n",
      "Epoch 27/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0047 - mae: 0.0487- ETA: 0s - loss: 0.0046 - mae: 0.04\n",
      "Epoch 00027: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 729us/sample - loss: 0.0047 - mae: 0.0488 - val_loss: 4.8966e-04 - val_mae: 0.0221\n",
      "Epoch 28/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0049 - mae: 0.0494\n",
      "Epoch 00028: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 741us/sample - loss: 0.0048 - mae: 0.0490 - val_loss: 4.9486e-04 - val_mae: 0.0222\n",
      "Epoch 29/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0040 - mae: 0.0450\n",
      "Epoch 00029: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 729us/sample - loss: 0.0039 - mae: 0.0449 - val_loss: 4.5909e-04 - val_mae: 0.0214\n",
      "Epoch 30/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0048 - mae: 0.0493\n",
      "Epoch 00030: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 732us/sample - loss: 0.0048 - mae: 0.0490 - val_loss: 4.2159e-04 - val_mae: 0.0205\n",
      "Epoch 31/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0039 - mae: 0.0451\n",
      "Epoch 00031: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 723us/sample - loss: 0.0039 - mae: 0.0450 - val_loss: 4.3634e-04 - val_mae: 0.0209\n",
      "Epoch 32/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0040 - mae: 0.0454\n",
      "Epoch 00032: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 723us/sample - loss: 0.0040 - mae: 0.0454 - val_loss: 4.1667e-04 - val_mae: 0.0204\n",
      "Epoch 33/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0038 - mae: 0.0441\n",
      "Epoch 00033: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 731us/sample - loss: 0.0038 - mae: 0.0442 - val_loss: 3.9559e-04 - val_mae: 0.0199\n",
      "Epoch 34/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0038 - mae: 0.0435\n",
      "Epoch 00034: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 715us/sample - loss: 0.0039 - mae: 0.0439 - val_loss: 3.5735e-04 - val_mae: 0.0189\n",
      "Epoch 35/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0036 - mae: 0.0434\n",
      "Epoch 00035: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 724us/sample - loss: 0.0036 - mae: 0.0433 - val_loss: 3.5420e-04 - val_mae: 0.0188\n",
      "Epoch 36/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0037 - mae: 0.0429\n",
      "Epoch 00036: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 717us/sample - loss: 0.0038 - mae: 0.0431 - val_loss: 3.4758e-04 - val_mae: 0.0186\n",
      "Epoch 37/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0035 - mae: 0.0423\n",
      "Epoch 00037: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 719us/sample - loss: 0.0035 - mae: 0.0423 - val_loss: 3.3808e-04 - val_mae: 0.0184\n",
      "Epoch 38/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0039 - mae: 0.0434- ETA: 0s - loss: 0.0039 - mae: 0.043\n",
      "Epoch 00038: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 722us/sample - loss: 0.0039 - mae: 0.0433 - val_loss: 3.6238e-04 - val_mae: 0.0190\n",
      "Epoch 39/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0402\n",
      "Epoch 00039: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 735us/sample - loss: 0.0034 - mae: 0.0404 - val_loss: 3.7659e-04 - val_mae: 0.0194\n",
      "Epoch 40/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0029 - mae: 0.0390\n",
      "Epoch 00040: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 721us/sample - loss: 0.0029 - mae: 0.0390 - val_loss: 3.3801e-04 - val_mae: 0.0184\n",
      "Epoch 41/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0409\n",
      "Epoch 00041: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 728us/sample - loss: 0.0033 - mae: 0.0407 - val_loss: 3.1606e-04 - val_mae: 0.0178\n",
      "Epoch 42/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0033 - mae: 0.0407\n",
      "Epoch 00042: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 712us/sample - loss: 0.0033 - mae: 0.0408 - val_loss: 3.1570e-04 - val_mae: 0.0178\n",
      "Epoch 43/50\n",
      "1344/1425 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0413\n",
      "Epoch 00043: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 736us/sample - loss: 0.0034 - mae: 0.0408 - val_loss: 3.1307e-04 - val_mae: 0.0177\n",
      "Epoch 44/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0028 - mae: 0.0379\n",
      "Epoch 00044: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 720us/sample - loss: 0.0028 - mae: 0.0377 - val_loss: 2.9259e-04 - val_mae: 0.0171\n",
      "Epoch 45/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0031 - mae: 0.0384\n",
      "Epoch 00045: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 721us/sample - loss: 0.0031 - mae: 0.0383 - val_loss: 3.3928e-04 - val_mae: 0.0184\n",
      "Epoch 46/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0031 - mae: 0.0385\n",
      "Epoch 00046: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 705us/sample - loss: 0.0031 - mae: 0.0386 - val_loss: 3.2444e-04 - val_mae: 0.0180\n",
      "Epoch 47/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0025 - mae: 0.0351\n",
      "Epoch 00047: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 707us/sample - loss: 0.0025 - mae: 0.0351 - val_loss: 3.0962e-04 - val_mae: 0.0176\n",
      "Epoch 48/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0357\n",
      "Epoch 00048: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 720us/sample - loss: 0.0026 - mae: 0.0356 - val_loss: 2.8757e-04 - val_mae: 0.0170\n",
      "Epoch 49/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0030 - mae: 0.0368\n",
      "Epoch 00049: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 728us/sample - loss: 0.0029 - mae: 0.0368 - val_loss: 2.9165e-04 - val_mae: 0.0171\n",
      "Epoch 50/50\n",
      "1376/1425 [===========================>..] - ETA: 0s - loss: 0.0029 - mae: 0.0359\n",
      "Epoch 00050: val_loss did not improve from 0.00002\n",
      "1425/1425 [==============================] - 1s 719us/sample - loss: 0.0029 - mae: 0.0359 - val_loss: 2.9660e-04 - val_mae: 0.0172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f74ae3bd68>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_a_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_a.fit(model_a_X_train_padded, model_a_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_a.predict(model_a_X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.0172435 ]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724358]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.0172435 ]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]\n",
      " [-0.01724361]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 0s 376us/sample - loss: 2.9841e-04 - mae: 0.0173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.000298409072583083, 0.017268343]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model_a.evaluate(model_a_X_test_padded, model_a_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (word vectors + price history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_df = df.dropna(subset=['cleaned_text', 'prev_60mins_prices'])\n",
    "\n",
    "model_b_X = model_b_df.loc[:, ['cleaned_text_2', 'prev_60mins_prices']]\n",
    "model_b_y = model_b_df.loc[:, '60mins_price_diff_perc']\n",
    "\n",
    "model_b_X_train, model_b_X_test, model_b_y_train, model_b_y_test = train_test_split(model_b_X, model_b_y, test_size=0.33, random_state=42)\n",
    "\n",
    "model_b_X_train_text = model_b_X_train.iloc[:, 0]\n",
    "model_b_X_train_price_history = model_b_X_train.iloc[:, 1].apply(lambda x: split(strip()))\n",
    "model_b_X_test_text = model_b_X_test.iloc[:, 0]\n",
    "model_b_X_test_price_history = model_b_X_test.iloc[:, 1]\n",
    "                           \n",
    "model_b_corpus_list = []\n",
    "\n",
    "for i in model_b_X_train_text:\n",
    "    model_b_corpus_list.append(i.split())\n",
    "    \n",
    "model_b_word2vec_model = Word2Vec(model_b_corpus_list, min_count=1, size=100)\n",
    "model_b_pretrained_weights = model_b_word2vec_model.wv.vectors\n",
    "\n",
    "model_b_num_words = [len(i) for i in model_b_corpus_list]\n",
    "model_b_longest_sentence_len = max(model_b_num_words)\n",
    "\n",
    "model_b_X_train_padded = sentence_to_indices_padded(model_b_X_train_text, model_b_longest_sentence_len)\n",
    "model_b_X_test_padded = sentence_to_indices_padded(model_b_X_test_text, model_b_longest_sentence_len)\n",
    "\n",
    "model_b_X_train_input = [model_b_X_train_padded, np.array(model_b_X_train_price_history)]\n",
    "model_b_X_test_input = [model_b_X_test_padded, model_b_X_test_price_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(model_b_X_train_input[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(pretrained_weights, longest_sentence_len, price_history_shape):\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    \n",
    "    # word vectors model\n",
    "    model1_input = layers.Input(shape=longest_sentence_len, dtype='int32', name='sentence_index_input')\n",
    "    model1 = layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], trainable=False)(model1_input)  \n",
    "    model1 = layers.LSTM(4, return_sequences=True, name='model1_LSTM1')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout1')(model1)\n",
    "    model1 = layers.LSTM(4, return_sequences=False, name='model1_LSTM2')(model1)\n",
    "    model1 = layers.Dropout(0.25,name='model1_dropout2')(model1)\n",
    "    \n",
    "    # price history model\n",
    "    model2_input = layers.Input(shape=price_history_shape, dtype='float32', name='price_history_input')\n",
    "    model2 = layers.LSTM(4, return_sequences=True, name='model2_LSTM1')(model2_input)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout1')(model2)\n",
    "    model2 = layers.LSTM(4, return_sequences=False, name='model2_LSTM2')(model2)\n",
    "    model2 = layers.Dropout(0.25,name='model2_dropout2')(model2)\n",
    "    \n",
    "    model_concat = layers.concatenate([model1, model2])\n",
    "    model_concat = layers.Dense(4,name='Dense',activation='relu')(model_concat)\n",
    "    model_concat = layers.Dropout(0.1)(model_concat)\n",
    "    model_concat = layers.Dense(1,activation='linear')(model_concat)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[model1_input, model2_input], outputs = model_concat)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\brgoh\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model_b = create_model_b(model_b_pretrained_weights, model_b_longest_sentence_len, (30,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_index_input (InputLaye [(None, 157)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 157, 100)     568600      sentence_index_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "price_history_input (InputLayer [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM1 (LSTM)             (None, 157, 4)       1680        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM1 (LSTM)             (None, 30, 4)        96          price_history_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout1 (Dropout)       (None, 157, 4)       0           model1_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout1 (Dropout)       (None, 30, 4)        0           model2_LSTM1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model1_LSTM2 (LSTM)             (None, 4)            144         model1_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model2_LSTM2 (LSTM)             (None, 4)            144         model2_dropout1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "model1_dropout2 (Dropout)       (None, 4)            0           model1_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model2_dropout2 (Dropout)       (None, 4)            0           model2_LSTM2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           model1_dropout2[0][0]            \n",
      "                                                                 model2_dropout2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 4)            36          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           Dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 570,705\n",
      "Trainable params: 2,105\n",
      "Non-trainable params: 568,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected price_history_input to have 3 dimensions, but got array with shape (1697, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-51ac47a17a6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     save_best_only=True) \n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_b_X_train_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_b_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda33\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    374\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected price_history_input to have 3 dimensions, but got array with shape (1697, 1)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y %H%Mh\")\n",
    "\n",
    "checkpoint_filepath = f'./model_b_checkpoint/{dt_string}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose = 1,\n",
    "    save_best_only=True) \n",
    "\n",
    "model_b.fit(model_b_X_train_input, model_b_y_train, validation_split=0.2, epochs=50, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[277.26, 277.27, 277.28, 277.28999999999996, 277.3, 277.305, 277.31, 277.31, 277.34000000000003, 277.37, 277.34, 277.3, 277.31, 277.31, 277.31, 277.31, 277.31, 277.35, 277.35, 277.35, 277.35, 277.35, 277.35, 277.35, 277.346, 277.342, 277.338, 277.334, 277.33, 277.33]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b_X_train_input[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
